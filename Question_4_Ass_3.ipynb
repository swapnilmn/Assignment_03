{
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "YBG6O3-EalNm",
        "ME3Bw-Mt6o9C",
        "OSpaGMeKaMEe",
        "Q8iP5v9WaVdm",
        "ZUxIF8m1L3gM",
        "Iz_KOyjNefG4",
        "yovwHiaBcamF",
        "EW1U4NZFcs3E",
        "fAswLBk6y5ZP",
        "sZzBuWUD4RDm",
        "0Sj1uhq8rBMl",
        "5zCg2m6QrBMp",
        "IcChvjHPoubu",
        "Syzjj9lMrBMu"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swapnilmn/Assignment_03/blob/main/Question_4_Ass_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4 (10 Marks)\n",
        "\n",
        "You will now apply your best model on the test data (You shouldn't have used test data so far. All the above experiments should have been done using train and val data only). "
      ],
      "metadata": {
        "id": "dgmgyHL03-H5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "!pip install wandb -qU\n",
        "import wandb\n",
        "\n",
        "# Check if CUDA is available and set the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def my_function():\n",
        "    # Install and import the wandb library for experiment tracking\n",
        "    !pip install wandb -qU\n",
        "    import wandb\n",
        "    \n",
        "    # Login to wandb with your API key\n",
        "    !wandb login 5dbfdc4a0de265f2dd69d623f169b7884aa4436f\n",
        "    \n",
        "    # Mount Google Drive to access files\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Call the function\n",
        "my_function()\n"
      ],
      "metadata": {
        "id": "rgNYBCrQug_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class Helper_Functions:\n",
        "    @staticmethod\n",
        "    def Time(s):\n",
        "        # Converts seconds to minutes and seconds\n",
        "        m = math.floor(s / 60)\n",
        "        s -= m * 60\n",
        "        return '%dm %ds' % (m, s)\n",
        "\n",
        "    @staticmethod\n",
        "    def Span(since, percent):\n",
        "        # Calculates the elapsed time and estimated remaining time\n",
        "        now = time.time()\n",
        "        s = now - since\n",
        "        es = s / percent\n",
        "        rs = es - s\n",
        "        return '%s (- %s)' % (Helper_Functions.Time(s), Helper_Functions.Time(rs))\n",
        "\n",
        "    @staticmethod\n",
        "    def key_printing(d, v):\n",
        "        # Prints keys from a dictionary that have specific values\n",
        "        f = [k for k, val in d.items() if val in v]\n",
        "        if f:\n",
        "            s = ''.join(f)\n",
        "            if s[-1] == '\\n':\n",
        "                s = s[:-1]\n",
        "            elif s[0] == '\\t':\n",
        "                s = s[1:]\n",
        "        else:\n",
        "            print(\"No keys\")\n",
        "\n",
        "        return s\n",
        "\n",
        "    @staticmethod\n",
        "    def char_acc(t, o):\n",
        "        # Calculates character-level accuracy between predicted and target tensors\n",
        "        with torch.no_grad():\n",
        "            se = [[o[i][j].item() == t[i][j].item() for j in range(t.shape[1])] for i in range(t.shape[0])]\n",
        "            c = np.sum(se)\n",
        "            tc = np.sum([len(row) for row in se])\n",
        "        return c / tc\n",
        "\n",
        "    @staticmethod\n",
        "    def word_acc(t, o):\n",
        "        # Calculates word-level accuracy between predicted and target tensors\n",
        "        o1 = torch.argmax(o, dim=1)\n",
        "        with torch.no_grad():\n",
        "            c = sum([(o1[i] == t[i]).sum().item() == t.shape[1] for i in range(t.shape[0])])\n",
        "        return c / t.shape[0]\n",
        "\n",
        "    @staticmethod\n",
        "    def sample_equidi_pts(d, e):\n",
        "        # Samples equidistant points from a given list\n",
        "        s = len(d) // e\n",
        "        i = np.arange(0, len(d), s)\n",
        "        p = [d[j] for j in i]\n",
        "        return p\n",
        "\n",
        "\n",
        "HF = Helper_Functions()\n",
        "\n",
        "# Creating a dictionary of helper functions\n",
        "variables = {\n",
        "    'asMinutes': HF.Time,\n",
        "    'timeSince': HF.Span,\n",
        "    'print_keys_for_values': HF.key_printing,\n",
        "    'char_level_accuracy': HF.char_acc,\n",
        "    'word_level_accuracy': HF.word_acc,\n",
        "    'sample_equidistant_points': HF.sample_equidi_pts\n",
        "}\n",
        "\n",
        "# Accessing the variables\n",
        "asMinutes = variables['asMinutes']\n",
        "timeSince = variables['timeSince']\n",
        "print_keys_for_values = variables['print_keys_for_values']\n",
        "char_level_accuracy = variables['char_level_accuracy']\n",
        "word_level_accuracy = variables['word_level_accuracy']\n",
        "sample_equidistant_points = variables['sample_equidistant_points']\n",
        "\n",
        "# Usage\n",
        "print(asMinutes)  # Prints the formatted time in minutes and seconds\n",
        "print(timeSince)  # Prints the elapsed time and estimated remaining time\n",
        "print(print_keys_for_values)  # Prints keys from a dictionary that have specific values\n",
        "print(char_level_accuracy)  # Calculates and returns character-level accuracy\n",
        "print(word_level_accuracy)  # Calculates and returns word-level accuracy\n",
        "print(sample_equidistant_points)  # Samples equidistant points from a given list\n"
      ],
      "metadata": {
        "id": "yMtxX6Pfv-1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dictionary of variables and their corresponding characters\n",
        "variables = {\n",
        "    'start_char': '\\t',     # Represents the tab character\n",
        "    'end_char': '\\n',       # Represents the newline character\n",
        "    'blank_char': ' ',      # Represents a blank space character\n",
        "    'unknown_char': '\\r'    # Represents a carriage return character\n",
        "}\n",
        "\n",
        "# Assign the value of 'start_char' from the 'variables' dictionary to the variable 'start_char'\n",
        "start_char = variables['start_char']\n",
        "\n",
        "# Assign the value of 'end_char' from the 'variables' dictionary to the variable 'end_char'\n",
        "end_char = variables['end_char']\n",
        "\n",
        "# Assign the value of 'blank_char' from the 'variables' dictionary to the variable 'blank_char'\n",
        "blank_char = variables['blank_char']\n",
        "\n",
        "# Assign the value of 'unknown_char' from the 'variables' dictionary to the variable 'unknown_char'\n",
        "unknown_char = variables['unknown_char']\n"
      ],
      "metadata": {
        "id": "DS5XcLIToSUz",
        "execution": {
          "iopub.status.busy": "2023-05-21T05:31:38.074939Z",
          "iopub.execute_input": "2023-05-21T05:31:38.075348Z",
          "iopub.status.idle": "2023-05-21T05:31:38.080865Z",
          "shell.execute_reply.started": "2023-05-21T05:31:38.075311Z",
          "shell.execute_reply": "2023-05-21T05:31:38.079611Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv  # Import the csv module\n",
        "import torch  # Import the torch module\n",
        "\n",
        "class DataPreProcessor:\n",
        "    def __init__(self):\n",
        "        self.file_paths = {\n",
        "            'file1': '/content/drive/MyDrive/aksharantar_sampled/mar/mar_test.csv',\n",
        "            'file2': '/content/drive/MyDrive/aksharantar_sampled/mar/mar_train.csv',\n",
        "            'file3': '/content/drive/MyDrive/aksharantar_sampled/mar/mar_train.csv'\n",
        "        }\n",
        "\n",
        "        self.variables = {\n",
        "            'start_char': '\\t',      # Represents the tab character\n",
        "            'end_char': '\\n',        # Represents the newline character\n",
        "            'blank_char': ' ',       # Represents a blank space character\n",
        "            'unknown_char': '\\r'     # Represents a carriage return character\n",
        "        }\n",
        "\n",
        "    def read_csv_files(self):\n",
        "        fr = {}  # Dictionary to store csv reader objects\n",
        "        hd = {}  # Dictionary to store headers of each file\n",
        "        dt = {}  # Dictionary to store data from each file\n",
        "\n",
        "        for key, fp in self.file_paths.items():\n",
        "            file = open(fp)  # Open the file with the given file path\n",
        "            csv_reader = csv.reader(file)  # Create a csv reader object\n",
        "            fr[key] = csv_reader  # Store the csv reader object in the dictionary\n",
        "            hd[key] = next(csv_reader)  # Retrieve the headers from the csv reader\n",
        "            dt[key] = []  # Initialize an empty list to store the data\n",
        "\n",
        "            for row in csv_reader:\n",
        "                dt[key].append(row)  # Append each row of data to the corresponding list\n",
        "\n",
        "            file.close()  # Close the file after reading\n",
        "\n",
        "        # Return the headers and data from each file\n",
        "        return hd['file1'], hd['file2'], hd['file3'], dt['file1'], dt['file2'], dt['file3']\n",
        "\n",
        "    def Reading_Data(self, lst):\n",
        "        i = [pair[0] for pair in lst]  # Extract the first element from each pair in the given list\n",
        "        t = [pair[1] for pair in lst]  # Extract the second element from each pair in the given list\n",
        "        return i, t  # Return the extracted first elements as 'i' and second elements as 't'\n",
        "\n",
        "\n",
        "    def Dict_lang(self, inputs, targets):\n",
        "        i_dict = {}         # Dictionary to store character-to-index mapping for inputs\n",
        "        max_i_length = 0    # Variable to track maximum input string length\n",
        "        i_char = []         # List to store unique characters in inputs\n",
        "\n",
        "        t_dict = {}         # Dictionary to store character-to-index mapping for targets\n",
        "        max_t_length = 0    # Variable to track maximum target string length\n",
        "        t_char = []         # List to store unique characters in targets\n",
        "\n",
        "        # Encoding Inputs and updating i_dict\n",
        "        for string in inputs:\n",
        "            max_i_length = max(len(string), max_i_length)  # Update maximum input string length\n",
        "            for char in string:\n",
        "                if char not in i_dict:\n",
        "                    i_dict[char] = len(i_char)   # Assign a unique index to each unique character\n",
        "                    i_char.append(char)         # Store the unique character in the list\n",
        "\n",
        "        if self.variables['blank_char'] not in i_dict:\n",
        "            i_dict[self.variables['blank_char']] = len(i_char)   # Assign index to the blank character if not present\n",
        "            i_char.append(self.variables['blank_char'])\n",
        "\n",
        "        i_dict[self.variables['unknown_char']] = len(i_char)     # Assign index to the unknown character\n",
        "        i_char.append(self.variables['unknown_char'])\n",
        "\n",
        "        if self.variables['start_char'] not in t_dict:\n",
        "            t_dict[self.variables['start_char']] = len(t_char)   # Assign index to the start character if not present\n",
        "            t_char.append(self.variables['start_char'])\n",
        "\n",
        "        # Encoding Targets and updating t_dict\n",
        "        for string in targets:\n",
        "            max_t_length = max(len(string) + 2, max_t_length)    # Update maximum target string length\n",
        "            for char in string:\n",
        "                if char not in t_dict:\n",
        "                    t_dict[char] = len(t_char)   # Assign a unique index to each unique character\n",
        "                    t_char.append(char)         # Store the unique character in the list\n",
        "\n",
        "        if self.variables['end_char'] not in t_dict:\n",
        "            t_dict[self.variables['end_char']] = len(t_char)     # Assign index to the end character if not present\n",
        "            t_char.append(self.variables['end_char'])\n",
        "\n",
        "        if self.variables['blank_char'] not in t_dict:\n",
        "            t_dict[self.variables['blank_char']] = len(t_char)   # Assign index to the blank character if not present\n",
        "            t_char.append(self.variables['blank_char'])\n",
        "\n",
        "        return i_dict, max_i_length, i_char, t_dict, max_t_length, t_char\n",
        "\n",
        "    def encoding_words(self, wl, ld, ml, l):\n",
        "        encds = []   # List to store the encoded sequences\n",
        "        for w in wl:\n",
        "            encd = [ld[c] if c in ld else ld[self.variables['unknown_char']] for c in w]\n",
        "            # Assign the index of each character in the word, or the index of unknown character if not present\n",
        "\n",
        "            if l == 0:\n",
        "                encd.extend([ld[self.variables['blank_char']]] * (ml - len(encd)))\n",
        "                # If 'l' is 0 (inputs), pad the sequence with blank character indices up to the maximum length\n",
        "            if l == 1:\n",
        "                encd = [ld[self.variables['start_char']]] + encd + [ld[self.variables['end_char']]]\n",
        "                encd.extend([ld[self.variables['blank_char']]] * (ml - len(encd)))\n",
        "                # If 'l' is 1 (targets), add start and end character indices to the sequence and pad it with blank character indices up to the maximum length\n",
        "\n",
        "            encds.append(encd)  # Append the encoded sequence to the list\n",
        "\n",
        "        return encds  # Return the list of encoded sequences\n",
        "\n",
        "\n",
        "    def Tokenzieations(self, train, val, test, input_dict, target_dict, max_input_length, max_target_length):\n",
        "        t_i, t_t = self.Reading_Data(train)  # Reading train data\n",
        "        te_i, te_t = self.Reading_Data(test)  # Reading test data\n",
        "        v_i, v_t = self.Reading_Data(val)  # Reading validation data\n",
        "\n",
        "        e_t_i = self.encoding_words(t_i, input_dict, max_input_length, 0)  # Encoding train inputs\n",
        "        e_t_t = self.encoding_words(t_t, target_dict, max_target_length, 1)  # Encoding train targets\n",
        "        e_v_i = self.encoding_words(v_i, input_dict, max_input_length, 0)  # Encoding validation inputs\n",
        "        e_v_t = self.encoding_words(v_t, target_dict, max_target_length, 1)  # Encoding validation targets\n",
        "        e_te_i = self.encoding_words(te_i, input_dict, max_input_length, 0)  # Encoding test inputs\n",
        "        e_te_t = self.encoding_words(te_t, target_dict, max_target_length, 1)  # Encoding test targets\n",
        "\n",
        "        return {\n",
        "            'encoded_train_inputs': e_t_i,\n",
        "            'encoded_train_targets': e_t_t,\n",
        "            'encoded_val_inputs': e_v_i,\n",
        "            'encoded_val_targets': e_v_t,\n",
        "            'encoded_test_inputs': e_te_i,\n",
        "            'encoded_test_targets': e_te_t\n",
        "        }\n",
        "\n",
        "    def tensor_pair_conversion(self, train_inputs, train_targets):\n",
        "        pairs = [(torch.tensor(input_data), torch.tensor(target_data)) for input_data, target_data in\n",
        "                zip(train_inputs, train_targets)]  # Creating pairs of tensor inputs and targets\n",
        "        return pairs\n",
        "\n",
        "DPP = DataPreProcessor()\n",
        "\n",
        "# Dictionary of functions\n",
        "function_dict = {\n",
        "    'tensor_pair_conversion': DPP.tensor_pair_conversion,\n",
        "    'Tokenzieations': DPP.Tokenzieations,\n",
        "    'encoding_words': DPP.encoding_words,\n",
        "    'Reading_Data': DPP.Reading_Data,\n",
        "    'Dict_lang': DPP.Dict_lang,\n",
        "    'read_csv_files': DPP.read_csv_files\n",
        "}\n",
        "\n",
        "tensor_pair_conversion = function_dict['tensor_pair_conversion']\n",
        "Tokenzieations = function_dict['Tokenzieations']\n",
        "encoding_words = function_dict['encoding_words']\n",
        "Reading_Data = function_dict['Reading_Data']\n",
        "Dict_lang = function_dict['Dict_lang']\n",
        "read_csv_files = function_dict['read_csv_files']\n",
        "\n",
        "# Call the function and assign the returned values to variables\n",
        "h1, h2, h3, test, val, train = read_csv_files()\n",
        "\n",
        "train_inputs, train_targets = Reading_Data(train)  # Reading train data\n",
        "test_inputs, test_targets = Reading_Data(test)  # Reading test data\n",
        "val_inputs, val_targets = Reading_Data(val)  # Reading validation data\n",
        "\n",
        "# print(train_inputs[1])  # Print train input at index 1\n",
        "# print(train_targets[1])  # Print train target at index 1\n",
        "\n",
        "input_dict, max_input_length, input_char, target_dict, max_target_length, target_char = Dict_lang(train_inputs + val_inputs + test_inputs, train_targets + val_targets + test_targets)  # Generating dictionaries for inputs and targets\n",
        "\n",
        "result = Tokenzieations(train, val, test, input_dict, target_dict, max_input_length, max_target_length)  # Tokenizing the data\n",
        "\n",
        "encoded_train_inputs = result['encoded_train_inputs']  # Encoded train inputs\n",
        "encoded_train_targets = result['encoded_train_targets']  # Encoded train targets\n",
        "encoded_val_inputs = result['encoded_val_inputs']  # Encoded validation inputs\n",
        "encoded_val_targets = result['encoded_val_targets']  # Encoded validation targets\n",
        "encoded_test_inputs = result['encoded_test_inputs']  # Encoded test inputs\n",
        "encoded_test_targets = result['encoded_test_targets']  # Encoded test targets\n",
        "\n",
        "r = random.randint(0,100)  # Generate a random number between 0 and 100\n",
        "# print(print_keys_for_values(input_dict, encoded_train_inputs[int(r)]))  # Print the keys corresponding to the values in input_dict for the randomly selected encoded train input\n",
        "# print(print_keys_for_values(target_dict, encoded_train_targets[int(r)]))  # Print the keys corresponding to the values in target_dict for the randomly selected encoded train target\n",
        "\n",
        "encoded_train_pairs = tensor_pair_conversion(encoded_train_inputs, encoded_train_targets)  # Convert encoded train inputs and targets into pairs of tensors\n",
        "encoded_val_pairs = tensor_pair_conversion(encoded_val_inputs, encoded_val_targets)  # Convert encoded validation inputs and targets into pairs of tensors\n",
        "encoded_test_pairs = tensor_pair_conversion(encoded_test_inputs, encoded_test_targets)  # Convert encoded test inputs and targets into pairs of tensors\n",
        "\n",
        "pairs = (encoded_train_pairs, encoded_val_pairs, encoded_test_pairs)  # Combine pairs into a tuple\n",
        "pair = random.choice(encoded_train_pairs)  # Randomly select a pair from the encoded train pairs\n",
        "\n",
        "# print(print_keys_for_values(input_dict, pair[0]))  # Print the keys corresponding to the values in input_dict for the selected pair input\n",
        "# print(print_keys_for_values(target_dict, pair[1]))  # Print the keys corresponding to the values in target_dict for the selected pair target\n"
      ],
      "metadata": {
        "id": "hVm6ME71IQEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, device, cell_type, vocab_size, embed_dim, hidden_size, num_layers=1, bidirectional=False, dropout_p=0):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # Embedding layer for converting input indices to dense vectors\n",
        "        self.hidden_size = hidden_size  # Size of the hidden state in the RNN\n",
        "        self.num_layers = num_layers  # Number of layers in the RNN\n",
        "        self.bidirectional = bidirectional  # Flag indicating whether the RNN is bidirectional or not\n",
        "        self.cell_type = cell_type  # Type of RNN cell ('lstm', 'rnn', or 'gru')\n",
        "        self.dropout_p = dropout_p  # Dropout probability\n",
        "        self.dropout = nn.Dropout(self.dropout_p)  # Dropout layer for regularization\n",
        "        \n",
        "        # Initialize the RNN cell based on the specified cell type\n",
        "        if cell_type == 'lstm':\n",
        "            self.rnn = nn.LSTM(embed_dim, hidden_size, num_layers=self.num_layers, batch_first=True, dropout=dropout_p, bidirectional=self.bidirectional)\n",
        "        elif cell_type == 'rnn':\n",
        "            self.rnn = nn.RNN(embed_dim, hidden_size, num_layers=self.num_layers, batch_first=True, dropout=dropout_p, bidirectional=self.bidirectional)\n",
        "        elif cell_type == 'gru':\n",
        "            self.rnn = nn.GRU(embed_dim, hidden_size, num_layers=self.num_layers, batch_first=True, dropout=dropout_p, bidirectional=self.bidirectional)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        out = self.embedding(x)  # Perform embedding lookup to get dense representations of input indices\n",
        "        out = self.dropout(out)  # Apply dropout to the input embeddings\n",
        "        if self.cell_type == 'lstm':\n",
        "            out, (hidden, cell) = self.rnn(out, (hidden, cell))  # Forward pass through the LSTM\n",
        "            return out, hidden, cell\n",
        "        elif self.cell_type == 'rnn':\n",
        "            out, hidden = self.rnn(out, hidden)  # Forward pass through the RNN\n",
        "            return out, hidden\n",
        "        elif self.cell_type == 'gru':\n",
        "            out, hidden = self.rnn(out, hidden)  # Forward pass through the GRU\n",
        "            return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        # Initialize the hidden and cell states with random values\n",
        "        hidden = torch.randn((1 + int(self.bidirectional)) * self.num_layers, batch_size, self.hidden_size, device=device)\n",
        "        cell = torch.randn((1 + int(self.bidirectional)) * self.num_layers, batch_size, self.hidden_size, device=device)\n",
        "        return hidden, cell\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, device, cell_type, output_vocab, embed_size, hidden_size, max_length, dropout_p=0.1, num_layers=1, bidirectional=False):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size  # Hidden size of the decoder\n",
        "        self.output_size = output_vocab  # Size of the output vocabulary\n",
        "        self.embed_size = embed_size  # Size of the input embedding\n",
        "        self.dropout_p = dropout_p  # Dropout probability\n",
        "        self.cell_type = cell_type  # Type of RNN cell (lstm, gru, rnn)\n",
        "        self.max_length = max_length  # Maximum length of input sequence\n",
        "        self.device = device  # Device (e.g., CPU or GPU) to be used for computations\n",
        "        self.num_layers = num_layers  # Number of layers in the decoder\n",
        "        self.embedding_decoder = nn.Embedding(self.output_size, self.embed_size)  # Embedding layer for the decoder\n",
        "        self.dropout = nn.Dropout(self.dropout_p)  # Dropout layer\n",
        "        self.bidirectional = bidirectional  # Flag indicating if the encoder is bidirectional\n",
        "\n",
        "        # Determine the type of RNN cell based on the given cell_type\n",
        "        if cell_type == 'lstm':\n",
        "            self.rnn = nn.LSTM(self.embed_size, hidden_size, num_layers=self.num_layers, batch_first=True, bidirectional=self.bidirectional, dropout=self.dropout_p)\n",
        "        elif cell_type == 'gru':\n",
        "            self.rnn = nn.GRU(self.embed_size, hidden_size, num_layers=self.num_layers, batch_first=True, bidirectional=self.bidirectional, dropout=self.dropout_p)\n",
        "        elif cell_type == 'rnn':\n",
        "            self.rnn = nn.RNN(self.embed_size, hidden_size, num_layers=self.num_layers, batch_first=True, bidirectional=self.bidirectional, dropout=self.dropout_p)\n",
        "\n",
        "        self.out = nn.Linear((1 + int(self.bidirectional)) * self.hidden_size, self.output_size)  # Linear layer for output prediction\n",
        "        self.out_activation = nn.LogSoftmax(dim=-1)  # Log softmax activation for output probabilities\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        input = input.unsqueeze(1)  # Add a singleton dimension to the input tensor\n",
        "        embedded_decoder = self.embedding_decoder(input)  # Apply embedding to the input\n",
        "        embedded_decoder = self.dropout(embedded_decoder)  # Apply dropout to the embedded input\n",
        "\n",
        "        # Pass the embedded input through the RNN cell\n",
        "        if self.cell_type == 'lstm':\n",
        "            output, (hidden, cell) = self.rnn(embedded_decoder, (hidden, cell))\n",
        "        elif self.cell_type == 'gru':\n",
        "            output, hidden = self.rnn(embedded_decoder, hidden)\n",
        "        elif self.cell_type == 'rnn':\n",
        "            output, hidden = self.rnn(embedded_decoder, hidden)\n",
        "\n",
        "        output = F.relu(self.out(output))  # Apply ReLU activation to the output\n",
        "        output = F.log_softmax(output, dim=-1)  # Apply log softmax activation to obtain output probabilities\n",
        "\n",
        "        return output, hidden, cell  # Return the output, hidden state, and cell state\n",
        "\n",
        "    def init_hidden(self, encoder_hidden, encoder_cell, encoder_bidirectional):\n",
        "        hidden = encoder_hidden[-(1 + int(encoder_bidirectional)): ].repeat(self.num_layers, 1, 1)  # Initialize the hidden state using the encoder's hidden state\n",
        "        cell = encoder_cell[-(1 + int(encoder_bidirectional)): ].repeat(self.num_layers, 1, 1)  # Initialize the cell state using the encoder's cell state\n",
        "        return hidden, cell  # Return the initialized hidden state and cell state\n",
        "\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder  # Initialize the encoder\n",
        "        self.decoder = decoder  # Initialize the decoder\n",
        "        self.device = device  # Store the device (e.g., CPU or GPU)\n",
        "        self.max_target_length = 0  # Variable to store the maximum target sequence length\n",
        "        self.sos = 0  # Start-of-sequence token\n",
        "        \n",
        "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "        batch_size = target.shape[0]  # Get the batch size\n",
        "        target_len = target.shape[1]  # Get the target sequence length\n",
        "        self.max_target_length = target_len  # Update the maximum target sequence length\n",
        "        target_vocab_size = self.decoder.output_size  # Get the target vocabulary size\n",
        "        \n",
        "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)  # Initialize the outputs tensor\n",
        "        \n",
        "        encoder_hidden, encoder_cell = self.encoder.init_hidden(batch_size)  # Initialize the encoder hidden state and cell state\n",
        "        \n",
        "        if (self.encoder.cell_type == 'lstm'):\n",
        "            encoder_outputs, encoder_hidden, encoder_cell = self.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "            # Forward pass through the encoder LSTM\n",
        "        if (self.encoder.cell_type == 'rnn'):\n",
        "            encoder_outputs, encoder_hidden = self.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "            # Forward pass through the encoder RNN\n",
        "        if (self.encoder.cell_type == 'gru'):\n",
        "            encoder_outputs, encoder_hidden = self.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "            # Forward pass through the encoder GRU\n",
        "        \n",
        "        input = target[:, 0]  # Set the first input to the decoder as the <sos> token\n",
        "        self.sos = target[:, 0]  # Store the <sos> token\n",
        "        \n",
        "        hidden, cell = self.decoder.init_hidden(encoder_hidden, encoder_cell, self.encoder.bidirectional)\n",
        "        # Initialize the decoder hidden state and cell state\n",
        "        \n",
        "        for t in range(1, target_len):\n",
        "            output, hidden, cell = self.decoder.forward(input, hidden, cell)\n",
        "            # Forward pass through the decoder\n",
        "            outputs[:, t] = output.squeeze(1)  # Store the decoder output in the outputs tensor\n",
        "            teacher_force = random.random() < teacher_forcing_ratio  # Determine whether to use teacher forcing\n",
        "            top1 = output.argmax(-1)  # Get the index of the highest probability output\n",
        "            input = target[:, t] if teacher_force else top1.squeeze(1)  # Set the next input to the decoder\n",
        "            \n",
        "        return outputs\n",
        "    \n",
        "    def inference(self, source, target):\n",
        "        batch_size = source.shape[0]  # Get the batch size\n",
        "        target_len = self.max_target_length  # Get the maximum target sequence length\n",
        "        target_vocab_size = self.decoder.output_size  # Get the target vocabulary size\n",
        "        \n",
        "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)  # Initialize the outputs tensor\n",
        "        \n",
        "        encoder_hidden, encoder_cell = self.encoder.init_hidden(batch_size)  # Initialize the encoder hidden state and cell state\n",
        "        \n",
        "        if (self.encoder.cell_type == 'lstm'):\n",
        "            encoder_outputs, encoder_hidden, encoder_cell = self.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "            # Forward pass through the encoder LSTM\n",
        "        if (self.encoder.cell_type == 'rnn'):\n",
        "            encoder_outputs, encoder_hidden = self.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "            # Forward pass through the encoder RNN\n",
        "        if (self.encoder.cell_type == 'gru'):\n",
        "            encoder_outputs, encoder_hidden = self.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "            # Forward pass through the encoder GRU\n",
        "        \n",
        "        input = self.sos  # Set the first input to the <sos> token\n",
        "        \n",
        "        hidden, cell = self.decoder.init_hidden(encoder_hidden, encoder_cell, self.encoder.bidirectional)\n",
        "        # Initialize the decoder hidden state and cell state\n",
        "        \n",
        "        for t in range(1, target_len):\n",
        "            output, hidden, cell = self.decoder.forward(input, hidden, cell)\n",
        "            # Forward pass through the decoder\n",
        "            outputs[:, t] = output.squeeze(1)  # Store the decoder output in the outputs tensor\n",
        "            top1 = output.argmax(-1)  # Get the index of the highest probability output\n",
        "            input = top1.squeeze(1)  # Set the next input to the decoder\n",
        "            \n",
        "        return outputs\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UBE07BLxae-0",
        "execution": {
          "iopub.status.busy": "2023-05-21T05:31:46.924970Z",
          "iopub.execute_input": "2023-05-21T05:31:46.925567Z",
          "iopub.status.idle": "2023-05-21T05:31:46.949929Z",
          "shell.execute_reply.started": "2023-05-21T05:31:46.925519Z",
          "shell.execute_reply": "2023-05-21T05:31:46.948659Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(input_dict, target_dict):\n",
        "    variables = {}\n",
        "    \n",
        "    # Count the number of elements in the input and target dictionaries\n",
        "    variables['a'] = len(input_dict)\n",
        "    variables['b'] = len(target_dict)\n",
        "    \n",
        "    # Set values for variables c, d, e, f, g, h, i, j, k, and max_length\n",
        "    variables['c'] = 32    #batch size\n",
        "    variables['d'] = 32     #val batch size\n",
        "    variables['e'] = 256    #enc_embedding\n",
        "    variables['f'] = 256    #dec embedding\n",
        "    variables['g'] = 512    #hidden_size \n",
        "    variables['h'] = 3     #enc_number_layers\n",
        "    variables['i'] = 2     #dec_num_layers\n",
        "    variables['j'] = 0.4    #enc_dropout\n",
        "    variables['k'] = 0.4    #dec_drop_out\n",
        "    variables['max_length'] = max_target_length\n",
        "    \n",
        "    # Set value for variable l\n",
        "    variables['l'] = 'gru'\n",
        "    \n",
        "    # Create an instance of EncoderRNN and assign it to variable m\n",
        "    variables['m'] = EncoderRNN(device, variables['l'], variables['a'], variables['e'], variables['g'],\n",
        "                                variables['h'], bidirectional=True, dropout_p=variables['j'])\n",
        "    \n",
        "    # Create an instance of DecoderRNN and assign it to variable n\n",
        "    variables['n'] = DecoderRNN(device, variables['l'], variables['b'], variables['f'], variables['g'],\n",
        "                                variables['max_length'], variables['k'], variables['i'], bidirectional=True)\n",
        "    \n",
        "    # Create an instance of Seq2Seq by passing in the encoder and decoder instances, and assign it to variable o\n",
        "    variables['o'] = Seq2Seq(variables['m'], variables['n'], device).to(device)\n",
        "    \n",
        "    # Define a function count_parameters(model) that returns the total number of trainable parameters in a model\n",
        "    def count_parameters(model):\n",
        "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "    # Print the number of trainable parameters in the Seq2Seq model\n",
        "    print(f' {count_parameters(variables[\"o\"]):,} To be train params')\n",
        "    \n",
        "    # Create an instance of Adam optimizer and assign it to variable p\n",
        "    variables['p'] = torch.optim.Adam(variables['o'].parameters(), lr=0.001)\n",
        "    \n",
        "    # Create an instance of negative log likelihood loss and assign it to variable q\n",
        "    variables['q'] = nn.NLLLoss()\n",
        "    \n",
        "    # Return the Seq2Seq model, optimizer, and criterion\n",
        "    return variables['o'], variables['p'], variables['q']\n",
        "\n",
        "# Call the create_model function with input_dict and target_dict, and assign the returned values to model, optimizer, and criterion respectively\n",
        "model, optimizer, criterion = create_model(input_dict, target_dict)\n"
      ],
      "metadata": {
        "id": "LTsT62M9BrDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainIters(m, p, b, n, o, t, p_e=10, p_v=10, l=True, A=False):\n",
        "    s = time.time()  # Starting time\n",
        "\n",
        "    p_l = []  # List to store training loss\n",
        "    t_w_a = []  # List to store word-level accuracy on training set\n",
        "    v_w_a = []  # List to store word-level accuracy on validation set\n",
        "    p_l_t = 0  # Training loss per iteration\n",
        "    p_l_v_t = 0  # Validation loss per iteration\n",
        "\n",
        "    t_p = p[0]  # Training data\n",
        "    v_p = p[1]  # Validation data\n",
        "    t_a = 0  # Accumulated word-level accuracy on training set\n",
        "\n",
        "    c = nn.NLLLoss()  # Negative log likelihood loss\n",
        "\n",
        "    count = 0  # Iteration count\n",
        "    for it in range(1, n + 1):  # Number of iterations\n",
        "        for i in range(0, len(t_p) - b, b):  # Batch processing\n",
        "            t_a = 0  # Reset accumulated word-level accuracy\n",
        "            count += 1  # Increment iteration count\n",
        "\n",
        "            if i + b > len(t_p):\n",
        "                b = len(t_p) - i + 1  # Adjust batch size if remaining samples are less than the batch size\n",
        "\n",
        "            i_t = torch.stack([t_p[i + j][0] for j in range(b)]).squeeze(1).long().cuda()  # Input tensor\n",
        "            t_t = torch.stack([t_p[i + j][1] for j in range(b)]).squeeze(1).long().cuda()  # Target tensor\n",
        "\n",
        "            o.zero_grad()  # Clear gradients\n",
        "            out = m(i_t, t_t, teacher_forcing_ratio=t if count < 4000 else 0)  # Forward pass\n",
        "            out = torch.permute(out, [0, 2, 1])  # Permute output tensor dimensions\n",
        "            l = c(out, t_t)  # Calculate loss\n",
        "\n",
        "            t_a_w = word_level_accuracy(t_t, out) * b  # Calculate word-level accuracy on training set\n",
        "            t_a += t_a_w  # Accumulate word-level accuracy\n",
        "            l.backward()  # Backward pass\n",
        "            torch.nn.utils.clip_grad_norm_(m.parameters(), 1)  # Clip gradients to avoid exploding gradients\n",
        "            o.step()  # Update model parameters\n",
        "\n",
        "            p_l_t += l  # Accumulate training loss per iteration\n",
        "            p_l_v_t += l  # Accumulate validation loss per iteration\n",
        "\n",
        "            if count % 800 == 0:\n",
        "                v_i_t = torch.stack([v_p[j][0] for j in range(b)]).squeeze(1).long().cuda()  # Validation input tensor\n",
        "                v_t_t = torch.stack([v_p[j][1] for j in range(b)]).squeeze(1).long().cuda()  # Validation target tensor\n",
        "\n",
        "                if A:\n",
        "                    v_o, _ = m.inference(v_i_t, v_t_t)\n",
        "                else:\n",
        "                    v_o = m.inference(v_i_t, v_t_t)\n",
        "\n",
        "                v_o = v_o.permute(0, 2, 1)  # Permute validation output tensor dimensions\n",
        "                v_l = c(v_o, v_t_t)  # Calculate validation loss\n",
        "                v_l_s = v_l\n",
        "                wandb.log({'Val_Loss': v_l_s})  # Log validation loss\n",
        "\n",
        "            if count % 800 == 0:\n",
        "                p_l_a = p_l_t / 800  # Average training loss over 800 iterations\n",
        "                p_l_t = 0  # Reset training loss accumulator\n",
        "                print('%s (%d %d%%) %.7f' % (timeSince(s, it / n), it, it / n * 100, p_l_a))  # Print progress\n",
        "\n",
        "            if count % 800 == 0:\n",
        "                p_l_a = p_l_v_t / 800  # Average validation loss over 800 iterations\n",
        "                p_l.append(p_l_a.detach())  # Append training loss to list\n",
        "                wandb.log({'Train Loss': p_l_a})  # Log training loss\n",
        "                p_l_v_t = 0  # Reset validation loss accumulator\n",
        "\n",
        "        t_a = t_a / (len(t_p) - b)  # Calculate average word-level accuracy on training set\n",
        "        t_w_a.append(t_a)  # Append word-level accuracy to list\n",
        "\n",
        "    print(t_w_a)  # Print word-level accuracy on training set\n",
        "\n",
        "    p_l = [l.cpu().numpy() for l in p_l]  # Convert training loss to numpy array\n",
        "    p_l_s = sample_equidistant_points(p_l, n)  # Sample equidistant points from the training loss\n",
        "\n",
        "    w_c = 0  # Total correct predictions\n",
        "    for i in range(0, len(v_p) - b, b):  # Batch processing on validation set\n",
        "        if i + b > len(v_p):\n",
        "            b = len(v_p) - i + 1  # Adjust batch size if remaining samples are less than the batch size\n",
        "\n",
        "        v_i_t = torch.stack([v_p[i + j][0] for j in range(b)]).squeeze(1).long().cuda()  # Validation input tensor\n",
        "        v_t_t = torch.stack([v_p[i + j][1] for j in range(b)]).squeeze(1).long().cuda()  # Validation target tensor\n",
        "\n",
        "        if A:\n",
        "            v_o, _ = m.inference(v_i_t, v_t_t)\n",
        "        else:\n",
        "            v_o = m.inference(v_i_t, v_t_t)\n",
        "\n",
        "        v_o = v_o.permute(0, 2, 1)  # Permute validation output tensor dimensions\n",
        "        v_l = c(v_o, v_t_t)  # Calculate validation loss\n",
        "\n",
        "        v_a_w = word_level_accuracy(v_t_t, v_o)  # Calculate word-level accuracy on validation set\n",
        "        w_c += v_a_w * b  # Accumulate correct predictions\n",
        "\n",
        "    w_a = w_c / (len(v_p) - b)  # Average word-level accuracy on validation set\n",
        "    m = {'Val_Accuracy': w_a}  # Log validation accuracy\n",
        "    wandb.log(m)\n",
        "\n",
        "    print(f\"Val loss = {v_l}\")  # Print validation loss\n",
        "    print(f'Word-level-accuracy on val set = {w_a}')  # Print word-level accuracy on validation set\n"
      ],
      "metadata": {
        "id": "3_VZGr2LQ4Lk",
        "execution": {
          "iopub.status.busy": "2023-05-21T05:32:02.467407Z",
          "iopub.execute_input": "2023-05-21T05:32:02.467856Z",
          "iopub.status.idle": "2023-05-21T05:32:07.376445Z",
          "shell.execute_reply.started": "2023-05-21T05:32:02.467816Z",
          "shell.execute_reply": "2023-05-21T05:32:07.375225Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the sweep configuration\n",
        "sw = {\n",
        "    'method': 'bayes',\n",
        "    'name': 'Hyperparameter Tuning-Bayesian'\n",
        "}\n",
        "\n",
        "# Defining the metric configuration\n",
        "m = {\n",
        "    'name': 'Val_Accuracy',\n",
        "    'goal': 'maximize'\n",
        "}\n",
        "\n",
        "# Adding the metric configuration to the sweep configuration\n",
        "sw['metric'] = m\n",
        "\n",
        "# Defining the parameters configuration\n",
        "p = {\n",
        "    'optimiser': {'values': ['adam','nadam']},\n",
        "    'teacher_forcing_ratio': {'values': [0.3, 0.5, 0.7]},\n",
        "    'bidirectional': {'values': [True, False]},\n",
        "    'enc_embedding': {'values': [128,256]},\n",
        "    'dec_embedding': {'values': [128,256]},\n",
        "    'epochs': {'values': [2,5,8]},\n",
        "    'hidden_size': {'values': [64,128,256,512]},\n",
        "    'enc_layers': {'values': [2,3]},\n",
        "    'dec_layers': {'values': [2,3]},\n",
        "    'dropout': {'values': [0.3,0.4]},\n",
        "    'cell_type': {'values': ['lstm','gru','rnn']}\n",
        "}\n",
        "\n",
        "# Adding the parameters configuration to the sweep configuration\n",
        "sw['parameters'] = p\n",
        "\n",
        "# Creating a sweep using the sweep configuration and assigning it to sw_id\n",
        "sw_id = wandb.sweep(sw, project=\"CS6910 Assignment 3\")\n",
        "\n",
        "# Function to train the sweep\n",
        "def train_sweep(c=None):\n",
        "    # Initializing wandb run with the given config\n",
        "    with wandb.init(config=c) as r:\n",
        "        c = wandb.config\n",
        "\n",
        "        # Creating the validation data dictionary\n",
        "        vd = {\n",
        "            'input_dim': len(input_dict),\n",
        "            'output_dim': len(target_dict),\n",
        "            'batch_size': 32,\n",
        "            'val_batch_size': 32,\n",
        "            'enc_embedding': c.enc_embedding,\n",
        "            'dec_embedding': c.dec_embedding,\n",
        "            'hidden': c.hidden_size,\n",
        "            'enc_num_layers': c.enc_layers,\n",
        "            'dec_num_layers': c.dec_layers,\n",
        "            'enc_dropout': c.dropout,\n",
        "            'dec_dropout': c.dropout,\n",
        "            'max_length': max_target_length,\n",
        "            'cell_type': c.cell_type\n",
        "        }\n",
        "\n",
        "        # Extracting values from the validation data dictionary\n",
        "        input_dim = vd['input_dim']\n",
        "        output_dim = vd['output_dim']\n",
        "        batch_size = vd['batch_size']\n",
        "        val_batch_size = vd['val_batch_size']\n",
        "        enc_embedding = vd['enc_embedding']\n",
        "        dec_embedding = vd['dec_embedding']\n",
        "        hidden = vd['hidden']\n",
        "        enc_num_layers = vd['enc_num_layers']\n",
        "        dec_num_layers = vd['dec_num_layers']\n",
        "        enc_dropout = vd['enc_dropout']\n",
        "        dec_dropout = vd['dec_dropout']\n",
        "        max_length = vd['max_length']\n",
        "        cell_type = vd['cell_type']\n",
        "\n",
        "        # Creating the encoder and decoder models\n",
        "        enc = EncoderRNN(device, cell_type, input_dim, enc_embedding, hidden, enc_num_layers,\n",
        "                         bidirectional=c.bidirectional, dropout_p=enc_dropout)\n",
        "        dec = DecoderRNN(device, cell_type, output_dim, dec_embedding, hidden, max_length, dec_dropout,\n",
        "                         dec_num_layers, bidirectional=c.bidirectional)\n",
        "        model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "        # Setting the experiment name based on the config\n",
        "        exp_name = f\"{c.cell_type}_e_{c.optimiser}\"\n",
        "\n",
        "        # Assigning the experiment name to the wandb run\n",
        "        wandb.run.name = exp_name\n",
        "\n",
        "        # Creating the optimizer based on the chosen optimiser\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001) if c.optimiser == 'adam' else torch.optim.NAdam(\n",
        "            model.parameters(), lr=0.001)\n",
        "\n",
        "        criterion = nn.NLLLoss()\n",
        "\n",
        "\n",
        "        # Training the model using the trainIters function\n",
        "        trainIters(model, pairs, 32, c.epochs, optimizer, c.teacher_forcing_ratio)\n",
        "\n",
        "# Running the sweep agent with the given sweep ID and train_sweep function\n",
        "wandb.agent(sw_id, train_sweep, count=1)\n",
        "\n",
        "# Finishing the wandb run\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "hlQ8__dMfy2I",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def BestVanillaModel():\n",
        "    variables = {\n",
        "        'a': len(input_dict),  # Input dimension\n",
        "        'b': len(target_dict),  # Output dimension\n",
        "        'c': 32,  # Batch_size\n",
        "        'd': 32,  # val_batch_size\n",
        "        'e': 128,  # enc_embedding\n",
        "        'f': 128,  # dec_embedding\n",
        "        'g': 512,  # hidden\n",
        "        'h': 10,  # epochs\n",
        "        'i': 3,  # enc_num_layers\n",
        "        'j': 3,  # dec_num_layers\n",
        "        'k': 0.3,  # enc_dropout\n",
        "        'l': 0.3,  # dec_dropout\n",
        "        'm': max_target_length,\n",
        "        'n': 'lstm',\n",
        "        'o': 0.7  # teacher_forcing_ratio\n",
        "    }\n",
        "\n",
        "    p = EncoderRNN(device, variables['n'], variables['a'], variables['e'], variables['g'], variables['i'], bidirectional=True, dropout_p=variables['k'])\n",
        "    q = DecoderRNN(device, variables['n'], variables['b'], variables['f'], variables['g'], variables['m'], variables['l'], variables['j'], bidirectional=True)\n",
        "    r = Seq2Seq(p, q, device).to(device)\n",
        "\n",
        "    s = torch.optim.Adam(r.parameters(), lr=0.001)\n",
        "\n",
        "    wandb.init()\n",
        "\n",
        "    trainIters(r, pairs, variables['c'], variables['h'], s, variables['o'])\n",
        "\n",
        "BestVanillaModel()"
      ],
      "metadata": {
        "id": "cUA5I2rpXo_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the values of variables using dictionaries\n",
        "variables = {\n",
        "    'a': len(input_dict),\n",
        "    'b': len(target_dict),\n",
        "    'c': 32,  # Batch_size\n",
        "    'd': 32,  #val_batch_size\n",
        "    'e': 128, #enc_embedding\n",
        "    'f': 128,#dec_embedding\n",
        "    'g': 256,#hidden\n",
        "    'h': 10,#epochs\n",
        "    'i': 2,#enc_num_layers\n",
        "    'j': 2,#dec_num_layers\n",
        "    'k': 0.3,#enc_dropout\n",
        "    'l': 0.3,#dec_dropout\n",
        "    'm': max_target_length,\n",
        "    'n': 'lstm',\n",
        "    'o': 0.7#teacher_forcing_ratio\n",
        "}\n",
        "\n",
        "# Create the encoder, decoder, and Seq2Seq models\n",
        "p = EncoderRNN(device, variables['n'], variables['a'], variables['e'], variables['g'], variables['i'], bidirectional=True, dropout_p=variables['k'])\n",
        "q = DecoderRNN(device, variables['n'], variables['b'], variables['f'], variables['g'], variables['m'], variables['l'], variables['j'], bidirectional=True)\n",
        "r = Seq2Seq(p, q, device).to(device)\n",
        "\n",
        "# Define the optimizer\n",
        "s = torch.optim.Adam(r.parameters(), lr=0.001)\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init()\n",
        "\n",
        "# Train the model using trainIters function\n",
        "trainIters(r, pairs, variables['c'], variables['h'], s, variables['o'])\n",
        "\n",
        "\n",
        "import wandb\n",
        "import csv\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "wandb.init(project='Ass-3-test-accuracy-plot')\n",
        "\n",
        "class MyClass:\n",
        "    @staticmethod\n",
        "    def append_strings_to_csv(file_path, s1, s2, s3):\n",
        "        with open(file_path, 'a', newline='') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerow([s1, s2, s3])\n",
        "\n",
        "    def train_and_evaluate(self):\n",
        "        # Prepare for testing\n",
        "        t = pairs[2]\n",
        "        u = 0\n",
        "        v = 1\n",
        "        w = 'VanillaFinalPredictions' + str(v)\n",
        "        x = '/content/drive/MyDrive/' + w + '.csv'\n",
        "\n",
        "        # Iterate over test_pairs in batches\n",
        "        for y in np.arange(start=0, stop=len(t) - variables['c'], step=variables['c']):\n",
        "            if y + variables['c'] > len(t):\n",
        "                variables['c'] = len(t) - y + 1\n",
        "            z = []\n",
        "            aa = []\n",
        "\n",
        "            # Gather input and target tensors for the batch\n",
        "            for ab in range(variables['c']):\n",
        "                z.append(t[y + ab][0])\n",
        "                aa.append(t[y + ab][1])\n",
        "\n",
        "            z = torch.stack(z).squeeze(1).long().cuda()\n",
        "            aa = torch.stack(aa).squeeze(1).long().cuda()\n",
        "\n",
        "            # Perform inference and calculate the loss\n",
        "            ac = r.inference(z, aa)\n",
        "            ac = ac.permute(0, 2, 1)\n",
        "            ad = criterion(ac, aa)\n",
        "\n",
        "            # Iterate over each example in the batch\n",
        "            for ae in range(variables['c']):\n",
        "                af = print_keys_for_values(input_dict, z[ae])\n",
        "                ag = print_keys_for_values(target_dict, torch.argmax(ac[ae], dim=0))\n",
        "                ah = print_keys_for_values(target_dict, aa[ae])\n",
        "                self.append_strings_to_csv(x, af, ah, ag)\n",
        "\n",
        "            # Calculate word-level accuracy\n",
        "            ai = word_level_accuracy(aa, ac)\n",
        "            u = u + (ai) * variables['c']\n",
        "\n",
        "        # Calculate and log the accuracy on the test set\n",
        "        aj = u / (len(t) - variables['c'])\n",
        "        wandb.log({'Test Accuracy': aj})\n",
        "\n",
        "        print(f\"Accuracy on Test Set is {aj}\")\n",
        "\n",
        "wandb.init(project='Ass-3-test-accuracy-plot')\n",
        "\n",
        "# Create an instance of MyClass\n",
        "my_object = MyClass()\n",
        "\n",
        "# Call the train_and_evaluate method\n",
        "my_object.train_and_evaluate()"
      ],
      "metadata": {
        "id": "HGIt66vnYRpa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}