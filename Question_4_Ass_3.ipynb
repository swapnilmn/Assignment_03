{
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swapnilmn/Assignment_03/blob/main/Question_4_Ass_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "!pip install wandb -qU\n",
        "import wandb\n",
        "\n",
        "# Check if CUDA is available and set the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def my_function():\n",
        "    # Install and import the wandb library for experiment tracking\n",
        "    !pip install wandb -qU\n",
        "    import wandb\n",
        "    \n",
        "    # Login to wandb with your API key\n",
        "    !wandb login 5dbfdc4a0de265f2dd69d623f169b7884aa4436f\n",
        "    \n",
        "    # Mount Google Drive to access files\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Call the function\n",
        "my_function()\n"
      ],
      "metadata": {
        "id": "rgNYBCrQug_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class Helper_Functions:\n",
        "    @staticmethod\n",
        "    def Time(s):\n",
        "        # Converts seconds to minutes and seconds\n",
        "        m = math.floor(s / 60)\n",
        "        s -= m * 60\n",
        "        return '%dm %ds' % (m, s)\n",
        "\n",
        "    @staticmethod\n",
        "    def Span(since, percent):\n",
        "        # Calculates the elapsed time and estimated remaining time\n",
        "        now = time.time()\n",
        "        s = now - since\n",
        "        es = s / percent\n",
        "        rs = es - s\n",
        "        return '%s (- %s)' % (Helper_Functions.Time(s), Helper_Functions.Time(rs))\n",
        "\n",
        "    @staticmethod\n",
        "    def key_printing(d, v):\n",
        "        # Prints keys from a dictionary that have specific values\n",
        "        f = [k for k, val in d.items() if val in v]\n",
        "        if f:\n",
        "            s = ''.join(f)\n",
        "            if s[-1] == '\\n':\n",
        "                s = s[:-1]\n",
        "            elif s[0] == '\\t':\n",
        "                s = s[1:]\n",
        "        else:\n",
        "            print(\"No keys\")\n",
        "\n",
        "        return s\n",
        "\n",
        "    @staticmethod\n",
        "    def char_acc(t, o):\n",
        "        # Calculates character-level accuracy between predicted and target tensors\n",
        "        with torch.no_grad():\n",
        "            se = [[o[i][j].item() == t[i][j].item() for j in range(t.shape[1])] for i in range(t.shape[0])]\n",
        "            c = np.sum(se)\n",
        "            tc = np.sum([len(row) for row in se])\n",
        "        return c / tc\n",
        "\n",
        "    @staticmethod\n",
        "    def word_acc(t, o):\n",
        "        # Calculates word-level accuracy between predicted and target tensors\n",
        "        o1 = torch.argmax(o, dim=1)\n",
        "        with torch.no_grad():\n",
        "            c = sum([(o1[i] == t[i]).sum().item() == t.shape[1] for i in range(t.shape[0])])\n",
        "        return c / t.shape[0]\n",
        "\n",
        "    @staticmethod\n",
        "    def sample_equidi_pts(d, e):\n",
        "        # Samples equidistant points from a given list\n",
        "        s = len(d) // e\n",
        "        i = np.arange(0, len(d), s)\n",
        "        p = [d[j] for j in i]\n",
        "        return p\n",
        "\n",
        "\n",
        "HF = Helper_Functions()\n",
        "\n",
        "# Creating a dictionary of helper functions\n",
        "variables = {\n",
        "    'asMinutes': HF.Time,\n",
        "    'timeSince': HF.Span,\n",
        "    'key_printing': HF.key_printing,\n",
        "    'char_acc': HF.char_acc,\n",
        "    'word_acc': HF.word_acc,\n",
        "    'sample_equidi_pts': HF.sample_equidi_pts\n",
        "}\n",
        "\n",
        "# Accessing the variables\n",
        "asMinutes = variables['asMinutes']\n",
        "timeSince = variables['timeSince']\n",
        "key_printing = variables['key_printing']\n",
        "char_acc = variables['char_acc']\n",
        "word_acc = variables['word_acc']\n",
        "sample_equidi_pts = variables['sample_equidi_pts']\n",
        "\n",
        "# Usage\n",
        "print(asMinutes)  # Prints the formatted time in minutes and seconds\n",
        "print(timeSince)  # Prints the elapsed time and estimated remaining time\n",
        "print(key_printing)  # Prints keys from a dictionary that have specific values\n",
        "print(char_acc)  # Calculates and returns character-level accuracy\n",
        "print(word_acc)  # Calculates and returns word-level accuracy\n",
        "print(sample_equidi_pts)  # Samples equidistant points from a given list\n"
      ],
      "metadata": {
        "id": "yMtxX6Pfv-1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dictionary of variables and their corresponding characters\n",
        "variables = {\n",
        "    's_t_c_r': '\\t',     # Represents the tab character\n",
        "    'e_d_c_r': '\\n',       # Represents the newline character\n",
        "    'b_k_c_r': ' ',      # Represents a blank space character\n",
        "    'u_n_c_r': '\\r'    # Represents a carriage return character\n",
        "}\n",
        "\n",
        "# Assign the value of 's_t_c_r' from the 'variables' dictionary to the variable 's_t_c_r'\n",
        "s_t_c_r = variables['s_t_c_r']\n",
        "\n",
        "# Assign the value of 'e_d_c_r' from the 'variables' dictionary to the variable 'e_d_c_r'\n",
        "e_d_c_r = variables['e_d_c_r']\n",
        "\n",
        "# Assign the value of 'b_k_c_r' from the 'variables' dictionary to the variable 'b_k_c_r'\n",
        "b_k_c_r = variables['b_k_c_r']\n",
        "\n",
        "# Assign the value of 'u_n_c_r' from the 'variables' dictionary to the variable 'u_n_c_r'\n",
        "u_n_c_r = variables['u_n_c_r']\n"
      ],
      "metadata": {
        "id": "DS5XcLIToSUz",
        "execution": {
          "iopub.status.busy": "2023-05-21T05:31:38.074939Z",
          "iopub.execute_input": "2023-05-21T05:31:38.075348Z",
          "iopub.status.idle": "2023-05-21T05:31:38.080865Z",
          "shell.execute_reply.started": "2023-05-21T05:31:38.075311Z",
          "shell.execute_reply": "2023-05-21T05:31:38.079611Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv  # Import the csv module\n",
        "import torch  # Import the torch module\n",
        "\n",
        "class DataPreProcessor:\n",
        "    def __init__(self):\n",
        "        self.file_paths = {\n",
        "            'file1': '/content/drive/MyDrive/aksharantar_sampled/mar/mar_test.csv',\n",
        "            'file2': '/content/drive/MyDrive/aksharantar_sampled/mar/mar_train.csv',\n",
        "            'file3': '/content/drive/MyDrive/aksharantar_sampled/mar/mar_train.csv'\n",
        "        }\n",
        "\n",
        "        self.variables = {\n",
        "            's_t_c_r': '\\t',      # Represents the tab character\n",
        "            'e_d_c_r': '\\n',        # Represents the newline character\n",
        "            'b_k_c_r': ' ',       # Represents a blank space character\n",
        "            'u_n_c_r': '\\r'     # Represents a carriage return character\n",
        "        }\n",
        "\n",
        "    def read_csv_files(self):\n",
        "        fr = {}  # Dictionary to store csv reader objects\n",
        "        hd = {}  # Dictionary to store headers of each file\n",
        "        dt = {}  # Dictionary to store data from each file\n",
        "\n",
        "        for key, fp in self.file_paths.items():\n",
        "            file = open(fp)  # Open the file with the given file path\n",
        "            csv_reader = csv.reader(file)  # Create a csv reader object\n",
        "            fr[key] = csv_reader  # Store the csv reader object in the dictionary\n",
        "            hd[key] = next(csv_reader)  # Retrieve the headers from the csv reader\n",
        "            dt[key] = []  # Initialize an empty list to store the data\n",
        "\n",
        "            for row in csv_reader:\n",
        "                dt[key].append(row)  # Append each row of data to the corresponding list\n",
        "\n",
        "            file.close()  # Close the file after reading\n",
        "\n",
        "        # Return the headers and data from each file\n",
        "        return hd['file1'], hd['file2'], hd['file3'], dt['file1'], dt['file2'], dt['file3']\n",
        "\n",
        "    def Reading_Data(self, lst):\n",
        "        i = [pair[0] for pair in lst]  # Extract the first element from each pair in the given list\n",
        "        t = [pair[1] for pair in lst]  # Extract the second element from each pair in the given list\n",
        "        return i, t  # Return the extracted first elements as 'i' and second elements as 't'\n",
        "\n",
        "\n",
        "    def Dict_lang(self, inputs, targets):\n",
        "        i_dict = {}         # Dictionary to store character-to-index mapping for inputs\n",
        "        max_i_length = 0    # Variable to track maximum input string length\n",
        "        i_char = []         # List to store unique characters in inputs\n",
        "\n",
        "        t_dict = {}         # Dictionary to store character-to-index mapping for targets\n",
        "        max_t_length = 0    # Variable to track maximum target string length\n",
        "        t_char = []         # List to store unique characters in targets\n",
        "\n",
        "        # Encoding Inputs and updating i_dict\n",
        "        for string in inputs:\n",
        "            max_i_length = max(len(string), max_i_length)  # Update maximum input string length\n",
        "            for char in string:\n",
        "                if char not in i_dict:\n",
        "                    i_dict[char] = len(i_char)   # Assign a unique index to each unique character\n",
        "                    i_char.append(char)         # Store the unique character in the list\n",
        "\n",
        "        if self.variables['b_k_c_r'] not in i_dict:\n",
        "            i_dict[self.variables['b_k_c_r']] = len(i_char)   # Assign index to the blank character if not present\n",
        "            i_char.append(self.variables['b_k_c_r'])\n",
        "\n",
        "        i_dict[self.variables['u_n_c_r']] = len(i_char)     # Assign index to the unknown character\n",
        "        i_char.append(self.variables['u_n_c_r'])\n",
        "\n",
        "        if self.variables['s_t_c_r'] not in t_dict:\n",
        "            t_dict[self.variables['s_t_c_r']] = len(t_char)   # Assign index to the start character if not present\n",
        "            t_char.append(self.variables['s_t_c_r'])\n",
        "\n",
        "        # Encoding Targets and updating t_dict\n",
        "        for string in targets:\n",
        "            max_t_length = max(len(string) + 2, max_t_length)    # Update maximum target string length\n",
        "            for char in string:\n",
        "                if char not in t_dict:\n",
        "                    t_dict[char] = len(t_char)   # Assign a unique index to each unique character\n",
        "                    t_char.append(char)         # Store the unique character in the list\n",
        "\n",
        "        if self.variables['e_d_c_r'] not in t_dict:\n",
        "            t_dict[self.variables['e_d_c_r']] = len(t_char)     # Assign index to the end character if not present\n",
        "            t_char.append(self.variables['e_d_c_r'])\n",
        "\n",
        "        if self.variables['b_k_c_r'] not in t_dict:\n",
        "            t_dict[self.variables['b_k_c_r']] = len(t_char)   # Assign index to the blank character if not present\n",
        "            t_char.append(self.variables['b_k_c_r'])\n",
        "\n",
        "        return i_dict, max_i_length, i_char, t_dict, max_t_length, t_char\n",
        "\n",
        "    def encoding_words(self, wl, ld, ml, l):\n",
        "        encds = []   # List to store the encoded sequences\n",
        "        for w in wl:\n",
        "            encd = [ld[c] if c in ld else ld[self.variables['u_n_c_r']] for c in w]\n",
        "            # Assign the index of each character in the word, or the index of unknown character if not present\n",
        "\n",
        "            if l == 0:\n",
        "                encd.extend([ld[self.variables['b_k_c_r']]] * (ml - len(encd)))\n",
        "                # If 'l' is 0 (inputs), pad the sequence with blank character indices up to the maximum length\n",
        "            if l == 1:\n",
        "                encd = [ld[self.variables['s_t_c_r']]] + encd + [ld[self.variables['e_d_c_r']]]\n",
        "                encd.extend([ld[self.variables['b_k_c_r']]] * (ml - len(encd)))\n",
        "                # If 'l' is 1 (targets), add start and end character indices to the sequence and pad it with blank character indices up to the maximum length\n",
        "\n",
        "            encds.append(encd)  # Append the encoded sequence to the list\n",
        "\n",
        "        return encds  # Return the list of encoded sequences\n",
        "\n",
        "\n",
        "    def Tokenzieations(self, train, val, test, input_dict, target_dict, max_input_length, max_target_length):\n",
        "        t_i, t_t = self.Reading_Data(train)  # Reading train data\n",
        "        te_i, te_t = self.Reading_Data(test)  # Reading test data\n",
        "        v_i, v_t = self.Reading_Data(val)  # Reading validation data\n",
        "\n",
        "        e_t_i = self.encoding_words(t_i, input_dict, max_input_length, 0)  # Encoding train inputs\n",
        "        e_t_t = self.encoding_words(t_t, target_dict, max_target_length, 1)  # Encoding train targets\n",
        "        e_v_i = self.encoding_words(v_i, input_dict, max_input_length, 0)  # Encoding validation inputs\n",
        "        e_v_t = self.encoding_words(v_t, target_dict, max_target_length, 1)  # Encoding validation targets\n",
        "        e_te_i = self.encoding_words(te_i, input_dict, max_input_length, 0)  # Encoding test inputs\n",
        "        e_te_t = self.encoding_words(te_t, target_dict, max_target_length, 1)  # Encoding test targets\n",
        "\n",
        "        return {\n",
        "            'en_tr_ip': e_t_i,\n",
        "            'en_tr_tr': e_t_t,\n",
        "            'en_vl_ip': e_v_i,\n",
        "            'en_vl_tr': e_v_t,\n",
        "            'en_tt_ip': e_te_i,\n",
        "            'en_tt_tr': e_te_t\n",
        "        }\n",
        "\n",
        "    def tensor_pair_conversion(self, tt_ip, tt_tr):\n",
        "        pairs = [(torch.tensor(input_data), torch.tensor(target_data)) for input_data, target_data in\n",
        "                zip(tt_ip, tt_tr)]  # Creating pairs of tensor inputs and targets\n",
        "        return pairs\n",
        "\n",
        "DPP = DataPreProcessor()\n",
        "\n",
        "# Dictionary of functions\n",
        "function_dict = {\n",
        "    'tensor_pair_conversion': DPP.tensor_pair_conversion,\n",
        "    'Tokenzieations': DPP.Tokenzieations,\n",
        "    'encoding_words': DPP.encoding_words,\n",
        "    'Reading_Data': DPP.Reading_Data,\n",
        "    'Dict_lang': DPP.Dict_lang,\n",
        "    'read_csv_files': DPP.read_csv_files\n",
        "}\n",
        "\n",
        "tensor_pair_conversion = function_dict['tensor_pair_conversion']\n",
        "Tokenzieations = function_dict['Tokenzieations']\n",
        "encoding_words = function_dict['encoding_words']\n",
        "Reading_Data = function_dict['Reading_Data']\n",
        "Dict_lang = function_dict['Dict_lang']\n",
        "read_csv_files = function_dict['read_csv_files']\n",
        "\n",
        "# Call the function and assign the returned values to variables\n",
        "h1, h2, h3, test, val, train = read_csv_files()\n",
        "\n",
        "tt_ip, tt_tr = Reading_Data(train)  # Reading train data\n",
        "tst_ip, tst_tr = Reading_Data(test)  # Reading test data\n",
        "vl_ip, vl_tr = Reading_Data(val)  # Reading validation data\n",
        "\n",
        "print(tt_ip[1])  # Print train input at index 1\n",
        "print(tt_tr[1])  # Print train target at index 1\n",
        "\n",
        "input_dict, max_input_length, input_char, target_dict, max_target_length, target_char = Dict_lang(tt_ip + vl_ip + tst_ip, tt_tr + vl_tr + tst_tr)  # Generating dictionaries for inputs and targets\n",
        "\n",
        "result = Tokenzieations(train, val, test, input_dict, target_dict, max_input_length, max_target_length)  # Tokenizing the data\n",
        "\n",
        "en_tr_ip = result['en_tr_ip']  # Encoded train inputs\n",
        "en_tr_tr = result['en_tr_tr']  # Encoded train targets\n",
        "en_vl_ip = result['en_vl_ip']  # Encoded validation inputs\n",
        "en_vl_tr = result['en_vl_tr']  # Encoded validation targets\n",
        "en_tt_ip = result['en_tt_ip']  # Encoded test inputs\n",
        "en_tt_tr = result['en_tt_tr']  # Encoded test targets\n",
        "\n",
        "r = random.randint(0,100)  # Generate a random number between 0 and 100\n",
        "print(key_printing(input_dict, en_tr_ip[int(r)]))  # Print the keys corresponding to the values in input_dict for the randomly selected encoded train input\n",
        "print(key_printing(target_dict, en_tr_tr[int(r)]))  # Print the keys corresponding to the values in target_dict for the randomly selected encoded train target\n",
        "\n",
        "en_tt_pa = tensor_pair_conversion(en_tr_ip, en_tr_tr)  # Convert encoded train inputs and targets into pairs of tensors\n",
        "en_vl_pa = tensor_pair_conversion(en_vl_ip, en_vl_tr)  # Convert encoded validation inputs and targets into pairs of tensors\n",
        "en_tst_pa = tensor_pair_conversion(en_tt_ip, en_tt_tr)  # Convert encoded test inputs and targets into pairs of tensors\n",
        "\n",
        "pairs = (en_tt_pa, en_vl_pa, en_tst_pa)  # Combine pairs into a tuple\n",
        "pair = random.choice(en_tt_pa)  # Randomly select a pair from the encoded train pairs\n",
        "\n",
        "print(key_printing(input_dict, pair[0]))  # Print the keys corresponding to the values in input_dict for the selected pair input\n",
        "print(key_printing(target_dict, pair[1]))  # Print the keys corresponding to the values in target_dict for the selected pair target\n"
      ],
      "metadata": {
        "id": "hVm6ME71IQEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Recurrent_NN_Encoder(nn.Module):\n",
        "    def __init__(self, device, cell_type, vocab_size, embed_dim, hidden_size, num_layers=1, bidirectional=False, dropout_p=0):\n",
        "        super(Recurrent_NN_Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # Embedding layer for converting input indices to dense vectors\n",
        "        self.hidden_size = hidden_size  # Size of the hidden state in the RNN\n",
        "        self.num_layers = num_layers  # Number of layers in the RNN\n",
        "        self.bidirectional = bidirectional  # Flag indicating whether the RNN is bidirectional or not\n",
        "        self.cell_type = cell_type  # Type of RNN cell ('lstm', 'rnn', or 'gru')\n",
        "        self.dropout_p = dropout_p  # Dropout probability\n",
        "        self.dropout = nn.Dropout(self.dropout_p)  # Dropout layer for regularization\n",
        "        \n",
        "        # Initialize the RNN cell based on the specified cell type\n",
        "        if cell_type == 'lstm':\n",
        "            self.rnn = nn.LSTM(embed_dim, hidden_size, num_layers=self.num_layers, batch_first=True, dropout=dropout_p, bidirectional=self.bidirectional)\n",
        "        elif cell_type == 'rnn':\n",
        "            self.rnn = nn.RNN(embed_dim, hidden_size, num_layers=self.num_layers, batch_first=True, dropout=dropout_p, bidirectional=self.bidirectional)\n",
        "        elif cell_type == 'gru':\n",
        "            self.rnn = nn.GRU(embed_dim, hidden_size, num_layers=self.num_layers, batch_first=True, dropout=dropout_p, bidirectional=self.bidirectional)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        out = self.embedding(x)  # Perform embedding lookup to get dense representations of input indices\n",
        "        out = self.dropout(out)  # Apply dropout to the input embeddings\n",
        "        if self.cell_type == 'lstm':\n",
        "            out, (hidden, cell) = self.rnn(out, (hidden, cell))  # Forward pass through the LSTM\n",
        "            return out, hidden, cell\n",
        "        elif self.cell_type == 'rnn':\n",
        "            out, hidden = self.rnn(out, hidden)  # Forward pass through the RNN\n",
        "            return out, hidden\n",
        "        elif self.cell_type == 'gru':\n",
        "            out, hidden = self.rnn(out, hidden)  # Forward pass through the GRU\n",
        "            return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        # Initialize the hidden and cell states with random values\n",
        "        hidden = torch.randn((1 + int(self.bidirectional)) * self.num_layers, batch_size, self.hidden_size, device=device)\n",
        "        cell = torch.randn((1 + int(self.bidirectional)) * self.num_layers, batch_size, self.hidden_size, device=device)\n",
        "        return hidden, cell\n",
        "\n",
        "\n",
        "class Recurrent_NN_Decoder(nn.Module):\n",
        "    def __init__(self, device, cell_type, output_vocab, embed_size, hidden_size, max_length, dropout_p=0.1, num_layers=1, bidirectional=False):\n",
        "        super(Recurrent_NN_Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size  # Hidden size of the decoder\n",
        "        self.output_size = output_vocab  # Size of the output vocabulary\n",
        "        self.embed_size = embed_size  # Size of the input embedding\n",
        "        self.dropout_p = dropout_p  # Dropout probability\n",
        "        self.cell_type = cell_type  # Type of RNN cell (lstm, gru, rnn)\n",
        "        self.max_length = max_length  # Maximum length of input sequence\n",
        "        self.device = device  # Device (e.g., CPU or GPU) to be used for computations\n",
        "        self.num_layers = num_layers  # Number of layers in the decoder\n",
        "        self.embedding_decoder = nn.Embedding(self.output_size, self.embed_size)  # Embedding layer for the decoder\n",
        "        self.dropout = nn.Dropout(self.dropout_p)  # Dropout layer\n",
        "        self.bidirectional = bidirectional  # Flag indicating if the encoder is bidirectional\n",
        "\n",
        "        # Determine the type of RNN cell based on the given cell_type\n",
        "        if cell_type == 'lstm':\n",
        "            self.rnn = nn.LSTM(self.embed_size, hidden_size, num_layers=self.num_layers, batch_first=True, bidirectional=self.bidirectional, dropout=self.dropout_p)\n",
        "        elif cell_type == 'gru':\n",
        "            self.rnn = nn.GRU(self.embed_size, hidden_size, num_layers=self.num_layers, batch_first=True, bidirectional=self.bidirectional, dropout=self.dropout_p)\n",
        "        elif cell_type == 'rnn':\n",
        "            self.rnn = nn.RNN(self.embed_size, hidden_size, num_layers=self.num_layers, batch_first=True, bidirectional=self.bidirectional, dropout=self.dropout_p)\n",
        "\n",
        "        self.out = nn.Linear((1 + int(self.bidirectional)) * self.hidden_size, self.output_size)  # Linear layer for output prediction\n",
        "        self.out_activation = nn.LogSoftmax(dim=-1)  # Log softmax activation for output probabilities\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        input = input.unsqueeze(1)  # Add a singleton dimension to the input tensor\n",
        "        embedded_decoder = self.embedding_decoder(input)  # Apply embedding to the input\n",
        "        embedded_decoder = self.dropout(embedded_decoder)  # Apply dropout to the embedded input\n",
        "\n",
        "        # Pass the embedded input through the RNN cell\n",
        "        if self.cell_type == 'lstm':\n",
        "            output, (hidden, cell) = self.rnn(embedded_decoder, (hidden, cell))\n",
        "        elif self.cell_type == 'gru':\n",
        "            output, hidden = self.rnn(embedded_decoder, hidden)\n",
        "        elif self.cell_type == 'rnn':\n",
        "            output, hidden = self.rnn(embedded_decoder, hidden)\n",
        "\n",
        "        output = F.relu(self.out(output))  # Apply ReLU activation to the output\n",
        "        output = F.log_softmax(output, dim=-1)  # Apply log softmax activation to obtain output probabilities\n",
        "\n",
        "        return output, hidden, cell  # Return the output, hidden state, and cell state\n",
        "\n",
        "    def init_hidden(self, encoder_hidden, encoder_cell, encoder_bidirectional):\n",
        "        hidden = encoder_hidden[-(1 + int(encoder_bidirectional)): ].repeat(self.num_layers, 1, 1)  # Initialize the hidden state using the encoder's hidden state\n",
        "        cell = encoder_cell[-(1 + int(encoder_bidirectional)): ].repeat(self.num_layers, 1, 1)  # Initialize the cell state using the encoder's cell state\n",
        "        return hidden, cell  # Return the initialized hidden state and cell state\n",
        "\n",
        "\n",
        "\n",
        "class Seqence_2_Seqence_Network(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder  # Initialize the encoder\n",
        "        self.decoder = decoder  # Initialize the decoder\n",
        "        self.device = device  # Store the device (e.g., CPU or GPU)\n",
        "        self.max_target_length = 0  # Variable to store the maximum target sequence length\n",
        "        self.sos = 0  # Start-of-sequence token\n",
        "        \n",
        "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "        batch_size = target.shape[0]  # Get the batch size\n",
        "        target_len = target.shape[1]  # Get the target sequence length\n",
        "        self.max_target_length = target_len  # Update the maximum target sequence length\n",
        "        target_vocab_size = self.decoder.output_size  # Get the target vocabulary size\n",
        "        \n",
        "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)  # Initialize the outputs tensor\n",
        "        \n",
        "        encoder_hidden, encoder_cell = self.encoder.init_hidden(batch_size)  # Initialize the encoder hidden state and cell state\n",
        "        \n",
        "        if (self.encoder.cell_type == 'lstm'):\n",
        "            encoder_outputs, encoder_hidden, encoder_cell = self.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "            # Forward pass through the encoder LSTM\n",
        "        if (self.encoder.cell_type == 'rnn'):\n",
        "            encoder_outputs, encoder_hidden = self.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "            # Forward pass through the encoder RNN\n",
        "        if (self.encoder.cell_type == 'gru'):\n",
        "            encoder_outputs, encoder_hidden = self.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "            # Forward pass through the encoder GRU\n",
        "        \n",
        "        input = target[:, 0]  # Set the first input to the decoder as the <sos> token\n",
        "        self.sos = target[:, 0]  # Store the <sos> token\n",
        "        \n",
        "        hidden, cell = self.decoder.init_hidden(encoder_hidden, encoder_cell, self.encoder.bidirectional)\n",
        "        # Initialize the decoder hidden state and cell state\n",
        "        \n",
        "        for t in range(1, target_len):\n",
        "            output, hidden, cell = self.decoder.forward(input, hidden, cell)\n",
        "            # Forward pass through the decoder\n",
        "            outputs[:, t] = output.squeeze(1)  # Store the decoder output in the outputs tensor\n",
        "            teacher_force = random.random() < teacher_forcing_ratio  # Determine whether to use teacher forcing\n",
        "            top1 = output.argmax(-1)  # Get the index of the highest probability output\n",
        "            input = target[:, t] if teacher_force else top1.squeeze(1)  # Set the next input to the decoder\n",
        "            \n",
        "        return outputs\n",
        "    \n",
        "    def inference(self, source, target):\n",
        "        batch_size = source.shape[0]  # Get the batch size\n",
        "        target_len = self.max_target_length  # Get the maximum target sequence length\n",
        "        target_vocab_size = self.decoder.output_size  # Get the target vocabulary size\n",
        "        \n",
        "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)  # Initialize the outputs tensor\n",
        "        \n",
        "        encoder_hidden, encoder_cell = self.encoder.init_hidden(batch_size)  # Initialize the encoder hidden state and cell state\n",
        "        \n",
        "        if (self.encoder.cell_type == 'lstm'):\n",
        "            encoder_outputs, encoder_hidden, encoder_cell = self.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "            # Forward pass through the encoder LSTM\n",
        "        if (self.encoder.cell_type == 'rnn'):\n",
        "            encoder_outputs, encoder_hidden = self.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "            # Forward pass through the encoder RNN\n",
        "        if (self.encoder.cell_type == 'gru'):\n",
        "            encoder_outputs, encoder_hidden = self.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "            # Forward pass through the encoder GRU\n",
        "        \n",
        "        input = self.sos  # Set the first input to the <sos> token\n",
        "        \n",
        "        hidden, cell = self.decoder.init_hidden(encoder_hidden, encoder_cell, self.encoder.bidirectional)\n",
        "        # Initialize the decoder hidden state and cell state\n",
        "        \n",
        "        for t in range(1, target_len):\n",
        "            output, hidden, cell = self.decoder.forward(input, hidden, cell)\n",
        "            # Forward pass through the decoder\n",
        "            outputs[:, t] = output.squeeze(1)  # Store the decoder output in the outputs tensor\n",
        "            top1 = output.argmax(-1)  # Get the index of the highest probability output\n",
        "            input = top1.squeeze(1)  # Set the next input to the decoder\n",
        "            \n",
        "        return outputs\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UBE07BLxae-0",
        "execution": {
          "iopub.status.busy": "2023-05-21T05:31:46.924970Z",
          "iopub.execute_input": "2023-05-21T05:31:46.925567Z",
          "iopub.status.idle": "2023-05-21T05:31:46.949929Z",
          "shell.execute_reply.started": "2023-05-21T05:31:46.925519Z",
          "shell.execute_reply": "2023-05-21T05:31:46.948659Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vanilla_model(input_dict, target_dict):\n",
        "    variables = {}\n",
        "    \n",
        "    # Count the number of elements in the input and target dictionaries\n",
        "    variables['a'] = len(input_dict)\n",
        "    variables['b'] = len(target_dict)\n",
        "    \n",
        "    # Set values for variables c, d, e, f, g, h, i, j, k, and max_length\n",
        "    variables['c'] = 32    #batch size\n",
        "    variables['d'] = 32     #val batch size\n",
        "    variables['e'] = 256    #enc_embedding\n",
        "    variables['f'] = 256    #dec embedding\n",
        "    variables['g'] = 512    #hidden_size \n",
        "    variables['h'] = 3     #enc_number_layers\n",
        "    variables['i'] = 2     #dec_num_layers\n",
        "    variables['j'] = 0.4    #enc_dropout\n",
        "    variables['k'] = 0.4    #dec_drop_out\n",
        "    variables['max_length'] = max_target_length\n",
        "    \n",
        "    # Set value for variable l\n",
        "    variables['l'] = 'gru'\n",
        "    \n",
        "    # Create an instance of Recurrent_NN_Encoder and assign it to variable m\n",
        "    variables['m'] = Recurrent_NN_Encoder(device, variables['l'], variables['a'], variables['e'], variables['g'],\n",
        "                                variables['h'], bidirectional=True, dropout_p=variables['j'])\n",
        "    \n",
        "    # Create an instance of Recurrent_NN_Decoder and assign it to variable n\n",
        "    variables['n'] = Recurrent_NN_Decoder(device, variables['l'], variables['b'], variables['f'], variables['g'],\n",
        "                                variables['max_length'], variables['k'], variables['i'], bidirectional=True)\n",
        "    \n",
        "    # Create an instance of Seqence_2_Seqence_Network by passing in the encoder and decoder instances, and assign it to variable o\n",
        "    variables['o'] = Seqence_2_Seqence_Network(variables['m'], variables['n'], device).to(device)\n",
        "    \n",
        "    # Define a function count_parameters(model) that returns the total number of trainable parameters in a model\n",
        "    def count_parameters(model):\n",
        "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "    # Print the number of trainable parameters in the Seqence_2_Seqence_Network model\n",
        "    print(f' {count_parameters(variables[\"o\"]):,} trainable parameters')\n",
        "    \n",
        "    # Create an instance of Adam optimizer and assign it to variable p\n",
        "    variables['p'] = torch.optim.Adam(variables['o'].parameters(), lr=0.001)\n",
        "    \n",
        "    # Create an instance of negative log likelihood loss and assign it to variable q\n",
        "    variables['q'] = nn.NLLLoss()\n",
        "    \n",
        "    # Return the Seqence_2_Seqence_Network model, optimizer, and criterion\n",
        "    return variables['o'], variables['p'], variables['q']\n",
        "\n",
        "# Call the vanilla_model function with input_dict and target_dict, and assign the returned values to model, optimizer, and criterion respectively\n",
        "model, optimizer, criterion = vanilla_model(input_dict, target_dict)\n"
      ],
      "metadata": {
        "id": "LTsT62M9BrDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_function(model, pairs, batch_size, n_iters, optimizer, tf, print_every=10, plot_every=10, log = True, Attention = False):\n",
        "\n",
        "def train_function(m, p, b, n, o, t, p_e=10, p_v=10, l=True, A=False):\n",
        "    s = time.time()  # Starting time\n",
        "\n",
        "    p_l = []  # List to store training loss\n",
        "    t_w_a = []  # List to store word-level accuracy on training set\n",
        "    v_w_a = []  # List to store word-level accuracy on validation set\n",
        "    p_l_t = 0  # Training loss per iteration\n",
        "    p_l_v_t = 0  # Validation loss per iteration\n",
        "\n",
        "    t_p = p[0]  # Training data\n",
        "    v_p = p[1]  # Validation data\n",
        "    t_a = 0  # Accumulated word-level accuracy on training set\n",
        "\n",
        "    c = nn.NLLLoss()  # Negative log likelihood loss\n",
        "\n",
        "    count = 0  # Iteration count\n",
        "    for it in range(1, n + 1):  # Number of iterations\n",
        "        for i in range(0, len(t_p) - b, b):  # Batch processing\n",
        "            t_a = 0  # Reset accumulated word-level accuracy\n",
        "            count += 1  # Increment iteration count\n",
        "\n",
        "            if i + b > len(t_p):\n",
        "                b = len(t_p) - i + 1  # Adjust batch size if remaining samples are less than the batch size\n",
        "\n",
        "            i_t = torch.stack([t_p[i + j][0] for j in range(b)]).squeeze(1).long().cuda()  # Input tensor\n",
        "            t_t = torch.stack([t_p[i + j][1] for j in range(b)]).squeeze(1).long().cuda()  # Target tensor\n",
        "\n",
        "            o.zero_grad()  # Clear gradients\n",
        "            out = m(i_t, t_t, teacher_forcing_ratio=t if count < 4000 else 0)  # Forward pass\n",
        "            out = torch.permute(out, [0, 2, 1])  # Permute output tensor dimensions\n",
        "            l = c(out, t_t)  # Calculate loss\n",
        "\n",
        "            t_a_w = word_acc(t_t, out) * b  # Calculate word-level accuracy on training set\n",
        "            t_a += t_a_w  # Accumulate word-level accuracy\n",
        "            l.backward()  # Backward pass\n",
        "            torch.nn.utils.clip_grad_norm_(m.parameters(), 1)  # Clip gradients to avoid exploding gradients\n",
        "            o.step()  # Update model parameters\n",
        "\n",
        "            p_l_t += l  # Accumulate training loss per iteration\n",
        "            p_l_v_t += l  # Accumulate validation loss per iteration\n",
        "\n",
        "            if count % 800 == 0:\n",
        "                v_i_t = torch.stack([v_p[j][0] for j in range(b)]).squeeze(1).long().cuda()  # Validation input tensor\n",
        "                v_t_t = torch.stack([v_p[j][1] for j in range(b)]).squeeze(1).long().cuda()  # Validation target tensor\n",
        "\n",
        "                if A:\n",
        "                    v_o, _ = m.inference(v_i_t, v_t_t)\n",
        "                else:\n",
        "                    v_o = m.inference(v_i_t, v_t_t)\n",
        "\n",
        "                v_o = v_o.permute(0, 2, 1)  # Permute validation output tensor dimensions\n",
        "                v_l = c(v_o, v_t_t)  # Calculate validation loss\n",
        "                v_l_s = v_l\n",
        "                wandb.log({'Val_Loss': v_l_s})  # Log validation loss\n",
        "\n",
        "            if count % 800 == 0:\n",
        "                p_l_a = p_l_t / 800  # Average training loss over 800 iterations\n",
        "                p_l_t = 0  # Reset training loss accumulator\n",
        "                print('%s (%d %d%%) %.7f' % (timeSince(s, it / n), it, it / n * 100, p_l_a))  # Print progress\n",
        "\n",
        "            if count % 800 == 0:\n",
        "                p_l_a = p_l_v_t / 800  # Average validation loss over 800 iterations\n",
        "                p_l.append(p_l_a.detach())  # Append training loss to list\n",
        "                wandb.log({'Train Loss': p_l_a})  # Log training loss\n",
        "                p_l_v_t = 0  # Reset validation loss accumulator\n",
        "\n",
        "        t_a = t_a / (len(t_p) - b)  # Calculate average word-level accuracy on training set\n",
        "        t_w_a.append(t_a)  # Append word-level accuracy to list\n",
        "\n",
        "    print(t_w_a)  # Print word-level accuracy on training set\n",
        "\n",
        "    p_l = [l.cpu().numpy() for l in p_l]  # Convert training loss to numpy array\n",
        "    p_l_s = sample_equidi_pts(p_l, n)  # Sample equidistant points from the training loss\n",
        "\n",
        "    w_c = 0  # Total correct predictions\n",
        "    for i in range(0, len(v_p) - b, b):  # Batch processing on validation set\n",
        "        if i + b > len(v_p):\n",
        "            b = len(v_p) - i + 1  # Adjust batch size if remaining samples are less than the batch size\n",
        "\n",
        "        v_i_t = torch.stack([v_p[i + j][0] for j in range(b)]).squeeze(1).long().cuda()  # Validation input tensor\n",
        "        v_t_t = torch.stack([v_p[i + j][1] for j in range(b)]).squeeze(1).long().cuda()  # Validation target tensor\n",
        "\n",
        "        if A:\n",
        "            v_o, _ = m.inference(v_i_t, v_t_t)\n",
        "        else:\n",
        "            v_o = m.inference(v_i_t, v_t_t)\n",
        "\n",
        "        v_o = v_o.permute(0, 2, 1)  # Permute validation output tensor dimensions\n",
        "        v_l = c(v_o, v_t_t)  # Calculate validation loss\n",
        "\n",
        "        v_a_w = word_acc(v_t_t, v_o)  # Calculate word-level accuracy on validation set\n",
        "        w_c += v_a_w * b  # Accumulate correct predictions\n",
        "\n",
        "    w_a = w_c / (len(v_p) - b)  # Average word-level accuracy on validation set\n",
        "    m = {'Val_Accuracy': w_a}  # Log validation accuracy\n",
        "    wandb.log(m)\n",
        "\n",
        "    print(f\"Val loss = {v_l}\")  # Print validation loss\n",
        "    print(f'Word-level-accuracy on val set = {w_a}')  # Print word-level accuracy on validation set\n"
      ],
      "metadata": {
        "id": "3_VZGr2LQ4Lk",
        "execution": {
          "iopub.status.busy": "2023-05-21T05:32:02.467407Z",
          "iopub.execute_input": "2023-05-21T05:32:02.467856Z",
          "iopub.status.idle": "2023-05-21T05:32:07.376445Z",
          "shell.execute_reply.started": "2023-05-21T05:32:02.467816Z",
          "shell.execute_reply": "2023-05-21T05:32:07.375225Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the sweep configuration\n",
        "sw = {\n",
        "    'method': 'bayes',\n",
        "    'name': 'Hyperparameter Tuning-Bayesian'\n",
        "}\n",
        "\n",
        "# Defining the metric configuration\n",
        "m = {\n",
        "    'name': 'Val_Accuracy',\n",
        "    'goal': 'maximize'\n",
        "}\n",
        "\n",
        "# Adding the metric configuration to the sweep configuration\n",
        "sw['metric'] = m\n",
        "\n",
        "# Defining the parameters configuration\n",
        "p = {\n",
        "    'optimiser': {'values': ['nadam']},\n",
        "    'teacher_forcing_ratio': {'values': [0.7]},\n",
        "    'bidirectional': {'values': [True]},\n",
        "    'enc_embedding': {'values': [128]},\n",
        "    'dec_embedding': {'values': [128]},\n",
        "    'epochs': {'values': [1]},\n",
        "    'hidden_size': {'values': [512]},\n",
        "    'enc_layers': {'values': [3]},\n",
        "    'dec_layers': {'values': [3]},\n",
        "    'dropout': {'values': [0.3]},\n",
        "    'cell_type': {'values': ['lstm']}\n",
        "}\n",
        "\n",
        "# Adding the parameters configuration to the sweep configuration\n",
        "sw['parameters'] = p\n",
        "\n",
        "# Creating a sweep using the sweep configuration and assigning it to sw_id\n",
        "sw_id = wandb.sweep(sw, project=\"CS6910 Assignment 3\")\n",
        "\n",
        "# Function to train the sweep\n",
        "def train_sweep(c=None):\n",
        "    # Initializing wandb run with the given config\n",
        "    with wandb.init(config=c) as r:\n",
        "        c = wandb.config\n",
        "\n",
        "        # Creating the validation data dictionary\n",
        "        vd = {\n",
        "            'input_dim': len(input_dict),\n",
        "            'output_dim': len(target_dict),\n",
        "            'batch_size': 32,\n",
        "            'val_batch_size': 32,\n",
        "            'enc_embedding': c.enc_embedding,\n",
        "            'dec_embedding': c.dec_embedding,\n",
        "            'hidden': c.hidden_size,\n",
        "            'enc_num_layers': c.enc_layers,\n",
        "            'dec_num_layers': c.dec_layers,\n",
        "            'enc_dropout': c.dropout,\n",
        "            'dec_dropout': c.dropout,\n",
        "            'max_length': max_target_length,\n",
        "            'cell_type': c.cell_type\n",
        "        }\n",
        "\n",
        "        # Extracting values from the validation data dictionary\n",
        "        input_dim = vd['input_dim']\n",
        "        output_dim = vd['output_dim']\n",
        "        batch_size = vd['batch_size']\n",
        "        val_batch_size = vd['val_batch_size']\n",
        "        enc_embedding = vd['enc_embedding']\n",
        "        dec_embedding = vd['dec_embedding']\n",
        "        hidden = vd['hidden']\n",
        "        enc_num_layers = vd['enc_num_layers']\n",
        "        dec_num_layers = vd['dec_num_layers']\n",
        "        enc_dropout = vd['enc_dropout']\n",
        "        dec_dropout = vd['dec_dropout']\n",
        "        max_length = vd['max_length']\n",
        "        cell_type = vd['cell_type']\n",
        "\n",
        "        # Creating the encoder and decoder models\n",
        "        enc = Recurrent_NN_Encoder(device, cell_type, input_dim, enc_embedding, hidden, enc_num_layers,\n",
        "                         bidirectional=c.bidirectional, dropout_p=enc_dropout)\n",
        "        dec = Recurrent_NN_Decoder(device, cell_type, output_dim, dec_embedding, hidden, max_length, dec_dropout,\n",
        "                         dec_num_layers, bidirectional=c.bidirectional)\n",
        "        model = Seqence_2_Seqence_Network(enc, dec, device).to(device)\n",
        "\n",
        "        # Setting the experiment name based on the config\n",
        "        exp_name = f\"{c.cell_type}_e_{c.optimiser}\"\n",
        "\n",
        "        # Assigning the experiment name to the wandb run\n",
        "        wandb.run.name = exp_name\n",
        "\n",
        "        # Creating the optimizer based on the chosen optimiser\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001) if c.optimiser == 'adam' else torch.optim.NAdam(\n",
        "            model.parameters(), lr=0.001)\n",
        "\n",
        "        criterion = nn.NLLLoss()\n",
        "\n",
        "\n",
        "        # Training the model using the train_function function\n",
        "        train_function(model, pairs, 32, c.epochs, optimizer, c.teacher_forcing_ratio)\n",
        "# def train_function(model, pairs, batch_size, n_iters, optimizer, tf, print_every=10, plot_every=10, log = True, Attention = False):\n",
        "\n",
        "# Running the sweep agent with the given sweep ID and train_sweep function\n",
        "wandb.agent(sw_id, train_sweep, count=1)\n",
        "\n",
        "# Finishing the wandb run\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "-ssEjRurX-IB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}