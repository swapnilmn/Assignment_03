{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swapnilmn/Assignment_03/blob/main/Question_1_Ass_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1 (15 Marks)\n",
        "Build a RNN based seq2seq model which contains the following layers: (i) input layer for character embeddings (ii) one encoder RNN which sequentially encodes the input character sequence (Latin) (iii) one decoder RNN which takes the last state of the encoder as input and produces one output character at a time (Devanagari). \n"
      ],
      "metadata": {
        "id": "Sha-7_A_8x99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Librares"
      ],
      "metadata": {
        "id": "6o7LlMzW9ALi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QXFkT1I8tdY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "adc85e64-2495-4842-f1db-8b137761d4c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.1/205.1 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "!pip install wandb -qU\n",
        "import wandb\n",
        "wandb.login()\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load of data"
      ],
      "metadata": {
        "id": "TMNzTbAT9D3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math  # Import the math module for mathematical operations\n",
        "import time  # Import the time module for time-related functions\n",
        "import torch  # Import the torch module for working with tensors\n",
        "import numpy as np  # Import the numpy module for numerical operations\n",
        "\n",
        "def asMinutes(s):\n",
        "    # Convert seconds to minutes and seconds\n",
        "    m = math.floor(s / 60)  # Calculate the number of minutes\n",
        "    s -= m * 60  # Subtract the minutes converted to seconds from the total seconds\n",
        "    return '%dm %ds' % (m, s)  # Return the formatted string for minutes and seconds\n",
        "\n",
        "def span(since, percent):\n",
        "    # Calculate the elapsed time and estimated remaining time\n",
        "    now = time.time()  # Get the current time\n",
        "    s = now - since  # Calculate the elapsed time in seconds\n",
        "    es = s / (percent)  # Calculate the estimated total time in seconds\n",
        "    rs = es - s  # Calculate the remaining time in seconds\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))  # Return the formatted string for elapsed time and remaining time\n",
        "\n",
        "def key_presenting(dictionary, values):\n",
        "    # Find keys in the dictionary that match the given values\n",
        "    found_keys = []  # Initialize an empty list to store the found keys\n",
        "    for value in values:  # Iterate over the given values\n",
        "        for key, val in dictionary.items():  # Iterate over the dictionary items\n",
        "            if value == val:  # If the value matches the dictionary value\n",
        "                found_keys.append(key)  # Add the key to the list of found keys\n",
        "    \n",
        "    if found_keys:  # If any keys are found\n",
        "        str = ''.join(found_keys)  # Concatenate the found keys into a single string\n",
        "        if str[-1] == '\\n':  # If the string ends with a newline character\n",
        "            str = str[:-1]  # Remove the newline character from the end of the string\n",
        "        elif str[0] == '\\t':  # If the string starts with a tab character\n",
        "            str = str[1:]  # Remove the tab character from the start of the string\n",
        "    else:  # If no keys are found\n",
        "        print(\"No keys found for the given values.\")  # Print a message indicating no keys were found\n",
        "    \n",
        "    return str  # Return the string containing the found keys\n",
        "\n",
        "def char_acc(targets, outputs):\n",
        "    # Calculate character-level accuracy\n",
        "    with torch.no_grad():  # Disable gradient calculation for efficiency\n",
        "        count = 0  # Initialize a counter for correct predictions\n",
        "        total_count = 0  # Initialize a counter for total predictions\n",
        "        for i in range(targets.shape[0]):  # Iterate over the rows of the targets tensor\n",
        "            same_elements = []  # Initialize an empty list to store the comparison results\n",
        "            for j in range(targets.shape[1]):  # Iterate over the columns of the targets tensor\n",
        "                if targets[i][j] != 67 or targets[i][j] != 66:  # Check if the target value is not equal to 67 or 66\n",
        "                    same_elements.append(outputs[i][j].item() == targets[i][j].item())  # Append the comparison result to the list\n",
        "            count += np.sum(same_elements)  # Add the number of True values in the list to the count\n",
        "            total_count += len(same_elements)  # Add the total number of elements in the list to the total count\n",
        "    return count / total_count  # Return the character-level accuracy as a ratio of correct predictions to total predictions\n",
        "\n",
        "def word_acc(targets, outputs):\n",
        "    # Calculate word-level accuracy\n",
        "    outputs1 = torch.argmax(outputs, dim=1)  # Get the index of the maximum value along the second dimension of the outputs tensor\n",
        "    with torch.no_grad():  # Disable gradient calculation for efficiency\n",
        "        count = 0  # Initialize a counter for correct predictions\n",
        "        for i in range(targets.shape[0]):  # Iterate over the rows of the targets tensor\n",
        "            if (outputs1[i] == targets[i]).sum().item() == targets.shape[1]:  # Check if all elements in the row are equal\n",
        "                count = count + 1  # Increment the count if all elements are equal\n",
        "    return count / targets.shape[0]  # Return the word-level accuracy as a ratio of correct predictions to total predictions\n",
        "\n",
        "def equi_points(data, epochs):\n",
        "    # Sample equidistant points from data\n",
        "    step = len(data) // epochs  # Calculate the step size to sample equidistant points\n",
        "    indices = np.arange(0, len(data), step)  # Generate indices using numpy arange function\n",
        "    equidistant_points = [data[i] for i in indices]  # Extract the equidistant points from the data list\n",
        "    \n",
        "    return equidistant_points  # Return the list of equidistant points\n"
      ],
      "metadata": {
        "id": "_25eBZBX9Klx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def read_csv_files():\n",
        "    # Define the file paths for the CSV files\n",
        "    file_paths = {\n",
        "        'file1': '/content/drive/MyDrive/aksharantar_sampled/mar/mar_test.csv',  # Path to 'mar_test.csv'\n",
        "        'file2': '/content/drive/MyDrive/aksharantar_sampled/mar/mar_valid.csv',  # Path to 'mar_valid.csv'\n",
        "        'file3': '/content/drive/MyDrive/aksharantar_sampled/mar/mar_train.csv'  # Path to 'mar_train.csv'\n",
        "    }\n",
        "\n",
        "    data = {}  # Dictionary to store the CSV data\n",
        "\n",
        "    for file_key, file_path in file_paths.items():\n",
        "        with open(file_path) as file:\n",
        "            csv_reader = csv.reader(file)\n",
        "            header = next(csv_reader)  # Get the header row of the CSV\n",
        "            rows = [row for row in csv_reader]  # Get all the data rows of the CSV\n",
        "            data[file_key] = {\n",
        "                'header': header,\n",
        "                'rows': rows\n",
        "            }\n",
        "\n",
        "    return data\n",
        "\n",
        "# Execute the function to read CSV files\n",
        "csv_data = read_csv_files()\n",
        "\n",
        "# Access the data\n",
        "test = csv_data['file1']['rows']  # Contains rows from 'mar_test.csv'\n",
        "val = csv_data['file2']['rows']  # Contains rows from 'mar_valid.csv'\n",
        "train = csv_data['file3']['rows']  # Contains rows from 'mar_train.csv'\n"
      ],
      "metadata": {
        "id": "3LTAnC4Q9NrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "bCIYZJEL9Qe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to read data from a list of pairs\n",
        "def Data_Reading(list):\n",
        "  i = []  # List to store the first element of each pair\n",
        "  t = []  # List to store the second element of each pair\n",
        "  for pair in list:\n",
        "    i.append(pair[0])  # Append the first element to 'i'\n",
        "    t.append(pair[1])  # Append the second element to 't'\n",
        "  return i, t  # Return the two lists\n",
        "\n",
        "# Read data from the 'train' list using the 'Data_Reading' function\n",
        "train_inputs, train_targets = Data_Reading(train)\n",
        "\n",
        "# Read data from the 'test' list using the 'Data_Reading' function\n",
        "test_inputs, test_targets = Data_Reading(test)\n",
        "\n",
        "# Read data from the 'val' list using the 'Data_Reading' function\n",
        "val_inputs, val_targets = Data_Reading(val)\n",
        "\n",
        "# Print the second element of the 'train_inputs' list\n",
        "# print(train_inputs[1])\n",
        "\n",
        "# Print the second element of the 'train_targets' list\n",
        "# print(train_targets[1])\n",
        "\n",
        "# Define a dictionary to store variable values\n",
        "variable_dict = {\n",
        "    'sc': '\\t',    # 'sc' represents a tab character\n",
        "    'eb': '\\n',    # 'eb' represents a newline character\n",
        "    'bc ': ' ',    # 'bc' represents a space character\n",
        "    'unc': '\\r'    # 'unc' represents a carriage return character\n",
        "}\n",
        "\n",
        "# Accessing the values from the dictionary\n",
        "sc = variable_dict['sc']    # Assign the value of '\\t' to 'sc'\n",
        "eb = variable_dict['eb']    # Assign the value of '\\n' to 'eb'\n",
        "bc  = variable_dict['bc ']  # Assign the value of ' ' to 'bc'\n",
        "unc = variable_dict['unc']  # Assign the value of '\\r' to 'unc'\n",
        "\n",
        "\n",
        "# Function to create dictionaries for language encoding\n",
        "\n",
        "def dictlang(inputs, targets):\n",
        "    dict = {}  # Dictionary for input language\n",
        "    mil = 0  # Maximum input language length\n",
        "    ichar = []  # List of characters in input language\n",
        "\n",
        "    dict_t = {}  # Dictionary for target language\n",
        "    mtl = 0  # Maximum target language length\n",
        "    tchar = []  # List of characters in target language\n",
        "\n",
        "    for s in inputs:\n",
        "        mil = max(len(s), mil)\n",
        "        for char in s:\n",
        "            if char not in dict:\n",
        "                dict[char] = len(ichar)\n",
        "                ichar.append(char)\n",
        "\n",
        "    # Add special characters if they are not present in the input language dictionary\n",
        "    if bc not in dict:\n",
        "        dict[bc] = len(ichar)\n",
        "        ichar.append(bc)\n",
        "\n",
        "    dict[unc] = len(ichar)\n",
        "    ichar.append(unc)\n",
        "\n",
        "    # Add special characters if they are not present in the target language dictionary\n",
        "    if sc not in dict_t:\n",
        "        dict_t[sc] = len(tchar)\n",
        "        tchar.append(sc)\n",
        "\n",
        "    for s in targets:\n",
        "        mtl = max(len(s) + 2, mtl)\n",
        "        for char in s:\n",
        "            if char not in dict_t:\n",
        "                dict_t[char] = len(tchar)\n",
        "                tchar.append(char)\n",
        "\n",
        "    # Add special characters if they are not present in the target language dictionary\n",
        "    if eb not in dict_t:\n",
        "        dict_t[eb] = len(tchar)\n",
        "        tchar.append(eb)\n",
        "\n",
        "    # Add special characters if they are not present in the target language dictionary\n",
        "    if bc not in dict_t:\n",
        "        dict_t[bc] = len(tchar)\n",
        "        tchar.append(bc)\n",
        "\n",
        "    return dict, mil, ichar, dict_t, mtl, tchar\n",
        "\n",
        "# Create dictionaries for language encoding using training, validation, and test inputs and targets\n",
        "dict, mil, ichar, dict_t, mtl, tchar = dictlang(train_inputs + val_inputs + test_inputs, train_targets + val_targets + test_targets)\n",
        "\n",
        "\n",
        "def encoding_w(a, d, ml, l):\n",
        "    encs = []  # List to store encoded words\n",
        "    for word in a:\n",
        "        enc = []  # List to store encoded characters of a word\n",
        "        for char in word:\n",
        "            if char in d:\n",
        "                enc.append(d[char])  # Encode character if present in the dictionary\n",
        "            else:\n",
        "                enc.append(d[unc])  # Encode unknown character\n",
        "        if (l == 0):\n",
        "            while len(enc) < ml:\n",
        "                enc.append(d[bc])  # Pad with blank character if required\n",
        "        if (l == 1):\n",
        "            enc.insert(0, d[sc])  # Insert start character at the beginning\n",
        "            while len(enc) < ml - 1:\n",
        "                enc.append(d[bc])  # Pad with blank character if required\n",
        "            enc.append(d[eb])  # Append end character at the end\n",
        "        encs.append(enc)  # Add encoded word to the list\n",
        "    return encs\n",
        "\n",
        "def tokenize(train,val,test,dict,dict_t,mil,mtl):\n",
        "    # Tokenize the data for training, validation, and testing sets\n",
        "    ti, tt = Data_Reading(train)  # Read training data\n",
        "    tti, ttt = Data_Reading(test)  # Read testing data\n",
        "    vi, vt = Data_Reading(val)  # Read validation data\n",
        "\n",
        "    # Encode the training, validation, and testing data\n",
        "    e_t_i = encoding_w(ti,dict,mil,0)  # Encode training input data\n",
        "    e_t_t = encoding_w(tt,dict_t,mtl,1)  # Encode training target data\n",
        "    e_v_i = encoding_w(vi,dict,mil,0)  # Encode validation input data\n",
        "    e_v_t = encoding_w(vt,dict_t,mtl,1)  # Encode validation target data\n",
        "    e_tt_i = encoding_w(tti,dict,mil,0)  # Encode testing input data\n",
        "    e_tt_t = encoding_w(ttt,dict_t,mtl,1)  # Encode testing target data\n",
        "\n",
        "    return e_t_i, e_t_t, e_v_i, e_v_t, e_tt_i, e_tt_t\n",
        "\n",
        "# Tokenize the data using the given parameters\n",
        "e_t_i, e_t_t, e_v_i, e_v_t, e_tt_i, e_tt_t = tokenize(train, val, test, dict, dict_t, mil, mtl)\n",
        "\n",
        "# Generate a random number within the range [0, 100]\n",
        "r = random.randint(0, 100)\n",
        "\n",
        "# Print the keys corresponding to the values of the encoded data at index r\n",
        "# print(key_presenting(dict, e_t_i[int(r)]))\n",
        "# print(key_presenting(dict_t, e_t_t[int(r)]))\n",
        "\n",
        "\n",
        "# Define a function to convert input and target data into tensor pairs\n",
        "def tensor_conversion(ti, tg):\n",
        "    pr = []\n",
        "    for id, td in zip(ti, tg):\n",
        "        # Convert each item in ti to a torch tensor\n",
        "        it = torch.tensor(id)\n",
        "        # Convert each item in tg to a torch tensor\n",
        "        tt = torch.tensor(td)\n",
        "        # Append the tensor pair to the result list\n",
        "        pr.append((it, tt))\n",
        "    return pr\n",
        "\n",
        "# Convert e_t_i and e_t_t to tensor pairs\n",
        "e_t_p = tensor_conversion(e_t_i, e_t_t)\n",
        "\n",
        "# Convert e_v_i and e_v_t to tensor pairs\n",
        "e_v_p = tensor_conversion(e_v_i, e_v_t)\n",
        "\n",
        "# Re-assign e_t_p with new tensor pairs converted from e_tt_i and e_tt_t\n",
        "e_t_p = tensor_conversion(e_tt_i, e_tt_t)\n",
        "\n",
        "# Create a tuple containing e_t_p, e_v_p, and e_t_p\n",
        "pairs = (e_t_p, e_v_p, e_t_p)\n",
        "\n",
        "# Randomly choose a pair from e_t_p\n",
        "pair = random.choice(e_t_p)\n",
        "\n",
        "# Print the keys in dict corresponding to the values in pair[0]\n",
        "# print(key_presenting(dict, pair[0]))\n",
        "\n",
        "# Print the keys in dict_t corresponding to the values in pair[1]\n",
        "# print(key_presenting(dict_t, pair[1]))\n"
      ],
      "metadata": {
        "id": "8JDuRQwV9VNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(this, device, cell_type, vocab_size, embed_dim, hidden_size, num_layers=1, bidirectional=False, dropout_p=0):\n",
        "        super(EncoderRNN, this).__init__()\n",
        "        this.embedding = nn.Embedding(vocab_size, embed_dim)  # Embedding layer for converting input indices to dense vectors.\n",
        "        this.hidden_size = hidden_size  # Dimensionality of the hidden state of the RNN.\n",
        "        this.num_layers = num_layers  # Number of recurrent layers.\n",
        "        this.bidirectional = bidirectional  # Flag indicating whether the RNN is bidirectional.\n",
        "        this.cell_type = cell_type  # Type of the RNN cell: 'lstm', 'rnn', or 'gru'.\n",
        "        this.dropout_p = dropout_p  # Dropout probability.\n",
        "        this.dropout = nn.Dropout(this.dropout_p)  # Dropout layer for regularization.\n",
        "        if cell_type == 'lstm':\n",
        "            this.rnn = nn.LSTM(embed_dim, hidden_size, num_layers=this.num_layers, batch_first=True, dropout=dropout_p, bidirectional=this.bidirectional)  # LSTM RNN module.\n",
        "        elif cell_type == 'rnn':\n",
        "            this.rnn = nn.RNN(embed_dim, hidden_size, num_layers=this.num_layers, batch_first=True, dropout=dropout_p, bidirectional=this.bidirectional)  # Vanilla RNN module.\n",
        "        elif cell_type == 'gru':\n",
        "            this.rnn = nn.GRU(embed_dim, hidden_size, num_layers=this.num_layers, batch_first=True, dropout=dropout_p, bidirectional=this.bidirectional)  # GRU module.\n",
        "\n",
        "    def forward(this, x, hidden, cell):\n",
        "        out = this.embedding(x)  # Convert input indices to dense vectors.\n",
        "        out = this.dropout(out)  # Apply dropout to the input.\n",
        "        if this.cell_type == 'lstm':\n",
        "            out, (hidden, cell) = this.rnn(out, (hidden, cell))  # Forward pass through the LSTM RNN.\n",
        "            return out, hidden, cell  # Return the output, hidden state, and cell state.\n",
        "        elif this.cell_type == 'rnn':\n",
        "            out, hidden = this.rnn(out, hidden)  # Forward pass through the vanilla RNN.\n",
        "            return out, hidden  # Return the output and hidden state.\n",
        "        elif this.cell_type == 'gru':\n",
        "            out, hidden = this.rnn(out, hidden)  # Forward pass through the GRU.\n",
        "            return out, hidden  # Return the output and hidden state.\n",
        "\n",
        "    def init_hidden(this, batch_size):\n",
        "        hidden = torch.randn((1 + int(this.bidirectional)) * this.num_layers, batch_size, this.hidden_size, device=device)  # Initialize the hidden state tensor.\n",
        "        cell = torch.randn((1 + int(this.bidirectional)) * this.num_layers, batch_size, this.hidden_size, device=device)  # Initialize the cell state tensor.\n",
        "        return hidden, cell\n"
      ],
      "metadata": {
        "id": "7pG-NmFEC0Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, device, cell_type, output_vocab, embed_size, hidden_size, max_length, dropout_p=0.1, num_layers=1, bidirectional=False):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        # Initialize the decoder attributes\n",
        "        self.hidden_size = hidden_size  # Size of the hidden state\n",
        "        self.output_size = output_vocab  # Size of the output vocabulary\n",
        "        self.embed_size = embed_size  # Size of the embedding\n",
        "        self.dropout_p = dropout_p  # Dropout probability\n",
        "        self.cell_type = cell_type  # Type of the RNN cell\n",
        "        self.max_length = max_length  # Maximum length of the input sequence\n",
        "        self.device = device  # Device to be used (e.g., 'cpu' or 'cuda')\n",
        "        self.num_layers = num_layers  # Number of layers in the RNN\n",
        "        self.embedding_decoder = nn.Embedding(self.output_size, self.embed_size)  # Embedding layer\n",
        "        self.dropout = nn.Dropout(self.dropout_p)  # Dropout layer\n",
        "        self.bidirectional = bidirectional  # Flag indicating bidirectional RNN\n",
        "\n",
        "        # Initialize the RNN based on the specified cell_type\n",
        "        if cell_type == 'lstm':\n",
        "            self.rnn = nn.LSTM(self.embed_size, hidden_size, num_layers=self.num_layers, batch_first=True, bidirectional=self.bidirectional, dropout=self.dropout_p)\n",
        "        elif cell_type == 'gru':\n",
        "            self.rnn = nn.GRU(self.embed_size, hidden_size, num_layers=self.num_layers, batch_first=True, bidirectional=self.bidirectional, dropout=self.dropout_p)\n",
        "        elif cell_type == 'rnn':\n",
        "            self.rnn = nn.RNN(self.embed_size, hidden_size, num_layers=self.num_layers, batch_first=True, bidirectional=self.bidirectional, dropout=self.dropout_p)\n",
        "\n",
        "        self.out = nn.Linear((1 + int(self.bidirectional)) * self.hidden_size, self.output_size)  # Linear layer\n",
        "        self.out_activation = nn.LogSoftmax(dim=-1)  # Log softmax activation function\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        input = input.unsqueeze(1)  # Add a dimension of size 1 to the input tensor\n",
        "        embedded_decoder = self.embedding_decoder(input)  # Embed the input tensor\n",
        "        embedded_decoder = self.dropout(embedded_decoder)  # Apply dropout to the embedded tensor\n",
        "\n",
        "        # Pass the embedded tensor through the RNN\n",
        "        if self.cell_type == 'lstm':\n",
        "            output, (hidden, cell) = self.rnn(embedded_decoder, (hidden, cell))\n",
        "        elif self.cell_type == 'gru':\n",
        "            output, hidden = self.rnn(embedded_decoder, hidden)\n",
        "        elif self.cell_type == 'rnn':\n",
        "            output, hidden = self.rnn(embedded_decoder, hidden)\n",
        "\n",
        "        output = F.relu(self.out(output))  # Apply ReLU activation to the output tensor\n",
        "        output = F.log_softmax(output, dim=-1)  # Apply log softmax activation to the output tensor\n",
        "\n",
        "        return output, hidden, cell\n",
        "\n",
        "    def init_hidden(self, encoder_hidden, encoder_cell, encoder_bidirectional):\n",
        "        hidden = encoder_hidden[-(1 + int(encoder_bidirectional)):].repeat(self.num_layers, 1, 1)  # Repeat the hidden tensor\n",
        "        cell = encoder_cell[-(1 + int(encoder_bidirectional)):].repeat(self.num_layers, 1, 1)  # Repeat the cell tensor\n",
        "        return hidden, cell\n"
      ],
      "metadata": {
        "id": "2mo_06fMDjR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(this, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        this.encoder = encoder\n",
        "        this.decoder = decoder\n",
        "        this.device = device\n",
        "        this.mtl = 0  # Initialize the maximum target length\n",
        "        this.sos = 0  # Initialize the start of sequence token\n",
        "        \n",
        "    def forward(this, source, target, teacher_forcing_ratio = 0.5):\n",
        "        # Get dimensions of input\n",
        "        batch_size = target.shape[0]\n",
        "        target_len = target.shape[1]\n",
        "        this.mtl = target_len  # Set the maximum target length\n",
        "        target_vocab_size = this.decoder.output_size\n",
        "        \n",
        "        # Initialize tensor for storing outputs\n",
        "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(this.device)\n",
        "\n",
        "        # Initialize hidden states of the encoder\n",
        "        encoder_hidden, encoder_cell = this.encoder.init_hidden(batch_size)\n",
        "\n",
        "        # Forward pass through the encoder\n",
        "        if (this.encoder.cell_type == 'lstm'):\n",
        "            encoder_outputs, encoder_hidden, encoder_cell = this.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "        if (this.encoder.cell_type == 'rnn'):\n",
        "            encoder_outputs, encoder_hidden = this.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "        if (this.encoder.cell_type == 'gru'):\n",
        "            encoder_outputs, encoder_hidden = this.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "\n",
        "        # First input to the decoder is the <sos> tokens\n",
        "        input = target[:,0]\n",
        "        this.sos = target[:,0]\n",
        "\n",
        "        # Initialize hidden states of the decoder\n",
        "        hidden, cell = this.decoder.init_hidden(encoder_hidden, encoder_cell, this.encoder.bidirectional)\n",
        "        \n",
        "        # Iterate over the target sequence\n",
        "        for t in range(1, target_len):\n",
        "            # Forward pass through the decoder\n",
        "            output, hidden, cell = this.decoder.forward(input, hidden, cell)\n",
        "            outputs[:,t] = output.squeeze(1)  # Store the decoder output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(-1)\n",
        "            input = target[:,t] if teacher_force else top1.squeeze(1)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def inference(this, source, target):\n",
        "        # Get dimensions of input\n",
        "        batch_size = source.shape[0]\n",
        "        target_len = this.mtl\n",
        "        target_vocab_size = this.decoder.output_size\n",
        "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(this.device)\n",
        "\n",
        "        # Initialize hidden states of the encoder\n",
        "        encoder_hidden, encoder_cell = this.encoder.init_hidden(batch_size)\n",
        "\n",
        "        # Forward pass through the encoder\n",
        "        if (this.encoder.cell_type == 'lstm'):\n",
        "            encoder_outputs, encoder_hidden, encoder_cell = this.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "        if (this.encoder.cell_type == 'rnn'):\n",
        "            encoder_outputs, encoder_hidden = this.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "        if (this.encoder.cell_type == 'gru'):\n",
        "            encoder_outputs, encoder_hidden = this.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "           \n",
        "        # First input to the decoder is the <sos> token\n",
        "        input = this.sos\n",
        "\n",
        "        # Initialize hidden states of the decoder\n",
        "        hidden, cell = this.decoder.init_hidden(encoder_hidden, encoder_cell, this.encoder.bidirectional)\n",
        "      \n",
        "        # Iterate over the target sequence\n",
        "        for t in range(1, target_len):\n",
        "            # Forward pass through the decoder\n",
        "            output, hidden, cell = this.decoder.forward(input, hidden, cell)\n",
        "            outputs[:,t] = output.squeeze(1)  # Store the decoder output\n",
        "            top1 = output.argmax(-1)\n",
        "            input = top1.squeeze(1)\n",
        "\n",
        "        # print(f\"Outputs = {outputs[:,0]}\")\n",
        "        # print(f\"Targets = {target[:,0]}\")\n",
        "        \n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "6Sc898q4EI0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Commented code\n",
        "\n",
        "# Define the input and output dimensions based on the length of the dictionary\n",
        "input_dim = len(dict)\n",
        "output_dim = len(dict_t)\n",
        "\n",
        "# Set the batch size for training and validation\n",
        "batch_size = 32\n",
        "val_batch_size = 32\n",
        "\n",
        "# Set the embedding dimensions for the encoder and decoder\n",
        "enc_embedding = 256\n",
        "dec_embedding = 256\n",
        "\n",
        "# Set the number of hidden units in the encoder and decoder\n",
        "hidden = 512\n",
        "\n",
        "# Set the number of layers in the encoder and decoder\n",
        "enc_num_layers = 3\n",
        "dec_num_layers = 2\n",
        "\n",
        "# Set the dropout probabilities for the encoder and decoder\n",
        "enc_dropout = 0.4\n",
        "dec_dropout = 0.4\n",
        "\n",
        "# Set the maximum length for the input sequence\n",
        "max_length = mtl\n",
        "\n",
        "# Set the cell type for the encoder and decoder\n",
        "cell_type = 'gru'\n",
        "\n",
        "# Initialize the encoder, decoder, and the overall model\n",
        "enc = EncoderRNN(device, cell_type, input_dim, enc_embedding, hidden, enc_num_layers, bidirectional=True, dropout_p=enc_dropout)\n",
        "dec = DecoderRNN(device, cell_type, output_dim, dec_embedding, hidden, max_length, dec_dropout, dec_num_layers, bidirectional=True)\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "# Function to count the number of trainable parameters in the model\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# Define the optimizer and the loss criterion\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Function to train the model for a given number of iterations\n",
        "# Function for training the model\n",
        "def trainIters(model, pairs, batch_size, n_iters, optimizer, tf, print_every=10, plot_every=10, log=True, Attention=False):\n",
        "    start = time.time()  # Start time for tracking training duration\n",
        "    plot_losses = []  # List to store plot losses for visualization\n",
        "    train_char_accuracy = []  # List to store training character-level accuracy\n",
        "    train_word_accuracy = []  # List to store training word-level accuracy\n",
        "    val_losses = []  # List to store validation losses\n",
        "    val_char_accuracy = []  # List to store validation character-level accuracy\n",
        "    val_word_accuracy = []  # List to store validation word-level accuracy\n",
        "    print_loss_total = 0  # Variable to track the cumulative print loss\n",
        "    plot_loss_total = 0  # Variable to track the cumulative plot loss\n",
        "    print_val_loss_total = 0  # Variable to track the cumulative validation print loss\n",
        "    plot_val_loss_total = 0  # Variable to track the cumulative validation plot loss\n",
        "\n",
        "    train_pairs = pairs[0]  # Training pairs\n",
        "    val_pairs = pairs[1]  # Validation pairs\n",
        "    train_accuracy = 0  # Training accuracy\n",
        "\n",
        "    criterion = nn.NLLLoss()  # Negative Log Likelihood loss function\n",
        "\n",
        "    count = 0  # Counter for tracking iterations\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        for i in np.arange(start=0, stop=len(train_pairs) - batch_size, step=batch_size):\n",
        "            train_accuracy = 0  # Reset training accuracy for each batch\n",
        "            count += 1  # Increment the counter\n",
        "            if (i + batch_size > len(train_pairs)):  # Check if the remaining examples are less than batch size\n",
        "                batch_size = len(train_pairs) - i + 1  # Adjust the batch size\n",
        "            input_tensor = []  # List to store input tensors for the batch\n",
        "            target_tensor = []  # List to store target tensors for the batch\n",
        "\n",
        "            for j in range(batch_size):  # Iterate over the batch examples\n",
        "                input_tensor.append(train_pairs[i + j][0])  # Append input tensor to the list\n",
        "                target_tensor.append(train_pairs[i + j][1])  # Append target tensor to the list\n",
        "\n",
        "            input_tensor = torch.stack(input_tensor).squeeze(1).long().cuda()  # Convert input tensors to tensor and move to GPU\n",
        "            target_tensor = torch.stack(target_tensor).squeeze(1).long().cuda()  # Convert target tensors to tensor and move to GPU\n",
        "\n",
        "            optimizer.zero_grad()  # Zero the gradients\n",
        "            if (count < 4000):  # Check if teacher forcing should be applied\n",
        "                out = model(input_tensor, target_tensor, teacher_forcing_ratio=tf)  # Perform forward pass with teacher forcing\n",
        "            else:\n",
        "                out = model(input_tensor, target_tensor, teacher_forcing_ratio=0)  # Perform forward pass without teacher forcing\n",
        "\n",
        "            out = torch.permute(out, [0, 2, 1])  # Permute the output tensor\n",
        "            loss = criterion(out, target_tensor)  # Calculate the loss\n",
        "\n",
        "            train_accuracy_word = word_acc(target_tensor, out) * batch_size  # Calculate word-level accuracy\n",
        "            train_accuracy = train_accuracy + train_accuracy_word  # Update training accuracy\n",
        "            loss.backward()  # Backpropagation\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)  # Clip gradients to prevent exploding gradients\n",
        "            optimizer.step()  # Update the model parameters\n",
        "\n",
        "            print_loss_total += loss  # Accumulate the print loss\n",
        "            plot_loss_total += loss  # Accumulate the plot loss\n",
        "\n",
        "            if count % 800 == 0:  # Perform validation every 800 iterations\n",
        "                val_input_tensor = []  # List to store validation input tensors\n",
        "                val_target_tensor = []  # List to store validation target tensors\n",
        "\n",
        "                for j in range(batch_size):  # Iterate over the validation batch examples\n",
        "                    val_input_tensor.append(val_pairs[j][0])  # Append validation input tensor to the list\n",
        "                    val_target_tensor.append(val_pairs[j][1])  # Append validation target tensor to the list\n",
        "\n",
        "                val_input_tensor = torch.stack(val_input_tensor).squeeze(1).long().cuda()  # Convert validation input tensors to tensor and move to GPU\n",
        "                val_target_tensor = torch.stack(val_target_tensor).squeeze(1).long().cuda()  # Convert validation target tensors to tensor and move to GPU\n",
        "                if (Attention == True):  # Check if attention mechanism is enabled\n",
        "                    val_out, _ = model.inference(val_input_tensor, val_target_tensor)  # Perform inference with attention\n",
        "                    val_out = val_out.permute(0, 2, 1)  # Permute the output tensor\n",
        "                else:\n",
        "                    val_out = model.inference(val_input_tensor, val_target_tensor)  # Perform inference without attention\n",
        "                    val_out = val_out.permute(0, 2, 1)  # Permute the output tensor\n",
        "                val_loss = criterion(val_out, val_target_tensor)  # Calculate the validation loss\n",
        "                val_loss_sampled = val_loss  # Store the sampled validation loss\n",
        "                wandb.log({'Val_Loss': val_loss_sampled})  # Log the validation loss\n",
        "\n",
        "            if count % 800 == 0:  # Print and log the training loss every 800 iterations\n",
        "                print_loss_avg = print_loss_total / 800  # Calculate the average print loss\n",
        "                print_loss_total = 0  # Reset the print loss total\n",
        "                print('%s (%d %d%%) %.7f' % (span(start, iter / n_iters), iter, iter / n_iters * 100, print_loss_avg))  # Print the progress\n",
        "\n",
        "            if count % 800 == 0:  # Plot and log the training loss every 800 iterations\n",
        "                plot_loss_avg = plot_loss_total / 800  # Calculate the average plot loss\n",
        "                plot_losses.append(plot_loss_avg.detach())  # Append the plot loss to the list\n",
        "                wandb.log({'Train Loss': plot_loss_avg})  # Log the training loss\n",
        "\n",
        "                plot_loss_total = 0  # Reset the plot loss total\n",
        "\n",
        "        train_accuracy = train_accuracy / (len(train_pairs) - batch_size)  # Calculate the average training accuracy\n",
        "        train_word_accuracy.append(train_accuracy)  # Append the training accuracy to the list\n",
        "\n",
        "    print(train_word_accuracy)  # Print the list of training word-level accuracies\n",
        "    plot_losses = [losses.cpu().numpy() for losses in plot_losses]  # Convert plot losses to numpy array\n",
        "\n",
        "    char_count = 0  # Variable to track character count\n",
        "    word_count = 0  # Variable to track word count\n",
        "\n",
        "    for i in np.arange(start=0, stop=len(val_pairs) - batch_size, step=batch_size):  # Iterate over the validation pairs\n",
        "        if (i + batch_size > len(val_pairs)):  # Check if the remaining examples are less than batch size\n",
        "            batch_size = len(val_pairs) - i + 1  # Adjust the batch size\n",
        "        val_input_tensor = []  # List to store validation input tensors\n",
        "        val_target_tensor = []  # List to store validation target tensors\n",
        "        for j in range(batch_size):  # Iterate over the batch examples\n",
        "            val_input_tensor.append(val_pairs[i + j][0])  # Append validation input tensor to the list\n",
        "            val_target_tensor.append(val_pairs[i + j][1])  # Append validation target tensor to the list\n",
        "\n",
        "        val_input_tensor = torch.stack(val_input_tensor).squeeze(1).long().cuda()  # Convert validation input tensors to tensor and move to GPU\n",
        "        val_target_tensor = torch.stack(val_target_tensor).squeeze(1).long().cuda()  # Convert validation target tensors to tensor and move to GPU\n",
        "        if (Attention == True):  # Check if attention mechanism is enabled\n",
        "            val_out, _ = model.inference(val_input_tensor, val_target_tensor)  # Perform inference with attention\n",
        "            val_out = val_out.permute(0, 2, 1)  # Permute the output tensor\n",
        "        else:\n",
        "            val_out = model.inference(val_input_tensor, val_target_tensor)  # Perform inference without attention\n",
        "            val_out = val_out.permute(0, 2, 1)  # Permute the output tensor\n",
        "        val_loss = criterion(val_out, val_target_tensor)  # Calculate the validation loss\n",
        "\n",
        "        val_accuracy_word = word_acc(val_target_tensor, val_out)  # Calculate word-level accuracy\n",
        "        word_count = word_count + (val_accuracy_word) * batch_size  # Update word count\n",
        "\n",
        "    word_accuracy = word_count / (len(val_pairs) - batch_size)  # Calculate word-level accuracy\n",
        "\n",
        "    metrics = {'Val_Accuracy': word_accuracy}  # Store validation metrics\n",
        "    wandb.log(metrics)  # Log the validation metrics\n",
        "\n",
        "    print(f\"Val loss = {val_loss}\")  # Print the validation loss\n",
        "    print(f'Word-level-accuracy on val set = {word_accuracy}')  # Print the word-level accuracy on the validation set\n",
        "\n"
      ],
      "metadata": {
        "id": "OwalnEDxEmHm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}