{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swapnilmn/Assignment_03/blob/main/Question_1_Ass_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1 (15 Marks)\n",
        "Build a RNN based seq2seq model which contains the following layers: (i) input layer for character embeddings (ii) one encoder RNN which sequentially encodes the input character sequence (Latin) (iii) one decoder RNN which takes the last state of the encoder as input and produces one output character at a time (Devanagari). \n"
      ],
      "metadata": {
        "id": "Sha-7_A_8x99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Librares"
      ],
      "metadata": {
        "id": "6o7LlMzW9ALi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QXFkT1I8tdY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "b94552bf-0728-42f6-8526-4fe62cfd9d04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.1/205.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "!pip install wandb -qU\n",
        "import wandb\n",
        "wandb.login()\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load of data"
      ],
      "metadata": {
        "id": "TMNzTbAT9D3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math  # Import the math module for mathematical operations\n",
        "import time  # Import the time module for time-related functions\n",
        "import torch  # Import the torch module for working with tensors\n",
        "import numpy as np  # Import the numpy module for numerical operations\n",
        "\n",
        "def asMinutes(s):\n",
        "    # Convert seconds to minutes and seconds\n",
        "    m = math.floor(s / 60)  # Calculate the number of minutes\n",
        "    s -= m * 60  # Subtract the minutes converted to seconds from the total seconds\n",
        "    return '%dm %ds' % (m, s)  # Return the formatted string for minutes and seconds\n",
        "\n",
        "def span(since, percent):\n",
        "    # Calculate the elapsed time and estimated remaining time\n",
        "    now = time.time()  # Get the current time\n",
        "    s = now - since  # Calculate the elapsed time in seconds\n",
        "    es = s / (percent)  # Calculate the estimated total time in seconds\n",
        "    rs = es - s  # Calculate the remaining time in seconds\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))  # Return the formatted string for elapsed time and remaining time\n",
        "\n",
        "def key_presenting(dictionary, values):\n",
        "    # Find keys in the dictionary that match the given values\n",
        "    found_keys = []  # Initialize an empty list to store the found keys\n",
        "    for value in values:  # Iterate over the given values\n",
        "        for key, val in dictionary.items():  # Iterate over the dictionary items\n",
        "            if value == val:  # If the value matches the dictionary value\n",
        "                found_keys.append(key)  # Add the key to the list of found keys\n",
        "    \n",
        "    if found_keys:  # If any keys are found\n",
        "        str = ''.join(found_keys)  # Concatenate the found keys into a single string\n",
        "        if str[-1] == '\\n':  # If the string ends with a newline character\n",
        "            str = str[:-1]  # Remove the newline character from the end of the string\n",
        "        elif str[0] == '\\t':  # If the string starts with a tab character\n",
        "            str = str[1:]  # Remove the tab character from the start of the string\n",
        "    else:  # If no keys are found\n",
        "        print(\"No keys found for the given values.\")  # Print a message indicating no keys were found\n",
        "    \n",
        "    return str  # Return the string containing the found keys\n",
        "\n",
        "def char_acc(targets, outputs):\n",
        "    # Calculate character-level accuracy\n",
        "    with torch.no_grad():  # Disable gradient calculation for efficiency\n",
        "        count = 0  # Initialize a counter for correct predictions\n",
        "        total_count = 0  # Initialize a counter for total predictions\n",
        "        for i in range(targets.shape[0]):  # Iterate over the rows of the targets tensor\n",
        "            same_elements = []  # Initialize an empty list to store the comparison results\n",
        "            for j in range(targets.shape[1]):  # Iterate over the columns of the targets tensor\n",
        "                if targets[i][j] != 67 or targets[i][j] != 66:  # Check if the target value is not equal to 67 or 66\n",
        "                    same_elements.append(outputs[i][j].item() == targets[i][j].item())  # Append the comparison result to the list\n",
        "            count += np.sum(same_elements)  # Add the number of True values in the list to the count\n",
        "            total_count += len(same_elements)  # Add the total number of elements in the list to the total count\n",
        "    return count / total_count  # Return the character-level accuracy as a ratio of correct predictions to total predictions\n",
        "\n",
        "def word_acc(targets, outputs):\n",
        "    # Calculate word-level accuracy\n",
        "    outputs1 = torch.argmax(outputs, dim=1)  # Get the index of the maximum value along the second dimension of the outputs tensor\n",
        "    with torch.no_grad():  # Disable gradient calculation for efficiency\n",
        "        count = 0  # Initialize a counter for correct predictions\n",
        "        for i in range(targets.shape[0]):  # Iterate over the rows of the targets tensor\n",
        "            if (outputs1[i] == targets[i]).sum().item() == targets.shape[1]:  # Check if all elements in the row are equal\n",
        "                count = count + 1  # Increment the count if all elements are equal\n",
        "    return count / targets.shape[0]  # Return the word-level accuracy as a ratio of correct predictions to total predictions\n",
        "\n",
        "def equi_points(data, epochs):\n",
        "    # Sample equidistant points from data\n",
        "    step = len(data) // epochs  # Calculate the step size to sample equidistant points\n",
        "    indices = np.arange(0, len(data), step)  # Generate indices using numpy arange function\n",
        "    equidistant_points = [data[i] for i in indices]  # Extract the equidistant points from the data list\n",
        "    \n",
        "    return equidistant_points  # Return the list of equidistant points\n"
      ],
      "metadata": {
        "id": "_25eBZBX9Klx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def read_csv_files():\n",
        "    # Define the file paths for the CSV files\n",
        "    file_paths = {\n",
        "        'file1': '/content/drive/MyDrive/aksharantar_sampled/mar/mar_test.csv',  # Path to 'mar_test.csv'\n",
        "        'file2': '/content/drive/MyDrive/aksharantar_sampled/mar/mar_valid.csv',  # Path to 'mar_valid.csv'\n",
        "        'file3': '/content/drive/MyDrive/aksharantar_sampled/mar/mar_train.csv'  # Path to 'mar_train.csv'\n",
        "    }\n",
        "\n",
        "    data = {}  # Dictionary to store the CSV data\n",
        "\n",
        "    for file_key, file_path in file_paths.items():\n",
        "        with open(file_path) as file:\n",
        "            csv_reader = csv.reader(file)\n",
        "            header = next(csv_reader)  # Get the header row of the CSV\n",
        "            rows = [row for row in csv_reader]  # Get all the data rows of the CSV\n",
        "            data[file_key] = {\n",
        "                'header': header,\n",
        "                'rows': rows\n",
        "            }\n",
        "\n",
        "    return data\n",
        "\n",
        "# Execute the function to read CSV files\n",
        "csv_data = read_csv_files()\n",
        "\n",
        "# Access the data\n",
        "test = csv_data['file1']['rows']  # Contains rows from 'mar_test.csv'\n",
        "val = csv_data['file2']['rows']  # Contains rows from 'mar_valid.csv'\n",
        "train = csv_data['file3']['rows']  # Contains rows from 'mar_train.csv'\n"
      ],
      "metadata": {
        "id": "3LTAnC4Q9NrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "bCIYZJEL9Qe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to read data from a list of pairs\n",
        "def Data_Reading(list):\n",
        "  i = []  # List to store the first element of each pair\n",
        "  t = []  # List to store the second element of each pair\n",
        "  for pair in list:\n",
        "    i.append(pair[0])  # Append the first element to 'i'\n",
        "    t.append(pair[1])  # Append the second element to 't'\n",
        "  return i, t  # Return the two lists\n",
        "\n",
        "# Read data from the 'train' list using the 'Data_Reading' function\n",
        "train_inputs, train_targets = Data_Reading(train)\n",
        "\n",
        "# Read data from the 'test' list using the 'Data_Reading' function\n",
        "test_inputs, test_targets = Data_Reading(test)\n",
        "\n",
        "# Read data from the 'val' list using the 'Data_Reading' function\n",
        "val_inputs, val_targets = Data_Reading(val)\n",
        "\n",
        "# Print the second element of the 'train_inputs' list\n",
        "# print(train_inputs[1])\n",
        "\n",
        "# Print the second element of the 'train_targets' list\n",
        "# print(train_targets[1])\n",
        "\n",
        "# Define a dictionary to store variable values\n",
        "variable_dict = {\n",
        "    'sc': '\\t',    # 'sc' represents a tab character\n",
        "    'eb': '\\n',    # 'eb' represents a newline character\n",
        "    'bc ': ' ',    # 'bc' represents a space character\n",
        "    'unc': '\\r'    # 'unc' represents a carriage return character\n",
        "}\n",
        "\n",
        "# Accessing the values from the dictionary\n",
        "sc = variable_dict['sc']    # Assign the value of '\\t' to 'sc'\n",
        "eb = variable_dict['eb']    # Assign the value of '\\n' to 'eb'\n",
        "bc  = variable_dict['bc ']  # Assign the value of ' ' to 'bc'\n",
        "unc = variable_dict['unc']  # Assign the value of '\\r' to 'unc'\n",
        "\n",
        "\n",
        "# Function to create dictionaries for language encoding\n",
        "\n",
        "def dictlang(inputs, targets):\n",
        "    dict = {}  # Dictionary for input language\n",
        "    mil = 0  # Maximum input language length\n",
        "    ichar = []  # List of characters in input language\n",
        "\n",
        "    dict_t = {}  # Dictionary for target language\n",
        "    mtl = 0  # Maximum target language length\n",
        "    tchar = []  # List of characters in target language\n",
        "\n",
        "    for s in inputs:\n",
        "        mil = max(len(s), mil)\n",
        "        for char in s:\n",
        "            if char not in dict:\n",
        "                dict[char] = len(ichar)\n",
        "                ichar.append(char)\n",
        "\n",
        "    # Add special characters if they are not present in the input language dictionary\n",
        "    if bc not in dict:\n",
        "        dict[bc] = len(ichar)\n",
        "        ichar.append(bc)\n",
        "\n",
        "    dict[unc] = len(ichar)\n",
        "    ichar.append(unc)\n",
        "\n",
        "    # Add special characters if they are not present in the target language dictionary\n",
        "    if sc not in dict_t:\n",
        "        dict_t[sc] = len(tchar)\n",
        "        tchar.append(sc)\n",
        "\n",
        "    for s in targets:\n",
        "        mtl = max(len(s) + 2, mtl)\n",
        "        for char in s:\n",
        "            if char not in dict_t:\n",
        "                dict_t[char] = len(tchar)\n",
        "                tchar.append(char)\n",
        "\n",
        "    # Add special characters if they are not present in the target language dictionary\n",
        "    if eb not in dict_t:\n",
        "        dict_t[eb] = len(tchar)\n",
        "        tchar.append(eb)\n",
        "\n",
        "    # Add special characters if they are not present in the target language dictionary\n",
        "    if bc not in dict_t:\n",
        "        dict_t[bc] = len(tchar)\n",
        "        tchar.append(bc)\n",
        "\n",
        "    return dict, mil, ichar, dict_t, mtl, tchar\n",
        "\n",
        "# Create dictionaries for language encoding using training, validation, and test inputs and targets\n",
        "dict, mil, ichar, dict_t, mtl, tchar = dictlang(train_inputs + val_inputs + test_inputs, train_targets + val_targets + test_targets)\n",
        "\n",
        "\n",
        "def encoding_w(a, d, ml, l):\n",
        "    encs = []  # List to store encoded words\n",
        "    for word in a:\n",
        "        enc = []  # List to store encoded characters of a word\n",
        "        for char in word:\n",
        "            if char in d:\n",
        "                enc.append(d[char])  # Encode character if present in the dictionary\n",
        "            else:\n",
        "                enc.append(d[unc])  # Encode unknown character\n",
        "        if (l == 0):\n",
        "            while len(enc) < ml:\n",
        "                enc.append(d[bc])  # Pad with blank character if required\n",
        "        if (l == 1):\n",
        "            enc.insert(0, d[sc])  # Insert start character at the beginning\n",
        "            while len(enc) < ml - 1:\n",
        "                enc.append(d[bc])  # Pad with blank character if required\n",
        "            enc.append(d[eb])  # Append end character at the end\n",
        "        encs.append(enc)  # Add encoded word to the list\n",
        "    return encs\n",
        "\n",
        "def tokenize(train,val,test,dict,dict_t,mil,mtl):\n",
        "    # Tokenize the data for training, validation, and testing sets\n",
        "    ti, tt = Data_Reading(train)  # Read training data\n",
        "    tti, ttt = Data_Reading(test)  # Read testing data\n",
        "    vi, vt = Data_Reading(val)  # Read validation data\n",
        "\n",
        "    # Encode the training, validation, and testing data\n",
        "    e_t_i = encoding_w(ti,dict,mil,0)  # Encode training input data\n",
        "    e_t_t = encoding_w(tt,dict_t,mtl,1)  # Encode training target data\n",
        "    e_v_i = encoding_w(vi,dict,mil,0)  # Encode validation input data\n",
        "    e_v_t = encoding_w(vt,dict_t,mtl,1)  # Encode validation target data\n",
        "    e_tt_i = encoding_w(tti,dict,mil,0)  # Encode testing input data\n",
        "    e_tt_t = encoding_w(ttt,dict_t,mtl,1)  # Encode testing target data\n",
        "\n",
        "    return e_t_i, e_t_t, e_v_i, e_v_t, e_tt_i, e_tt_t\n",
        "\n",
        "# Tokenize the data using the given parameters\n",
        "e_t_i, e_t_t, e_v_i, e_v_t, e_tt_i, e_tt_t = tokenize(train, val, test, dict, dict_t, mil, mtl)\n",
        "\n",
        "# Generate a random number within the range [0, 100]\n",
        "r = random.randint(0, 100)\n",
        "\n",
        "# Print the keys corresponding to the values of the encoded data at index r\n",
        "# print(key_presenting(dict, e_t_i[int(r)]))\n",
        "# print(key_presenting(dict_t, e_t_t[int(r)]))\n",
        "\n",
        "\n",
        "# Define a function to convert input and target data into tensor pairs\n",
        "def tensor_conversion(ti, tg):\n",
        "    pr = []\n",
        "    for id, td in zip(ti, tg):\n",
        "        # Convert each item in ti to a torch tensor\n",
        "        it = torch.tensor(id)\n",
        "        # Convert each item in tg to a torch tensor\n",
        "        tt = torch.tensor(td)\n",
        "        # Append the tensor pair to the result list\n",
        "        pr.append((it, tt))\n",
        "    return pr\n",
        "\n",
        "# Convert e_t_i and e_t_t to tensor pairs\n",
        "e_t_p = tensor_conversion(e_t_i, e_t_t)\n",
        "\n",
        "# Convert e_v_i and e_v_t to tensor pairs\n",
        "e_v_p = tensor_conversion(e_v_i, e_v_t)\n",
        "\n",
        "# Re-assign e_t_p with new tensor pairs converted from e_tt_i and e_tt_t\n",
        "e_t_p = tensor_conversion(e_tt_i, e_tt_t)\n",
        "\n",
        "# Create a tuple containing e_t_p, e_v_p, and e_t_p\n",
        "pairs = (e_t_p, e_v_p, e_t_p)\n",
        "\n",
        "# Randomly choose a pair from e_t_p\n",
        "pair = random.choice(e_t_p)\n",
        "\n",
        "# Print the keys in dict corresponding to the values in pair[0]\n",
        "# print(key_presenting(dict, pair[0]))\n",
        "\n",
        "# Print the keys in dict_t corresponding to the values in pair[1]\n",
        "# print(key_presenting(dict_t, pair[1]))\n"
      ],
      "metadata": {
        "id": "8JDuRQwV9VNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(this, device, cell_type, vocab_size, embed_dim, hidden_size, num_layers=1, bidirectional=False, dropout_p=0):\n",
        "        super(EncoderRNN, this).__init__()\n",
        "        this.embedding = nn.Embedding(vocab_size, embed_dim)  # Embedding layer for converting input indices to dense vectors.\n",
        "        this.hidden_size = hidden_size  # Dimensionality of the hidden state of the RNN.\n",
        "        this.num_layers = num_layers  # Number of recurrent layers.\n",
        "        this.bidirectional = bidirectional  # Flag indicating whether the RNN is bidirectional.\n",
        "        this.cell_type = cell_type  # Type of the RNN cell: 'lstm', 'rnn', or 'gru'.\n",
        "        this.dropout_p = dropout_p  # Dropout probability.\n",
        "        this.dropout = nn.Dropout(this.dropout_p)  # Dropout layer for regularization.\n",
        "        if cell_type == 'lstm':\n",
        "            this.rnn = nn.LSTM(embed_dim, hidden_size, num_layers=this.num_layers, batch_first=True, dropout=dropout_p, bidirectional=this.bidirectional)  # LSTM RNN module.\n",
        "        elif cell_type == 'rnn':\n",
        "            this.rnn = nn.RNN(embed_dim, hidden_size, num_layers=this.num_layers, batch_first=True, dropout=dropout_p, bidirectional=this.bidirectional)  # Vanilla RNN module.\n",
        "        elif cell_type == 'gru':\n",
        "            this.rnn = nn.GRU(embed_dim, hidden_size, num_layers=this.num_layers, batch_first=True, dropout=dropout_p, bidirectional=this.bidirectional)  # GRU module.\n",
        "\n",
        "    def forward(this, x, hidden, cell):\n",
        "        out = this.embedding(x)  # Convert input indices to dense vectors.\n",
        "        out = this.dropout(out)  # Apply dropout to the input.\n",
        "        if this.cell_type == 'lstm':\n",
        "            out, (hidden, cell) = this.rnn(out, (hidden, cell))  # Forward pass through the LSTM RNN.\n",
        "            return out, hidden, cell  # Return the output, hidden state, and cell state.\n",
        "        elif this.cell_type == 'rnn':\n",
        "            out, hidden = this.rnn(out, hidden)  # Forward pass through the vanilla RNN.\n",
        "            return out, hidden  # Return the output and hidden state.\n",
        "        elif this.cell_type == 'gru':\n",
        "            out, hidden = this.rnn(out, hidden)  # Forward pass through the GRU.\n",
        "            return out, hidden  # Return the output and hidden state.\n",
        "\n",
        "    def init_hidden(this, batch_size):\n",
        "        hidden = torch.randn((1 + int(this.bidirectional)) * this.num_layers, batch_size, this.hidden_size, device=device)  # Initialize the hidden state tensor.\n",
        "        cell = torch.randn((1 + int(this.bidirectional)) * this.num_layers, batch_size, this.hidden_size, device=device)  # Initialize the cell state tensor.\n",
        "        return hidden, cell\n"
      ],
      "metadata": {
        "id": "7pG-NmFEC0Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, device, cell_type, output_vocab, embed_size, hidden_size, max_length, dropout_p=0.1, num_layers=1, bidirectional=False):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        # Initialize the decoder attributes\n",
        "        self.hidden_size = hidden_size  # Size of the hidden state\n",
        "        self.output_size = output_vocab  # Size of the output vocabulary\n",
        "        self.embed_size = embed_size  # Size of the embedding\n",
        "        self.dropout_p = dropout_p  # Dropout probability\n",
        "        self.cell_type = cell_type  # Type of the RNN cell\n",
        "        self.max_length = max_length  # Maximum length of the input sequence\n",
        "        self.device = device  # Device to be used (e.g., 'cpu' or 'cuda')\n",
        "        self.num_layers = num_layers  # Number of layers in the RNN\n",
        "        self.embedding_decoder = nn.Embedding(self.output_size, self.embed_size)  # Embedding layer\n",
        "        self.dropout = nn.Dropout(self.dropout_p)  # Dropout layer\n",
        "        self.bidirectional = bidirectional  # Flag indicating bidirectional RNN\n",
        "\n",
        "        # Initialize the RNN based on the specified cell_type\n",
        "        if cell_type == 'lstm':\n",
        "            self.rnn = nn.LSTM(self.embed_size, hidden_size, num_layers=self.num_layers, batch_first=True, bidirectional=self.bidirectional, dropout=self.dropout_p)\n",
        "        elif cell_type == 'gru':\n",
        "            self.rnn = nn.GRU(self.embed_size, hidden_size, num_layers=self.num_layers, batch_first=True, bidirectional=self.bidirectional, dropout=self.dropout_p)\n",
        "        elif cell_type == 'rnn':\n",
        "            self.rnn = nn.RNN(self.embed_size, hidden_size, num_layers=self.num_layers, batch_first=True, bidirectional=self.bidirectional, dropout=self.dropout_p)\n",
        "\n",
        "        self.out = nn.Linear((1 + int(self.bidirectional)) * self.hidden_size, self.output_size)  # Linear layer\n",
        "        self.out_activation = nn.LogSoftmax(dim=-1)  # Log softmax activation function\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        input = input.unsqueeze(1)  # Add a dimension of size 1 to the input tensor\n",
        "        embedded_decoder = self.embedding_decoder(input)  # Embed the input tensor\n",
        "        embedded_decoder = self.dropout(embedded_decoder)  # Apply dropout to the embedded tensor\n",
        "\n",
        "        # Pass the embedded tensor through the RNN\n",
        "        if self.cell_type == 'lstm':\n",
        "            output, (hidden, cell) = self.rnn(embedded_decoder, (hidden, cell))\n",
        "        elif self.cell_type == 'gru':\n",
        "            output, hidden = self.rnn(embedded_decoder, hidden)\n",
        "        elif self.cell_type == 'rnn':\n",
        "            output, hidden = self.rnn(embedded_decoder, hidden)\n",
        "\n",
        "        output = F.relu(self.out(output))  # Apply ReLU activation to the output tensor\n",
        "        output = F.log_softmax(output, dim=-1)  # Apply log softmax activation to the output tensor\n",
        "\n",
        "        return output, hidden, cell\n",
        "\n",
        "    def init_hidden(self, encoder_hidden, encoder_cell, encoder_bidirectional):\n",
        "        hidden = encoder_hidden[-(1 + int(encoder_bidirectional)):].repeat(self.num_layers, 1, 1)  # Repeat the hidden tensor\n",
        "        cell = encoder_cell[-(1 + int(encoder_bidirectional)):].repeat(self.num_layers, 1, 1)  # Repeat the cell tensor\n",
        "        return hidden, cell\n"
      ],
      "metadata": {
        "id": "2mo_06fMDjR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(this, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        this.encoder = encoder\n",
        "        this.decoder = decoder\n",
        "        this.device = device\n",
        "        this.mtl = 0  # Initialize the maximum target length\n",
        "        this.sos = 0  # Initialize the start of sequence token\n",
        "        \n",
        "    def forward(this, source, target, teacher_forcing_ratio = 0.5):\n",
        "        # Get dimensions of input\n",
        "        batch_size = target.shape[0]\n",
        "        target_len = target.shape[1]\n",
        "        this.mtl = target_len  # Set the maximum target length\n",
        "        target_vocab_size = this.decoder.output_size\n",
        "        \n",
        "        # Initialize tensor for storing outputs\n",
        "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(this.device)\n",
        "\n",
        "        # Initialize hidden states of the encoder\n",
        "        encoder_hidden, encoder_cell = this.encoder.init_hidden(batch_size)\n",
        "\n",
        "        # Forward pass through the encoder\n",
        "        if (this.encoder.cell_type == 'lstm'):\n",
        "            encoder_outputs, encoder_hidden, encoder_cell = this.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "        if (this.encoder.cell_type == 'rnn'):\n",
        "            encoder_outputs, encoder_hidden = this.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "        if (this.encoder.cell_type == 'gru'):\n",
        "            encoder_outputs, encoder_hidden = this.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "\n",
        "        # First input to the decoder is the <sos> tokens\n",
        "        input = target[:,0]\n",
        "        this.sos = target[:,0]\n",
        "\n",
        "        # Initialize hidden states of the decoder\n",
        "        hidden, cell = this.decoder.init_hidden(encoder_hidden, encoder_cell, this.encoder.bidirectional)\n",
        "        \n",
        "        # Iterate over the target sequence\n",
        "        for t in range(1, target_len):\n",
        "            # Forward pass through the decoder\n",
        "            output, hidden, cell = this.decoder.forward(input, hidden, cell)\n",
        "            outputs[:,t] = output.squeeze(1)  # Store the decoder output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(-1)\n",
        "            input = target[:,t] if teacher_force else top1.squeeze(1)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def inference(this, source, target):\n",
        "        # Get dimensions of input\n",
        "        batch_size = source.shape[0]\n",
        "        target_len = this.mtl\n",
        "        target_vocab_size = this.decoder.output_size\n",
        "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(this.device)\n",
        "\n",
        "        # Initialize hidden states of the encoder\n",
        "        encoder_hidden, encoder_cell = this.encoder.init_hidden(batch_size)\n",
        "\n",
        "        # Forward pass through the encoder\n",
        "        if (this.encoder.cell_type == 'lstm'):\n",
        "            encoder_outputs, encoder_hidden, encoder_cell = this.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "        if (this.encoder.cell_type == 'rnn'):\n",
        "            encoder_outputs, encoder_hidden = this.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "        if (this.encoder.cell_type == 'gru'):\n",
        "            encoder_outputs, encoder_hidden = this.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "           \n",
        "        # First input to the decoder is the <sos> token\n",
        "        input = this.sos\n",
        "\n",
        "        # Initialize hidden states of the decoder\n",
        "        hidden, cell = this.decoder.init_hidden(encoder_hidden, encoder_cell, this.encoder.bidirectional)\n",
        "      \n",
        "        # Iterate over the target sequence\n",
        "        for t in range(1, target_len):\n",
        "            # Forward pass through the decoder\n",
        "            output, hidden, cell = this.decoder.forward(input, hidden, cell)\n",
        "            outputs[:,t] = output.squeeze(1)  # Store the decoder output\n",
        "            top1 = output.argmax(-1)\n",
        "            input = top1.squeeze(1)\n",
        "\n",
        "        # print(f\"Outputs = {outputs[:,0]}\")\n",
        "        # print(f\"Targets = {target[:,0]}\")\n",
        "        \n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "6Sc898q4EI0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Commented code\n",
        "\n",
        "# Define the input and output dimensions based on the length of the dictionary\n",
        "input_dim = len(dict)\n",
        "output_dim = len(dict_t)\n",
        "\n",
        "# Set the batch size for training and validation\n",
        "batch_size = 32\n",
        "val_batch_size = 32\n",
        "\n",
        "# Set the embedding dimensions for the encoder and decoder\n",
        "enc_embedding = 256\n",
        "dec_embedding = 256\n",
        "\n",
        "# Set the number of hidden units in the encoder and decoder\n",
        "hidden = 512\n",
        "\n",
        "# Set the number of layers in the encoder and decoder\n",
        "enc_num_layers = 3\n",
        "dec_num_layers = 2\n",
        "\n",
        "# Set the dropout probabilities for the encoder and decoder\n",
        "enc_dropout = 0.4\n",
        "dec_dropout = 0.4\n",
        "\n",
        "# Set the maximum length for the input sequence\n",
        "max_length = mtl\n",
        "\n",
        "# Set the cell type for the encoder and decoder\n",
        "cell_type = 'gru'\n",
        "\n",
        "# Initialize the encoder, decoder, and the overall model\n",
        "enc = EncoderRNN(device, cell_type, input_dim, enc_embedding, hidden, enc_num_layers, bidirectional=True, dropout_p=enc_dropout)\n",
        "dec = DecoderRNN(device, cell_type, output_dim, dec_embedding, hidden, max_length, dec_dropout, dec_num_layers, bidirectional=True)\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "# Function to count the number of trainable parameters in the model\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# Print the number of trainable parameters in the model\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "# Define the optimizer and the loss criterion\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Function to train the model for a given number of iterations\n",
        "def trainIters(model, pairs, batch_size, n_iters, optimizer, tf, print_every=10, plot_every=10, log=True, Attention=False):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    train_char_accuracy = []\n",
        "    train_word_accuracy = []\n",
        "    val_losses = []\n",
        "    val_char_accuracy = []\n",
        "    val_word_accuracy = []\n",
        "    print_loss_total = 0\n",
        "    plot_loss_total = 0\n",
        "    print_val_loss_total = 0\n",
        "    plot_val_loss_total = 0\n",
        "\n",
        "    train_pairs = pairs[0]\n",
        "    val_pairs = pairs[1]\n",
        "    train_accuracy = 0\n",
        "\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    count = 0\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        for i in np.arange(start=0, stop=len(train_pairs) - batch_size, step=batch_size):\n",
        "            train_accuracy = 0\n",
        "            count += 1\n",
        "            if (i + batch_size > len(train_pairs)):\n",
        "                batch_size = len(train_pairs) - i + 1\n",
        "            input_tensor = []\n",
        "            target_tensor = []\n",
        "\n",
        "            for j in range(batch_size):\n",
        "                input_tensor.append(train_pairs[i + j][0])\n",
        "                target_tensor.append(train_pairs[i + j][1])\n",
        "\n",
        "            input_tensor = torch.stack(input_tensor).squeeze(1).long().cuda()\n",
        "            target_tensor = torch.stack(target_tensor).squeeze(1).long().cuda()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            if (count < 4000):\n",
        "                out = model(input_tensor, target_tensor, teacher_forcing_ratio=tf)\n",
        "            else:\n",
        "                out = model(input_tensor, target_tensor, teacher_forcing_ratio=0)\n",
        "\n",
        "            out = torch.permute(out, [0, 2, 1])\n",
        "            loss = criterion(out, target_tensor)\n",
        "\n",
        "            train_accuracy_word = word_acc(target_tensor, out) * batch_size\n",
        "            train_accuracy = train_accuracy + train_accuracy_word\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "            optimizer.step()\n",
        "\n",
        "            print_loss_total += loss\n",
        "            plot_loss_total += loss\n",
        "\n",
        "            if count % 800 == 0:\n",
        "                val_input_tensor = []\n",
        "                val_target_tensor = []\n",
        "\n",
        "                for j in range(batch_size):\n",
        "                    val_input_tensor.append(val_pairs[j][0])\n",
        "                    val_target_tensor.append(val_pairs[j][1])\n",
        "\n",
        "                val_input_tensor = torch.stack(val_input_tensor).squeeze(1).long().cuda()\n",
        "                val_target_tensor = torch.stack(val_target_tensor).squeeze(1).long().cuda()\n",
        "                if (Attention == True):\n",
        "                    val_out, _ = model.inference(val_input_tensor, val_target_tensor)\n",
        "                    val_out = val_out.permute(0, 2, 1)\n",
        "                else:\n",
        "                    val_out = model.inference(val_input_tensor, val_target_tensor)\n",
        "                    val_out = val_out.permute(0, 2, 1)\n",
        "                val_loss = criterion(val_out, val_target_tensor)\n",
        "                val_loss_sampled = val_loss\n",
        "                wandb.log({'Val_Loss': val_loss_sampled})\n",
        "\n",
        "            if count % 800 == 0:\n",
        "                print_loss_avg = print_loss_total / 800\n",
        "                print_loss_total = 0\n",
        "                print('%s (%d %d%%) %.7f' % (span(start, iter / n_iters),\n",
        "                                             iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "            if count % 800 == 0:\n",
        "                plot_loss_avg = plot_loss_total / 800\n",
        "                plot_losses.append(plot_loss_avg.detach())\n",
        "                wandb.log({'Train Loss': plot_loss_avg})\n",
        "\n",
        "                plot_loss_total = 0\n",
        "\n",
        "        train_accuracy = train_accuracy / (len(train_pairs) - batch_size)\n",
        "        train_word_accuracy.append(train_accuracy)\n",
        "\n",
        "    print(train_word_accuracy)\n",
        "    plot_losses = [losses.cpu().numpy() for losses in plot_losses]\n",
        "\n",
        "    char_count = 0\n",
        "    word_count = 0\n",
        "\n",
        "    for i in np.arange(start=0, stop=len(val_pairs) - batch_size, step=batch_size):\n",
        "        if (i + batch_size > len(val_pairs)):\n",
        "            batch_size = len(val_pairs) - i + 1\n",
        "        val_input_tensor = []\n",
        "        val_target_tensor = []\n",
        "        for j in range(batch_size):\n",
        "            val_input_tensor.append(val_pairs[i + j][0])\n",
        "            val_target_tensor.append(val_pairs[i + j][1])\n",
        "\n",
        "        val_input_tensor = torch.stack(val_input_tensor).squeeze(1).long().cuda()\n",
        "        val_target_tensor = torch.stack(val_target_tensor).squeeze(1).long().cuda()\n",
        "        if (Attention == True):\n",
        "            val_out, _ = model.inference(val_input_tensor, val_target_tensor)\n",
        "            val_out = val_out.permute(0, 2, 1)\n",
        "        else:\n",
        "            val_out = model.inference(val_input_tensor, val_target_tensor)\n",
        "            val_out = val_out.permute(0, 2, 1)\n",
        "        val_loss = criterion(val_out, val_target_tensor)\n",
        "\n",
        "        val_accuracy_word = word_acc(val_target_tensor, val_out)\n",
        "        word_count = word_count + (val_accuracy_word) * batch_size\n",
        "\n",
        "    word_accuracy = word_count / (len(val_pairs) - batch_size)\n",
        "\n",
        "    metrics = {'Val_Accuracy': word_accuracy}\n",
        "    wandb.log(metrics)\n",
        "\n",
        "    print(f\"Val loss = {val_loss}\")\n",
        "    print(f'Word-level-accuracy on val set = {word_accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwalnEDxEmHm",
        "outputId": "d7a1ad46-9494-482c-8090-cf2bf28225ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 18,998,083 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter configuration\n",
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'name': 'Hyperparameter Tuning-Bayesian'\n",
        "}\n",
        "\n",
        "# Metric to optimize\n",
        "metric = {\n",
        "    'name': 'Val_Accuracy',\n",
        "    'goal': 'maximize'\n",
        "}\n",
        "\n",
        "# Assign metric to sweep config\n",
        "sweep_config['metric'] = metric\n",
        "\n",
        "# Define parameter values\n",
        "parameters_dict = {\n",
        "    'optimiser': {\n",
        "        'values': ['nadam']\n",
        "    },\n",
        "    'teacher_forcing_ratio': {\n",
        "        'values': [0.5, 0.7]\n",
        "    },\n",
        "    'bidirectional': {\n",
        "        'values': [True]\n",
        "    },\n",
        "    'enc_embedding': {\n",
        "        'values': [128]\n",
        "    },\n",
        "    'dec_embedding': {\n",
        "        'values': [128]\n",
        "    },\n",
        "    'epochs': {\n",
        "        'values': [2]\n",
        "    },\n",
        "    'hidden_size': {\n",
        "        'values': [64, 128, 256]\n",
        "    },\n",
        "    'enc_layers': {\n",
        "        'values': [3]\n",
        "    },\n",
        "    'dec_layers': {\n",
        "        'values': [3]\n",
        "    },\n",
        "    'dropout': {\n",
        "        'values': [0.3]\n",
        "    },\n",
        "    'cell_type': {\n",
        "        'values': ['rnn', 'gru']\n",
        "    }\n",
        "}\n",
        "\n",
        "# Assign parameters to sweep config\n",
        "sweep_config['parameters'] = parameters_dict\n",
        "\n",
        "# Initialize the sweep\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"CS6910 Assignment 3\")\n",
        "\n",
        "\n",
        "def train_sweep(config=None):\n",
        "    with wandb.init(config=config) as run:\n",
        "        config = wandb.config\n",
        "\n",
        "        # Set up model parameters\n",
        "        input_dim = len(dict)\n",
        "        output_dim = len(dict_t)\n",
        "        batch_size = 32\n",
        "        val_batch_size = 32\n",
        "        enc_embedding = config.enc_embedding\n",
        "        dec_embedding = config.dec_embedding\n",
        "        hidden = config.hidden_size\n",
        "        enc_num_layers = config.enc_layers\n",
        "        dec_num_layers = config.dec_layers\n",
        "        enc_dropout = config.dropout\n",
        "        dec_dropout = config.dropout\n",
        "        max_length = mtl\n",
        "        cell_type = config.cell_type\n",
        "\n",
        "        # Initialize the encoder, decoder, and model\n",
        "        enc = EncoderRNN(device, cell_type, input_dim, enc_embedding, hidden, enc_num_layers, bidirectional=config.bidirectional, dropout_p=enc_dropout)\n",
        "        dec = DecoderRNN(device, cell_type, output_dim, dec_embedding, hidden, max_length, dec_dropout, dec_num_layers, bidirectional=config.bidirectional)\n",
        "        model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "        # Set experiment name\n",
        "        exp_name = str(config.cell_type)\n",
        "        exp_name = exp_name\n",
        "        exp_name = exp_name+'_optim_'+ config.optimiser\n",
        "        wandb.run.name = exp_name\n",
        "\n",
        "        # Set optimizer based on config\n",
        "        if (config.optimiser == 'adam'):\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "        elif (config.optimiser == 'nadam'):\n",
        "            optimizer = torch.optim.NAdam(model.parameters(), lr=0.001)\n",
        "\n",
        "        # Set loss criterion\n",
        "        criterion = nn.NLLLoss()\n",
        "\n",
        "        # Train the model\n",
        "        trainIters(model, pairs, 32, config.epochs, optimizer, config.teacher_forcing_ratio)\n",
        "\n",
        "\n",
        "# Run the sweep\n",
        "wandb.agent(sweep_id, train_sweep, count=1)\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        },
        "id": "zxuzJlxzE2__",
        "outputId": "bc64fcae-19d9-438a-8af0-f445c70a9178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: 6inmyl1l\n",
            "Sweep URL: https://wandb.ai/ed22s009/CS6910%20Assignment%203/sweeps/6inmyl1l\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tklzn5py with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: rnn\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_embedding: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_embedding: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimiser: nadam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33med22s009\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230521_114343-tklzn5py</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ed22s009/CS6910%20Assignment%203/runs/tklzn5py' target=\"_blank\">volcanic-sweep-1</a></strong> to <a href='https://wandb.ai/ed22s009/CS6910%20Assignment%203' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ed22s009/CS6910%20Assignment%203/sweeps/6inmyl1l' target=\"_blank\">https://wandb.ai/ed22s009/CS6910%20Assignment%203/sweeps/6inmyl1l</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ed22s009/CS6910%20Assignment%203' target=\"_blank\">https://wandb.ai/ed22s009/CS6910%20Assignment%203</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/ed22s009/CS6910%20Assignment%203/sweeps/6inmyl1l' target=\"_blank\">https://wandb.ai/ed22s009/CS6910%20Assignment%203/sweeps/6inmyl1l</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ed22s009/CS6910%20Assignment%203/runs/tklzn5py' target=\"_blank\">https://wandb.ai/ed22s009/CS6910%20Assignment%203/runs/tklzn5py</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0, 0.0]\n",
            "Val loss = 1.4987664222717285\n",
            "Word-level-accuracy on val set = 0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Val_Accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Val_Accuracy</td><td>0.0</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">volcanic-sweep-1</strong> at: <a href='https://wandb.ai/ed22s009/CS6910%20Assignment%203/runs/tklzn5py' target=\"_blank\">https://wandb.ai/ed22s009/CS6910%20Assignment%203/runs/tklzn5py</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230521_114343-tklzn5py/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}