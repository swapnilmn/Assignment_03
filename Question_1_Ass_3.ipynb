{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxkgQ1Ea2PYM9fwKxaMgYE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swapnilmn/Assignment_03/blob/main/Question_1_Ass_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1 (15 Marks)\n",
        "Build a RNN based seq2seq model which contains the following layers: (i) input layer for character embeddings (ii) one encoder RNN which sequentially encodes the input character sequence (Latin) (iii) one decoder RNN which takes the last state of the encoder as input and produces one output character at a time (Devanagari). \n"
      ],
      "metadata": {
        "id": "Sha-7_A_8x99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Librares"
      ],
      "metadata": {
        "id": "6o7LlMzW9ALi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QXFkT1I8tdY"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "!pip install wandb -qU\n",
        "import wandb\n",
        "wandb.login()\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load of data"
      ],
      "metadata": {
        "id": "TMNzTbAT9D3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math  # Import the math module for mathematical operations\n",
        "import time  # Import the time module for time-related functions\n",
        "import torch  # Import the torch module for working with tensors\n",
        "import numpy as np  # Import the numpy module for numerical operations\n",
        "\n",
        "def asMinutes(s):\n",
        "    # Convert seconds to minutes and seconds\n",
        "    m = math.floor(s / 60)  # Calculate the number of minutes\n",
        "    s -= m * 60  # Subtract the minutes converted to seconds from the total seconds\n",
        "    return '%dm %ds' % (m, s)  # Return the formatted string for minutes and seconds\n",
        "\n",
        "def span(since, percent):\n",
        "    # Calculate the elapsed time and estimated remaining time\n",
        "    now = time.time()  # Get the current time\n",
        "    s = now - since  # Calculate the elapsed time in seconds\n",
        "    es = s / (percent)  # Calculate the estimated total time in seconds\n",
        "    rs = es - s  # Calculate the remaining time in seconds\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))  # Return the formatted string for elapsed time and remaining time\n",
        "\n",
        "def key_presenting(dictionary, values):\n",
        "    # Find keys in the dictionary that match the given values\n",
        "    found_keys = []  # Initialize an empty list to store the found keys\n",
        "    for value in values:  # Iterate over the given values\n",
        "        for key, val in dictionary.items():  # Iterate over the dictionary items\n",
        "            if value == val:  # If the value matches the dictionary value\n",
        "                found_keys.append(key)  # Add the key to the list of found keys\n",
        "    \n",
        "    if found_keys:  # If any keys are found\n",
        "        str = ''.join(found_keys)  # Concatenate the found keys into a single string\n",
        "        if str[-1] == '\\n':  # If the string ends with a newline character\n",
        "            str = str[:-1]  # Remove the newline character from the end of the string\n",
        "        elif str[0] == '\\t':  # If the string starts with a tab character\n",
        "            str = str[1:]  # Remove the tab character from the start of the string\n",
        "    else:  # If no keys are found\n",
        "        print(\"No keys found for the given values.\")  # Print a message indicating no keys were found\n",
        "    \n",
        "    return str  # Return the string containing the found keys\n",
        "\n",
        "def char_acc(targets, outputs):\n",
        "    # Calculate character-level accuracy\n",
        "    with torch.no_grad():  # Disable gradient calculation for efficiency\n",
        "        count = 0  # Initialize a counter for correct predictions\n",
        "        total_count = 0  # Initialize a counter for total predictions\n",
        "        for i in range(targets.shape[0]):  # Iterate over the rows of the targets tensor\n",
        "            same_elements = []  # Initialize an empty list to store the comparison results\n",
        "            for j in range(targets.shape[1]):  # Iterate over the columns of the targets tensor\n",
        "                if targets[i][j] != 67 or targets[i][j] != 66:  # Check if the target value is not equal to 67 or 66\n",
        "                    same_elements.append(outputs[i][j].item() == targets[i][j].item())  # Append the comparison result to the list\n",
        "            count += np.sum(same_elements)  # Add the number of True values in the list to the count\n",
        "            total_count += len(same_elements)  # Add the total number of elements in the list to the total count\n",
        "    return count / total_count  # Return the character-level accuracy as a ratio of correct predictions to total predictions\n",
        "\n",
        "def word_acc(targets, outputs):\n",
        "    # Calculate word-level accuracy\n",
        "    outputs1 = torch.argmax(outputs, dim=1)  # Get the index of the maximum value along the second dimension of the outputs tensor\n",
        "    with torch.no_grad():  # Disable gradient calculation for efficiency\n",
        "        count = 0  # Initialize a counter for correct predictions\n",
        "        for i in range(targets.shape[0]):  # Iterate over the rows of the targets tensor\n",
        "            if (outputs1[i] == targets[i]).sum().item() == targets.shape[1]:  # Check if all elements in the row are equal\n",
        "                count = count + 1  # Increment the count if all elements are equal\n",
        "    return count / targets.shape[0]  # Return the word-level accuracy as a ratio of correct predictions to total predictions\n",
        "\n",
        "def equi_points(data, epochs):\n",
        "    # Sample equidistant points from data\n",
        "    step = len(data) // epochs  # Calculate the step size to sample equidistant points\n",
        "    indices = np.arange(0, len(data), step)  # Generate indices using numpy arange function\n",
        "    equidistant_points = [data[i] for i in indices]  # Extract the equidistant points from the data list\n",
        "    \n",
        "    return equidistant_points  # Return the list of equidistant points\n"
      ],
      "metadata": {
        "id": "_25eBZBX9Klx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def read_csv_files():\n",
        "    # Define the file paths for the CSV files\n",
        "    file_paths = {\n",
        "        'file1': '/content/drive/MyDrive/aksharantar_sampled/mar/mar_test.csv',  # Path to 'mar_test.csv'\n",
        "        'file2': '/content/drive/MyDrive/aksharantar_sampled/mar/mar_valid.csv',  # Path to 'mar_valid.csv'\n",
        "        'file3': '/content/drive/MyDrive/aksharantar_sampled/mar/mar_train.csv'  # Path to 'mar_train.csv'\n",
        "    }\n",
        "\n",
        "    data = {}  # Dictionary to store the CSV data\n",
        "\n",
        "    for file_key, file_path in file_paths.items():\n",
        "        with open(file_path) as file:\n",
        "            csv_reader = csv.reader(file)\n",
        "            header = next(csv_reader)  # Get the header row of the CSV\n",
        "            rows = [row for row in csv_reader]  # Get all the data rows of the CSV\n",
        "            data[file_key] = {\n",
        "                'header': header,\n",
        "                'rows': rows\n",
        "            }\n",
        "\n",
        "    return data\n",
        "\n",
        "# Execute the function to read CSV files\n",
        "csv_data = read_csv_files()\n",
        "\n",
        "# Access the data\n",
        "test = csv_data['file1']['rows']  # Contains rows from 'mar_test.csv'\n",
        "val = csv_data['file2']['rows']  # Contains rows from 'mar_valid.csv'\n",
        "train = csv_data['file3']['rows']  # Contains rows from 'mar_train.csv'\n"
      ],
      "metadata": {
        "id": "3LTAnC4Q9NrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "bCIYZJEL9Qe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to read data from a list of pairs\n",
        "def Data_Reading(list):\n",
        "  i = []  # List to store the first element of each pair\n",
        "  t = []  # List to store the second element of each pair\n",
        "  for pair in list:\n",
        "    i.append(pair[0])  # Append the first element to 'i'\n",
        "    t.append(pair[1])  # Append the second element to 't'\n",
        "  return i, t  # Return the two lists\n",
        "\n",
        "# Read data from the 'train' list using the 'Data_Reading' function\n",
        "train_inputs, train_targets = Data_Reading(train)\n",
        "\n",
        "# Read data from the 'test' list using the 'Data_Reading' function\n",
        "test_inputs, test_targets = Data_Reading(test)\n",
        "\n",
        "# Read data from the 'val' list using the 'Data_Reading' function\n",
        "val_inputs, val_targets = Data_Reading(val)\n",
        "\n",
        "# Print the second element of the 'train_inputs' list\n",
        "print(train_inputs[1])\n",
        "\n",
        "# Print the second element of the 'train_targets' list\n",
        "print(train_targets[1])\n",
        "\n",
        "# Define a dictionary to store variable values\n",
        "variable_dict = {\n",
        "    'sc': '\\t',    # 'sc' represents a tab character\n",
        "    'eb': '\\n',    # 'eb' represents a newline character\n",
        "    'bc ': ' ',    # 'bc' represents a space character\n",
        "    'unc': '\\r'    # 'unc' represents a carriage return character\n",
        "}\n",
        "\n",
        "# Accessing the values from the dictionary\n",
        "sc = variable_dict['sc']    # Assign the value of '\\t' to 'sc'\n",
        "eb = variable_dict['eb']    # Assign the value of '\\n' to 'eb'\n",
        "bc  = variable_dict['bc ']  # Assign the value of ' ' to 'bc'\n",
        "unc = variable_dict['unc']  # Assign the value of '\\r' to 'unc'\n",
        "\n",
        "\n",
        "# Function to create dictionaries for language encoding\n",
        "\n",
        "def dictlang(inputs, targets):\n",
        "    dict = {}  # Dictionary for input language\n",
        "    mil = 0  # Maximum input language length\n",
        "    ichar = []  # List of characters in input language\n",
        "\n",
        "    dict_t = {}  # Dictionary for target language\n",
        "    mtl = 0  # Maximum target language length\n",
        "    tchar = []  # List of characters in target language\n",
        "\n",
        "    for s in inputs:\n",
        "        mil = max(len(s), mil)\n",
        "        for char in s:\n",
        "            if char not in dict:\n",
        "                dict[char] = len(ichar)\n",
        "                ichar.append(char)\n",
        "\n",
        "    # Add special characters if they are not present in the input language dictionary\n",
        "    if bc not in dict:\n",
        "        dict[bc] = len(ichar)\n",
        "        ichar.append(bc)\n",
        "\n",
        "    dict[unc] = len(ichar)\n",
        "    ichar.append(unc)\n",
        "\n",
        "    # Add special characters if they are not present in the target language dictionary\n",
        "    if sc not in dict_t:\n",
        "        dict_t[sc] = len(tchar)\n",
        "        tchar.append(sc)\n",
        "\n",
        "    for s in targets:\n",
        "        mtl = max(len(s) + 2, mtl)\n",
        "        for char in s:\n",
        "            if char not in dict_t:\n",
        "                dict_t[char] = len(tchar)\n",
        "                tchar.append(char)\n",
        "\n",
        "    # Add special characters if they are not present in the target language dictionary\n",
        "    if eb not in dict_t:\n",
        "        dict_t[eb] = len(tchar)\n",
        "        tchar.append(eb)\n",
        "\n",
        "    # Add special characters if they are not present in the target language dictionary\n",
        "    if bc not in dict_t:\n",
        "        dict_t[bc] = len(tchar)\n",
        "        tchar.append(bc)\n",
        "\n",
        "    return dict, mil, ichar, dict_t, mtl, tchar\n",
        "\n",
        "# Create dictionaries for language encoding using training, validation, and test inputs and targets\n",
        "dict, mil, ichar, dict_t, mtl, tchar = dictlang(train_inputs + val_inputs + test_inputs, train_targets + val_targets + test_targets)\n",
        "\n",
        "\n",
        "def encoding_w(a, d, ml, l):\n",
        "    encs = []  # List to store encoded words\n",
        "    for word in a:\n",
        "        enc = []  # List to store encoded characters of a word\n",
        "        for char in word:\n",
        "            if char in d:\n",
        "                enc.append(d[char])  # Encode character if present in the dictionary\n",
        "            else:\n",
        "                enc.append(d[unc])  # Encode unknown character\n",
        "        if (l == 0):\n",
        "            while len(enc) < ml:\n",
        "                enc.append(d[bc])  # Pad with blank character if required\n",
        "        if (l == 1):\n",
        "            enc.insert(0, d[sc])  # Insert start character at the beginning\n",
        "            while len(enc) < ml - 1:\n",
        "                enc.append(d[bc])  # Pad with blank character if required\n",
        "            enc.append(d[eb])  # Append end character at the end\n",
        "        encs.append(enc)  # Add encoded word to the list\n",
        "    return encs\n",
        "\n",
        "def tokenize(train,val,test,dict,dict_t,mil,mtl):\n",
        "    # Tokenize the data for training, validation, and testing sets\n",
        "    ti, tt = Data_Reading(train)  # Read training data\n",
        "    tti, ttt = Data_Reading(test)  # Read testing data\n",
        "    vi, vt = Data_Reading(val)  # Read validation data\n",
        "\n",
        "    # Encode the training, validation, and testing data\n",
        "    e_t_i = encoding_w(ti,dict,mil,0)  # Encode training input data\n",
        "    e_t_t = encoding_w(tt,dict_t,mtl,1)  # Encode training target data\n",
        "    e_v_i = encoding_w(vi,dict,mil,0)  # Encode validation input data\n",
        "    e_v_t = encoding_w(vt,dict_t,mtl,1)  # Encode validation target data\n",
        "    e_tt_i = encoding_w(tti,dict,mil,0)  # Encode testing input data\n",
        "    e_tt_t = encoding_w(ttt,dict_t,mtl,1)  # Encode testing target data\n",
        "\n",
        "    return e_t_i, e_t_t, e_v_i, e_v_t, e_tt_i, e_tt_t\n",
        "\n",
        "# Tokenize the data using the given parameters\n",
        "e_t_i, e_t_t, e_v_i, e_v_t, e_tt_i, e_tt_t = tokenize(train, val, test, dict, dict_t, mil, mtl)\n",
        "\n",
        "# Generate a random number within the range [0, 100]\n",
        "r = random.randint(0, 100)\n",
        "\n",
        "# Print the keys corresponding to the values of the encoded data at index r\n",
        "print(key_presenting(dict, e_t_i[int(r)]))\n",
        "print(key_presenting(dict_t, e_t_t[int(r)]))\n",
        "\n",
        "\n",
        "# Define a function to convert input and target data into tensor pairs\n",
        "def tensor_conversion(ti, tg):\n",
        "    pr = []\n",
        "    for id, td in zip(ti, tg):\n",
        "        # Convert each item in ti to a torch tensor\n",
        "        it = torch.tensor(id)\n",
        "        # Convert each item in tg to a torch tensor\n",
        "        tt = torch.tensor(td)\n",
        "        # Append the tensor pair to the result list\n",
        "        pr.append((it, tt))\n",
        "    return pr\n",
        "\n",
        "# Convert e_t_i and e_t_t to tensor pairs\n",
        "e_t_p = tensor_conversion(e_t_i, e_t_t)\n",
        "\n",
        "# Convert e_v_i and e_v_t to tensor pairs\n",
        "e_v_p = tensor_conversion(e_v_i, e_v_t)\n",
        "\n",
        "# Re-assign e_t_p with new tensor pairs converted from e_tt_i and e_tt_t\n",
        "e_t_p = tensor_conversion(e_tt_i, e_tt_t)\n",
        "\n",
        "# Create a tuple containing e_t_p, e_v_p, and e_t_p\n",
        "pairs = (e_t_p, e_v_p, e_t_p)\n",
        "\n",
        "# Randomly choose a pair from e_t_p\n",
        "pair = random.choice(e_t_p)\n",
        "\n",
        "# Print the keys in dict corresponding to the values in pair[0]\n",
        "print(key_presenting(dict, pair[0]))\n",
        "\n",
        "# Print the keys in dict_t corresponding to the values in pair[1]\n",
        "print(key_presenting(dict_t, pair[1]))\n"
      ],
      "metadata": {
        "id": "8JDuRQwV9VNN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}