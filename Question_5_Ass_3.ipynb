{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "234d251ea8104dee819b39236173e892": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_29d54038e5b54146b32e30cfd321b436",
              "IPY_MODEL_e17e870ac7a5438985075ad3fd02c683"
            ],
            "layout": "IPY_MODEL_6c3a71b7f635436f833bc2a0817a8ecf"
          }
        },
        "29d54038e5b54146b32e30cfd321b436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60a51484c18f4b66ae1673ff6bb9c66a",
            "placeholder": "​",
            "style": "IPY_MODEL_ed7bdf2d5998416788850d04ad328872",
            "value": "0.001 MB of 0.009 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "e17e870ac7a5438985075ad3fd02c683": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea2c226883c94ad18d6b6bbf3681af51",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_de1c94e1730b4738b61030388ec62a18",
            "value": 0.11919660880096891
          }
        },
        "6c3a71b7f635436f833bc2a0817a8ecf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60a51484c18f4b66ae1673ff6bb9c66a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed7bdf2d5998416788850d04ad328872": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea2c226883c94ad18d6b6bbf3681af51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de1c94e1730b4738b61030388ec62a18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swapnilmn/Assignment_03/blob/main/Question_5_Ass_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1 (15 Marks)\n",
        "Build a RNN based seq2seq model which contains the following layers: (i) input layer for character embeddings (ii) one encoder RNN which sequentially encodes the input character sequence (Latin) (iii) one decoder RNN which takes the last state of the encoder as input and produces one output character at a time (Devanagari). \n"
      ],
      "metadata": {
        "id": "Sha-7_A_8x99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Librares"
      ],
      "metadata": {
        "id": "6o7LlMzW9ALi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QXFkT1I8tdY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "18f2a8e8-5123-4742-dfb6-35870ca4b87a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.1/205.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m783.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "!pip install wandb -qU\n",
        "import wandb\n",
        "wandb.login()\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load of data"
      ],
      "metadata": {
        "id": "TMNzTbAT9D3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math  # Import the math module for mathematical operations\n",
        "import time  # Import the time module for time-related functions\n",
        "import torch  # Import the torch module for working with tensors\n",
        "import numpy as np  # Import the numpy module for numerical operations\n",
        "\n",
        "def asMinutes(s):\n",
        "    # Convert seconds to minutes and seconds\n",
        "    m = math.floor(s / 60)  # Calculate the number of minutes\n",
        "    s -= m * 60  # Subtract the minutes converted to seconds from the total seconds\n",
        "    return '%dm %ds' % (m, s)  # Return the formatted string for minutes and seconds\n",
        "\n",
        "def span(since, percent):\n",
        "    # Calculate the elapsed time and estimated remaining time\n",
        "    now = time.time()  # Get the current time\n",
        "    s = now - since  # Calculate the elapsed time in seconds\n",
        "    es = s / (percent)  # Calculate the estimated total time in seconds\n",
        "    rs = es - s  # Calculate the remaining time in seconds\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))  # Return the formatted string for elapsed time and remaining time\n",
        "\n",
        "def key_presenting(dictionary, values):\n",
        "    # Find keys in the dictionary that match the given values\n",
        "    found_keys = []  # Initialize an empty list to store the found keys\n",
        "    for value in values:  # Iterate over the given values\n",
        "        for key, val in dictionary.items():  # Iterate over the dictionary items\n",
        "            if value == val:  # If the value matches the dictionary value\n",
        "                found_keys.append(key)  # Add the key to the list of found keys\n",
        "    \n",
        "    if found_keys:  # If any keys are found\n",
        "        str = ''.join(found_keys)  # Concatenate the found keys into a single string\n",
        "        if str[-1] == '\\n':  # If the string ends with a newline character\n",
        "            str = str[:-1]  # Remove the newline character from the end of the string\n",
        "        elif str[0] == '\\t':  # If the string starts with a tab character\n",
        "            str = str[1:]  # Remove the tab character from the start of the string\n",
        "    else:  # If no keys are found\n",
        "        print(\"No keys found for the given values.\")  # Print a message indicating no keys were found\n",
        "    \n",
        "    return str  # Return the string containing the found keys\n",
        "\n",
        "def char_acc(targets, outputs):\n",
        "    # Calculate character-level accuracy\n",
        "    with torch.no_grad():  # Disable gradient calculation for efficiency\n",
        "        count = 0  # Initialize a counter for correct predictions\n",
        "        total_count = 0  # Initialize a counter for total predictions\n",
        "        for i in range(targets.shape[0]):  # Iterate over the rows of the targets tensor\n",
        "            same_elements = []  # Initialize an empty list to store the comparison results\n",
        "            for j in range(targets.shape[1]):  # Iterate over the columns of the targets tensor\n",
        "                if targets[i][j] != 67 or targets[i][j] != 66:  # Check if the target value is not equal to 67 or 66\n",
        "                    same_elements.append(outputs[i][j].item() == targets[i][j].item())  # Append the comparison result to the list\n",
        "            count += np.sum(same_elements)  # Add the number of True values in the list to the count\n",
        "            total_count += len(same_elements)  # Add the total number of elements in the list to the total count\n",
        "    return count / total_count  # Return the character-level accuracy as a ratio of correct predictions to total predictions\n",
        "\n",
        "def word_acc(targets, outputs):\n",
        "    # Calculate word-level accuracy\n",
        "    outputs1 = torch.argmax(outputs, dim=1)  # Get the index of the maximum value along the second dimension of the outputs tensor\n",
        "    with torch.no_grad():  # Disable gradient calculation for efficiency\n",
        "        count = 0  # Initialize a counter for correct predictions\n",
        "        for i in range(targets.shape[0]):  # Iterate over the rows of the targets tensor\n",
        "            if (outputs1[i] == targets[i]).sum().item() == targets.shape[1]:  # Check if all elements in the row are equal\n",
        "                count = count + 1  # Increment the count if all elements are equal\n",
        "    return count / targets.shape[0]  # Return the word-level accuracy as a ratio of correct predictions to total predictions\n",
        "\n",
        "def equi_points(data, epochs):\n",
        "    # Sample equidistant points from data\n",
        "    step = len(data) // epochs  # Calculate the step size to sample equidistant points\n",
        "    indices = np.arange(0, len(data), step)  # Generate indices using numpy arange function\n",
        "    equidistant_points = [data[i] for i in indices]  # Extract the equidistant points from the data list\n",
        "    \n",
        "    return equidistant_points  # Return the list of equidistant points\n"
      ],
      "metadata": {
        "id": "_25eBZBX9Klx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def read_csv_files():\n",
        "    # Define the file paths for the CSV files\n",
        "    file_paths = {\n",
        "        'file1': '/content/drive/MyDrive/aksharantar_sampled/mar/mar_test.csv',  # Path to 'mar_test.csv'\n",
        "        'file2': '/content/drive/MyDrive/aksharantar_sampled/mar/mar_valid.csv',  # Path to 'mar_valid.csv'\n",
        "        'file3': '/content/drive/MyDrive/aksharantar_sampled/mar/mar_train.csv'  # Path to 'mar_train.csv'\n",
        "    }\n",
        "\n",
        "    data = {}  # Dictionary to store the CSV data\n",
        "\n",
        "    for file_key, file_path in file_paths.items():\n",
        "        with open(file_path) as file:\n",
        "            csv_reader = csv.reader(file)\n",
        "            header = next(csv_reader)  # Get the header row of the CSV\n",
        "            rows = [row for row in csv_reader]  # Get all the data rows of the CSV\n",
        "            data[file_key] = {\n",
        "                'header': header,\n",
        "                'rows': rows\n",
        "            }\n",
        "\n",
        "    return data\n",
        "\n",
        "# Execute the function to read CSV files\n",
        "csv_data = read_csv_files()\n",
        "\n",
        "# Access the data\n",
        "test = csv_data['file1']['rows']  # Contains rows from 'mar_test.csv'\n",
        "val = csv_data['file2']['rows']  # Contains rows from 'mar_valid.csv'\n",
        "train = csv_data['file3']['rows']  # Contains rows from 'mar_train.csv'\n"
      ],
      "metadata": {
        "id": "3LTAnC4Q9NrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "bCIYZJEL9Qe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to read data from a list of pairs\n",
        "def Data_Reading(list):\n",
        "  i = []  # List to store the first element of each pair\n",
        "  t = []  # List to store the second element of each pair\n",
        "  for pair in list:\n",
        "    i.append(pair[0])  # Append the first element to 'i'\n",
        "    t.append(pair[1])  # Append the second element to 't'\n",
        "  return i, t  # Return the two lists\n",
        "\n",
        "# Read data from the 'train' list using the 'Data_Reading' function\n",
        "train_inputs, train_targets = Data_Reading(train)\n",
        "\n",
        "# Read data from the 'test' list using the 'Data_Reading' function\n",
        "test_inputs, test_targets = Data_Reading(test)\n",
        "\n",
        "# Read data from the 'val' list using the 'Data_Reading' function\n",
        "val_inputs, val_targets = Data_Reading(val)\n",
        "\n",
        "# Print the second element of the 'train_inputs' list\n",
        "# print(train_inputs[1])\n",
        "\n",
        "# Print the second element of the 'train_targets' list\n",
        "# print(train_targets[1])\n",
        "\n",
        "# Define a dictionary to store variable values\n",
        "variable_dict = {\n",
        "    'sc': '\\t',    # 'sc' represents a tab character\n",
        "    'eb': '\\n',    # 'eb' represents a newline character\n",
        "    'bc ': ' ',    # 'bc' represents a space character\n",
        "    'unc': '\\r'    # 'unc' represents a carriage return character\n",
        "}\n",
        "\n",
        "# Accessing the values from the dictionary\n",
        "sc = variable_dict['sc']    # Assign the value of '\\t' to 'sc'\n",
        "eb = variable_dict['eb']    # Assign the value of '\\n' to 'eb'\n",
        "bc  = variable_dict['bc ']  # Assign the value of ' ' to 'bc'\n",
        "unc = variable_dict['unc']  # Assign the value of '\\r' to 'unc'\n",
        "\n",
        "\n",
        "# Function to create dictionaries for language encoding\n",
        "\n",
        "def dictlang(inputs, targets):\n",
        "    dict = {}  # Dictionary for input language\n",
        "    mil = 0  # Maximum input language length\n",
        "    ichar = []  # List of characters in input language\n",
        "\n",
        "    dict_t = {}  # Dictionary for target language\n",
        "    mtl = 0  # Maximum target language length\n",
        "    tchar = []  # List of characters in target language\n",
        "\n",
        "    for s in inputs:\n",
        "        mil = max(len(s), mil)\n",
        "        for char in s:\n",
        "            if char not in dict:\n",
        "                dict[char] = len(ichar)\n",
        "                ichar.append(char)\n",
        "\n",
        "    # Add special characters if they are not present in the input language dictionary\n",
        "    if bc not in dict:\n",
        "        dict[bc] = len(ichar)\n",
        "        ichar.append(bc)\n",
        "\n",
        "    dict[unc] = len(ichar)\n",
        "    ichar.append(unc)\n",
        "\n",
        "    # Add special characters if they are not present in the target language dictionary\n",
        "    if sc not in dict_t:\n",
        "        dict_t[sc] = len(tchar)\n",
        "        tchar.append(sc)\n",
        "\n",
        "    for s in targets:\n",
        "        mtl = max(len(s) + 2, mtl)\n",
        "        for char in s:\n",
        "            if char not in dict_t:\n",
        "                dict_t[char] = len(tchar)\n",
        "                tchar.append(char)\n",
        "\n",
        "    # Add special characters if they are not present in the target language dictionary\n",
        "    if eb not in dict_t:\n",
        "        dict_t[eb] = len(tchar)\n",
        "        tchar.append(eb)\n",
        "\n",
        "    # Add special characters if they are not present in the target language dictionary\n",
        "    if bc not in dict_t:\n",
        "        dict_t[bc] = len(tchar)\n",
        "        tchar.append(bc)\n",
        "\n",
        "    return dict, mil, ichar, dict_t, mtl, tchar\n",
        "\n",
        "# Create dictionaries for language encoding using training, validation, and test inputs and targets\n",
        "dict, mil, ichar, dict_t, mtl, tchar = dictlang(train_inputs + val_inputs + test_inputs, train_targets + val_targets + test_targets)\n",
        "\n",
        "\n",
        "def encoding_w(a, d, ml, l):\n",
        "    encs = []  # List to store encoded words\n",
        "    for word in a:\n",
        "        enc = []  # List to store encoded characters of a word\n",
        "        for char in word:\n",
        "            if char in d:\n",
        "                enc.append(d[char])  # Encode character if present in the dictionary\n",
        "            else:\n",
        "                enc.append(d[unc])  # Encode unknown character\n",
        "        if (l == 0):\n",
        "            while len(enc) < ml:\n",
        "                enc.append(d[bc])  # Pad with blank character if required\n",
        "        if (l == 1):\n",
        "            enc.insert(0, d[sc])  # Insert start character at the beginning\n",
        "            while len(enc) < ml - 1:\n",
        "                enc.append(d[bc])  # Pad with blank character if required\n",
        "            enc.append(d[eb])  # Append end character at the end\n",
        "        encs.append(enc)  # Add encoded word to the list\n",
        "    return encs\n",
        "\n",
        "def tokenize(train,val,test,dict,dict_t,mil,mtl):\n",
        "    # Tokenize the data for training, validation, and testing sets\n",
        "    ti, tt = Data_Reading(train)  # Read training data\n",
        "    tti, ttt = Data_Reading(test)  # Read testing data\n",
        "    vi, vt = Data_Reading(val)  # Read validation data\n",
        "\n",
        "    # Encode the training, validation, and testing data\n",
        "    e_t_i = encoding_w(ti,dict,mil,0)  # Encode training input data\n",
        "    e_t_t = encoding_w(tt,dict_t,mtl,1)  # Encode training target data\n",
        "    e_v_i = encoding_w(vi,dict,mil,0)  # Encode validation input data\n",
        "    e_v_t = encoding_w(vt,dict_t,mtl,1)  # Encode validation target data\n",
        "    e_tt_i = encoding_w(tti,dict,mil,0)  # Encode testing input data\n",
        "    e_tt_t = encoding_w(ttt,dict_t,mtl,1)  # Encode testing target data\n",
        "\n",
        "    return e_t_i, e_t_t, e_v_i, e_v_t, e_tt_i, e_tt_t\n",
        "\n",
        "# Tokenize the data using the given parameters\n",
        "e_t_i, e_t_t, e_v_i, e_v_t, e_tt_i, e_tt_t = tokenize(train, val, test, dict, dict_t, mil, mtl)\n",
        "\n",
        "# Generate a random number within the range [0, 100]\n",
        "r = random.randint(0, 100)\n",
        "\n",
        "# Print the keys corresponding to the values of the encoded data at index r\n",
        "# print(key_presenting(dict, e_t_i[int(r)]))\n",
        "# print(key_presenting(dict_t, e_t_t[int(r)]))\n",
        "\n",
        "\n",
        "# Define a function to convert input and target data into tensor pairs\n",
        "def tensor_conversion(ti, tg):\n",
        "    pr = []\n",
        "    for id, td in zip(ti, tg):\n",
        "        # Convert each item in ti to a torch tensor\n",
        "        it = torch.tensor(id)\n",
        "        # Convert each item in tg to a torch tensor\n",
        "        tt = torch.tensor(td)\n",
        "        # Append the tensor pair to the result list\n",
        "        pr.append((it, tt))\n",
        "    return pr\n",
        "\n",
        "# Convert e_t_i and e_t_t to tensor pairs\n",
        "e_t_p = tensor_conversion(e_t_i, e_t_t)\n",
        "\n",
        "# Convert e_v_i and e_v_t to tensor pairs\n",
        "e_v_p = tensor_conversion(e_v_i, e_v_t)\n",
        "\n",
        "# Re-assign e_t_p with new tensor pairs converted from e_tt_i and e_tt_t\n",
        "e_t_p = tensor_conversion(e_tt_i, e_tt_t)\n",
        "\n",
        "# Create a tuple containing e_t_p, e_v_p, and e_t_p\n",
        "pairs = (e_t_p, e_v_p, e_t_p)\n",
        "\n",
        "# Randomly choose a pair from e_t_p\n",
        "pair = random.choice(e_t_p)\n",
        "\n",
        "# Print the keys in dict corresponding to the values in pair[0]\n",
        "# print(key_presenting(dict, pair[0]))\n",
        "\n",
        "# Print the keys in dict_t corresponding to the values in pair[1]\n",
        "# print(key_presenting(dict_t, pair[1]))\n"
      ],
      "metadata": {
        "id": "8JDuRQwV9VNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(this, device, cell_type, vocab_size, embed_dim, hidden_size, num_layers=1, bidirectional=False, dropout_p=0):\n",
        "        super(EncoderRNN, this).__init__()\n",
        "        this.embedding = nn.Embedding(vocab_size, embed_dim)  # Embedding layer for converting input indices to dense vectors.\n",
        "        this.hidden_size = hidden_size  # Dimensionality of the hidden state of the RNN.\n",
        "        this.num_layers = num_layers  # Number of recurrent layers.\n",
        "        this.bidirectional = bidirectional  # Flag indicating whether the RNN is bidirectional.\n",
        "        this.cell_type = cell_type  # Type of the RNN cell: 'lstm', 'rnn', or 'gru'.\n",
        "        this.dropout_p = dropout_p  # Dropout probability.\n",
        "        this.dropout = nn.Dropout(this.dropout_p)  # Dropout layer for regularization.\n",
        "        if cell_type == 'lstm':\n",
        "            this.rnn = nn.LSTM(embed_dim, hidden_size, num_layers=this.num_layers, batch_first=True, dropout=dropout_p, bidirectional=this.bidirectional)  # LSTM RNN module.\n",
        "        elif cell_type == 'rnn':\n",
        "            this.rnn = nn.RNN(embed_dim, hidden_size, num_layers=this.num_layers, batch_first=True, dropout=dropout_p, bidirectional=this.bidirectional)  # Vanilla RNN module.\n",
        "        elif cell_type == 'gru':\n",
        "            this.rnn = nn.GRU(embed_dim, hidden_size, num_layers=this.num_layers, batch_first=True, dropout=dropout_p, bidirectional=this.bidirectional)  # GRU module.\n",
        "\n",
        "    def forward(this, x, hidden, cell):\n",
        "        out = this.embedding(x)  # Convert input indices to dense vectors.\n",
        "        out = this.dropout(out)  # Apply dropout to the input.\n",
        "        if this.cell_type == 'lstm':\n",
        "            out, (hidden, cell) = this.rnn(out, (hidden, cell))  # Forward pass through the LSTM RNN.\n",
        "            return out, hidden, cell  # Return the output, hidden state, and cell state.\n",
        "        elif this.cell_type == 'rnn':\n",
        "            out, hidden = this.rnn(out, hidden)  # Forward pass through the vanilla RNN.\n",
        "            return out, hidden  # Return the output and hidden state.\n",
        "        elif this.cell_type == 'gru':\n",
        "            out, hidden = this.rnn(out, hidden)  # Forward pass through the GRU.\n",
        "            return out, hidden  # Return the output and hidden state.\n",
        "\n",
        "    def init_hidden(this, batch_size):\n",
        "        hidden = torch.randn((1 + int(this.bidirectional)) * this.num_layers, batch_size, this.hidden_size, device=device)  # Initialize the hidden state tensor.\n",
        "        cell = torch.randn((1 + int(this.bidirectional)) * this.num_layers, batch_size, this.hidden_size, device=device)  # Initialize the cell state tensor.\n",
        "        return hidden, cell\n"
      ],
      "metadata": {
        "id": "7pG-NmFEC0Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, device, cell_type, output_vocab, embed_size, hidden_size, max_length, dropout_p=0.1, num_layers=1, bidirectional=False):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        # Initialize the decoder attributes\n",
        "        self.hidden_size = hidden_size  # Size of the hidden state\n",
        "        self.output_size = output_vocab  # Size of the output vocabulary\n",
        "        self.embed_size = embed_size  # Size of the embedding\n",
        "        self.dropout_p = dropout_p  # Dropout probability\n",
        "        self.cell_type = cell_type  # Type of the RNN cell\n",
        "        self.max_length = max_length  # Maximum length of the input sequence\n",
        "        self.device = device  # Device to be used (e.g., 'cpu' or 'cuda')\n",
        "        self.num_layers = num_layers  # Number of layers in the RNN\n",
        "        self.embedding_decoder = nn.Embedding(self.output_size, self.embed_size)  # Embedding layer\n",
        "        self.dropout = nn.Dropout(self.dropout_p)  # Dropout layer\n",
        "        self.bidirectional = bidirectional  # Flag indicating bidirectional RNN\n",
        "\n",
        "        # Initialize the RNN based on the specified cell_type\n",
        "        if cell_type == 'lstm':\n",
        "            self.rnn = nn.LSTM(self.embed_size, hidden_size, num_layers=self.num_layers, batch_first=True, bidirectional=self.bidirectional, dropout=self.dropout_p)\n",
        "        elif cell_type == 'gru':\n",
        "            self.rnn = nn.GRU(self.embed_size, hidden_size, num_layers=self.num_layers, batch_first=True, bidirectional=self.bidirectional, dropout=self.dropout_p)\n",
        "        elif cell_type == 'rnn':\n",
        "            self.rnn = nn.RNN(self.embed_size, hidden_size, num_layers=self.num_layers, batch_first=True, bidirectional=self.bidirectional, dropout=self.dropout_p)\n",
        "\n",
        "        self.out = nn.Linear((1 + int(self.bidirectional)) * self.hidden_size, self.output_size)  # Linear layer\n",
        "        self.out_activation = nn.LogSoftmax(dim=-1)  # Log softmax activation function\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        input = input.unsqueeze(1)  # Add a dimension of size 1 to the input tensor\n",
        "        embedded_decoder = self.embedding_decoder(input)  # Embed the input tensor\n",
        "        embedded_decoder = self.dropout(embedded_decoder)  # Apply dropout to the embedded tensor\n",
        "\n",
        "        # Pass the embedded tensor through the RNN\n",
        "        if self.cell_type == 'lstm':\n",
        "            output, (hidden, cell) = self.rnn(embedded_decoder, (hidden, cell))\n",
        "        elif self.cell_type == 'gru':\n",
        "            output, hidden = self.rnn(embedded_decoder, hidden)\n",
        "        elif self.cell_type == 'rnn':\n",
        "            output, hidden = self.rnn(embedded_decoder, hidden)\n",
        "\n",
        "        output = F.relu(self.out(output))  # Apply ReLU activation to the output tensor\n",
        "        output = F.log_softmax(output, dim=-1)  # Apply log softmax activation to the output tensor\n",
        "\n",
        "        return output, hidden, cell\n",
        "\n",
        "    def init_hidden(self, encoder_hidden, encoder_cell, encoder_bidirectional):\n",
        "        hidden = encoder_hidden[-(1 + int(encoder_bidirectional)):].repeat(self.num_layers, 1, 1)  # Repeat the hidden tensor\n",
        "        cell = encoder_cell[-(1 + int(encoder_bidirectional)):].repeat(self.num_layers, 1, 1)  # Repeat the cell tensor\n",
        "        return hidden, cell\n"
      ],
      "metadata": {
        "id": "2mo_06fMDjR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(this, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        this.encoder = encoder\n",
        "        this.decoder = decoder\n",
        "        this.device = device\n",
        "        this.mtl = 0  # Initialize the maximum target length\n",
        "        this.sos = 0  # Initialize the start of sequence token\n",
        "        \n",
        "    def forward(this, source, target, teacher_forcing_ratio = 0.5):\n",
        "        # Get dimensions of input\n",
        "        batch_size = target.shape[0]\n",
        "        target_len = target.shape[1]\n",
        "        this.mtl = target_len  # Set the maximum target length\n",
        "        target_vocab_size = this.decoder.output_size\n",
        "        \n",
        "        # Initialize tensor for storing outputs\n",
        "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(this.device)\n",
        "\n",
        "        # Initialize hidden states of the encoder\n",
        "        encoder_hidden, encoder_cell = this.encoder.init_hidden(batch_size)\n",
        "\n",
        "        # Forward pass through the encoder\n",
        "        if (this.encoder.cell_type == 'lstm'):\n",
        "            encoder_outputs, encoder_hidden, encoder_cell = this.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "        if (this.encoder.cell_type == 'rnn'):\n",
        "            encoder_outputs, encoder_hidden = this.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "        if (this.encoder.cell_type == 'gru'):\n",
        "            encoder_outputs, encoder_hidden = this.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "\n",
        "        # First input to the decoder is the <sos> tokens\n",
        "        input = target[:,0]\n",
        "        this.sos = target[:,0]\n",
        "\n",
        "        # Initialize hidden states of the decoder\n",
        "        hidden, cell = this.decoder.init_hidden(encoder_hidden, encoder_cell, this.encoder.bidirectional)\n",
        "        \n",
        "        # Iterate over the target sequence\n",
        "        for t in range(1, target_len):\n",
        "            # Forward pass through the decoder\n",
        "            output, hidden, cell = this.decoder.forward(input, hidden, cell)\n",
        "            outputs[:,t] = output.squeeze(1)  # Store the decoder output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(-1)\n",
        "            input = target[:,t] if teacher_force else top1.squeeze(1)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def inference(this, source, target):\n",
        "        # Get dimensions of input\n",
        "        batch_size = source.shape[0]\n",
        "        target_len = this.mtl\n",
        "        target_vocab_size = this.decoder.output_size\n",
        "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(this.device)\n",
        "\n",
        "        # Initialize hidden states of the encoder\n",
        "        encoder_hidden, encoder_cell = this.encoder.init_hidden(batch_size)\n",
        "\n",
        "        # Forward pass through the encoder\n",
        "        if (this.encoder.cell_type == 'lstm'):\n",
        "            encoder_outputs, encoder_hidden, encoder_cell = this.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "        if (this.encoder.cell_type == 'rnn'):\n",
        "            encoder_outputs, encoder_hidden = this.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "        if (this.encoder.cell_type == 'gru'):\n",
        "            encoder_outputs, encoder_hidden = this.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "           \n",
        "        # First input to the decoder is the <sos> token\n",
        "        input = this.sos\n",
        "\n",
        "        # Initialize hidden states of the decoder\n",
        "        hidden, cell = this.decoder.init_hidden(encoder_hidden, encoder_cell, this.encoder.bidirectional)\n",
        "      \n",
        "        # Iterate over the target sequence\n",
        "        for t in range(1, target_len):\n",
        "            # Forward pass through the decoder\n",
        "            output, hidden, cell = this.decoder.forward(input, hidden, cell)\n",
        "            outputs[:,t] = output.squeeze(1)  # Store the decoder output\n",
        "            top1 = output.argmax(-1)\n",
        "            input = top1.squeeze(1)\n",
        "\n",
        "        # print(f\"Outputs = {outputs[:,0]}\")\n",
        "        # print(f\"Targets = {target[:,0]}\")\n",
        "        \n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "6Sc898q4EI0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Commented code\n",
        "\n",
        "# Define the input and output dimensions based on the length of the dictionary\n",
        "input_dim = len(dict)\n",
        "output_dim = len(dict_t)\n",
        "\n",
        "# Set the batch size for training and validation\n",
        "batch_size = 32\n",
        "val_batch_size = 32\n",
        "\n",
        "# Set the embedding dimensions for the encoder and decoder\n",
        "enc_embedding = 256\n",
        "dec_embedding = 256\n",
        "\n",
        "# Set the number of hidden units in the encoder and decoder\n",
        "hidden = 512\n",
        "\n",
        "# Set the number of layers in the encoder and decoder\n",
        "enc_num_layers = 3\n",
        "dec_num_layers = 2\n",
        "\n",
        "# Set the dropout probabilities for the encoder and decoder\n",
        "enc_dropout = 0.4\n",
        "dec_dropout = 0.4\n",
        "\n",
        "# Set the maximum length for the input sequence\n",
        "max_length = mtl\n",
        "\n",
        "# Set the cell type for the encoder and decoder\n",
        "cell_type = 'gru'\n",
        "\n",
        "# Initialize the encoder, decoder, and the overall model\n",
        "enc = EncoderRNN(device, cell_type, input_dim, enc_embedding, hidden, enc_num_layers, bidirectional=True, dropout_p=enc_dropout)\n",
        "dec = DecoderRNN(device, cell_type, output_dim, dec_embedding, hidden, max_length, dec_dropout, dec_num_layers, bidirectional=True)\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "# Function to count the number of trainable parameters in the model\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "\n",
        "# Define the optimizer and the loss criterion\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Function to train the model for a given number of iterations\n",
        "# Function for training the model\n",
        "def trainIters(model, pairs, batch_size, n_iters, optimizer, tf, print_every=10, plot_every=10, log=True, Attention=False):\n",
        "    start = time.time()  # Start time for tracking training duration\n",
        "    plot_losses = []  # List to store plot losses for visualization\n",
        "    train_char_accuracy = []  # List to store training character-level accuracy\n",
        "    train_word_accuracy = []  # List to store training word-level accuracy\n",
        "    val_losses = []  # List to store validation losses\n",
        "    val_char_accuracy = []  # List to store validation character-level accuracy\n",
        "    val_word_accuracy = []  # List to store validation word-level accuracy\n",
        "    print_loss_total = 0  # Variable to track the cumulative print loss\n",
        "    plot_loss_total = 0  # Variable to track the cumulative plot loss\n",
        "    print_val_loss_total = 0  # Variable to track the cumulative validation print loss\n",
        "    plot_val_loss_total = 0  # Variable to track the cumulative validation plot loss\n",
        "\n",
        "    train_pairs = pairs[0]  # Training pairs\n",
        "    val_pairs = pairs[1]  # Validation pairs\n",
        "    train_accuracy = 0  # Training accuracy\n",
        "\n",
        "    criterion = nn.NLLLoss()  # Negative Log Likelihood loss function\n",
        "\n",
        "    count = 0  # Counter for tracking iterations\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        for i in np.arange(start=0, stop=len(train_pairs) - batch_size, step=batch_size):\n",
        "            train_accuracy = 0  # Reset training accuracy for each batch\n",
        "            count += 1  # Increment the counter\n",
        "            if (i + batch_size > len(train_pairs)):  # Check if the remaining examples are less than batch size\n",
        "                batch_size = len(train_pairs) - i + 1  # Adjust the batch size\n",
        "            input_tensor = []  # List to store input tensors for the batch\n",
        "            target_tensor = []  # List to store target tensors for the batch\n",
        "\n",
        "            for j in range(batch_size):  # Iterate over the batch examples\n",
        "                input_tensor.append(train_pairs[i + j][0])  # Append input tensor to the list\n",
        "                target_tensor.append(train_pairs[i + j][1])  # Append target tensor to the list\n",
        "\n",
        "            input_tensor = torch.stack(input_tensor).squeeze(1).long().cuda()  # Convert input tensors to tensor and move to GPU\n",
        "            target_tensor = torch.stack(target_tensor).squeeze(1).long().cuda()  # Convert target tensors to tensor and move to GPU\n",
        "\n",
        "            optimizer.zero_grad()  # Zero the gradients\n",
        "            if (count < 4000):  # Check if teacher forcing should be applied\n",
        "                out = model(input_tensor, target_tensor, teacher_forcing_ratio=tf)  # Perform forward pass with teacher forcing\n",
        "            else:\n",
        "                out = model(input_tensor, target_tensor, teacher_forcing_ratio=0)  # Perform forward pass without teacher forcing\n",
        "\n",
        "            out = torch.permute(out, [0, 2, 1])  # Permute the output tensor\n",
        "            loss = criterion(out, target_tensor)  # Calculate the loss\n",
        "\n",
        "            train_accuracy_word = word_acc(target_tensor, out) * batch_size  # Calculate word-level accuracy\n",
        "            train_accuracy = train_accuracy + train_accuracy_word  # Update training accuracy\n",
        "            loss.backward()  # Backpropagation\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)  # Clip gradients to prevent exploding gradients\n",
        "            optimizer.step()  # Update the model parameters\n",
        "\n",
        "            print_loss_total += loss  # Accumulate the print loss\n",
        "            plot_loss_total += loss  # Accumulate the plot loss\n",
        "\n",
        "            if count % 800 == 0:  # Perform validation every 800 iterations\n",
        "                val_input_tensor = []  # List to store validation input tensors\n",
        "                val_target_tensor = []  # List to store validation target tensors\n",
        "\n",
        "                for j in range(batch_size):  # Iterate over the validation batch examples\n",
        "                    val_input_tensor.append(val_pairs[j][0])  # Append validation input tensor to the list\n",
        "                    val_target_tensor.append(val_pairs[j][1])  # Append validation target tensor to the list\n",
        "\n",
        "                val_input_tensor = torch.stack(val_input_tensor).squeeze(1).long().cuda()  # Convert validation input tensors to tensor and move to GPU\n",
        "                val_target_tensor = torch.stack(val_target_tensor).squeeze(1).long().cuda()  # Convert validation target tensors to tensor and move to GPU\n",
        "                if (Attention == True):  # Check if attention mechanism is enabled\n",
        "                    val_out, _ = model.inference(val_input_tensor, val_target_tensor)  # Perform inference with attention\n",
        "                    val_out = val_out.permute(0, 2, 1)  # Permute the output tensor\n",
        "                else:\n",
        "                    val_out = model.inference(val_input_tensor, val_target_tensor)  # Perform inference without attention\n",
        "                    val_out = val_out.permute(0, 2, 1)  # Permute the output tensor\n",
        "                val_loss = criterion(val_out, val_target_tensor)  # Calculate the validation loss\n",
        "                val_loss_sampled = val_loss  # Store the sampled validation loss\n",
        "                wandb.log({'Val_Loss': val_loss_sampled})  # Log the validation loss\n",
        "\n",
        "            if count % 800 == 0:  # Print and log the training loss every 800 iterations\n",
        "                print_loss_avg = print_loss_total / 800  # Calculate the average print loss\n",
        "                print_loss_total = 0  # Reset the print loss total\n",
        "                print('%s (%d %d%%) %.7f' % (span(start, iter / n_iters), iter, iter / n_iters * 100, print_loss_avg))  # Print the progress\n",
        "\n",
        "            if count % 800 == 0:  # Plot and log the training loss every 800 iterations\n",
        "                plot_loss_avg = plot_loss_total / 800  # Calculate the average plot loss\n",
        "                plot_losses.append(plot_loss_avg.detach())  # Append the plot loss to the list\n",
        "                wandb.log({'Train Loss': plot_loss_avg})  # Log the training loss\n",
        "\n",
        "                plot_loss_total = 0  # Reset the plot loss total\n",
        "\n",
        "        train_accuracy = train_accuracy / (len(train_pairs) - batch_size)  # Calculate the average training accuracy\n",
        "        train_word_accuracy.append(train_accuracy)  # Append the training accuracy to the list\n",
        "\n",
        "    print(train_word_accuracy)  # Print the list of training word-level accuracies\n",
        "    plot_losses = [losses.cpu().numpy() for losses in plot_losses]  # Convert plot losses to numpy array\n",
        "\n",
        "    char_count = 0  # Variable to track character count\n",
        "    word_count = 0  # Variable to track word count\n",
        "\n",
        "    for i in np.arange(start=0, stop=len(val_pairs) - batch_size, step=batch_size):  # Iterate over the validation pairs\n",
        "        if (i + batch_size > len(val_pairs)):  # Check if the remaining examples are less than batch size\n",
        "            batch_size = len(val_pairs) - i + 1  # Adjust the batch size\n",
        "        val_input_tensor = []  # List to store validation input tensors\n",
        "        val_target_tensor = []  # List to store validation target tensors\n",
        "        for j in range(batch_size):  # Iterate over the batch examples\n",
        "            val_input_tensor.append(val_pairs[i + j][0])  # Append validation input tensor to the list\n",
        "            val_target_tensor.append(val_pairs[i + j][1])  # Append validation target tensor to the list\n",
        "\n",
        "        val_input_tensor = torch.stack(val_input_tensor).squeeze(1).long().cuda()  # Convert validation input tensors to tensor and move to GPU\n",
        "        val_target_tensor = torch.stack(val_target_tensor).squeeze(1).long().cuda()  # Convert validation target tensors to tensor and move to GPU\n",
        "        if (Attention == True):  # Check if attention mechanism is enabled\n",
        "            val_out, _ = model.inference(val_input_tensor, val_target_tensor)  # Perform inference with attention\n",
        "            val_out = val_out.permute(0, 2, 1)  # Permute the output tensor\n",
        "        else:\n",
        "            val_out = model.inference(val_input_tensor, val_target_tensor)  # Perform inference without attention\n",
        "            val_out = val_out.permute(0, 2, 1)  # Permute the output tensor\n",
        "        val_loss = criterion(val_out, val_target_tensor)  # Calculate the validation loss\n",
        "\n",
        "        val_accuracy_word = word_acc(val_target_tensor, val_out)  # Calculate word-level accuracy\n",
        "        word_count = word_count + (val_accuracy_word) * batch_size  # Update word count\n",
        "\n",
        "    word_accuracy = word_count / (len(val_pairs) - batch_size)  # Calculate word-level accuracy\n",
        "\n",
        "    metrics = {'Val_Accuracy': word_accuracy}  # Store validation metrics\n",
        "    wandb.log(metrics)  # Log the validation metrics\n",
        "\n",
        "    print(f\"Val loss = {val_loss}\")  # Print the validation loss\n",
        "    print(f'Word-level-accuracy on val set = {word_accuracy}')  # Print the word-level accuracy on the validation set\n",
        "\n"
      ],
      "metadata": {
        "id": "OwalnEDxEmHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter configuration\n",
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'name': 'Hyperparameter Tuning-Bayesian'\n",
        "}\n",
        "\n",
        "# Metric to optimize\n",
        "metric = {\n",
        "    'name': 'Val_Accuracy',\n",
        "    'goal': 'maximize'\n",
        "}\n",
        "\n",
        "# Assign metric to sweep config\n",
        "sweep_config['metric'] = metric\n",
        "\n",
        "# Define parameter values\n",
        "parameters_dict = {\n",
        "    'optimiser': {\n",
        "        'values': ['nadam']\n",
        "    },\n",
        "    'teacher_forcing_ratio': {\n",
        "        'values': [0.5, 0.7]\n",
        "    },\n",
        "    'bidirectional': {\n",
        "        'values': [True]\n",
        "    },\n",
        "    'enc_embedding': {\n",
        "        'values': [128]\n",
        "    },\n",
        "    'dec_embedding': {\n",
        "        'values': [128]\n",
        "    },\n",
        "    'epochs': {\n",
        "        'values': [2]\n",
        "    },\n",
        "    'hidden_size': {\n",
        "        'values': [64, 128, 256]\n",
        "    },\n",
        "    'enc_layers': {\n",
        "        'values': [3]\n",
        "    },\n",
        "    'dec_layers': {\n",
        "        'values': [3]\n",
        "    },\n",
        "    'dropout': {\n",
        "        'values': [0.3]\n",
        "    },\n",
        "    'cell_type': {\n",
        "        'values': ['rnn', 'gru']\n",
        "    }\n",
        "}\n",
        "\n",
        "# Assign parameters to sweep config\n",
        "sweep_config['parameters'] = parameters_dict\n",
        "\n",
        "# Initialize the sweep\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"CS6910 Assignment 3\")\n",
        "\n",
        "\n",
        "def train_sweep(config=None):\n",
        "    with wandb.init(config=config) as run:\n",
        "        config = wandb.config\n",
        "\n",
        "        # Set up model parameters\n",
        "        input_dim = len(dict)\n",
        "        output_dim = len(dict_t)\n",
        "        batch_size = 32\n",
        "        val_batch_size = 32\n",
        "        enc_embedding = config.enc_embedding\n",
        "        dec_embedding = config.dec_embedding\n",
        "        hidden = config.hidden_size\n",
        "        enc_num_layers = config.enc_layers\n",
        "        dec_num_layers = config.dec_layers\n",
        "        enc_dropout = config.dropout\n",
        "        dec_dropout = config.dropout\n",
        "        max_length = mtl\n",
        "        cell_type = config.cell_type\n",
        "\n",
        "        # Initialize the encoder, decoder, and model\n",
        "        enc = EncoderRNN(device, cell_type, input_dim, enc_embedding, hidden, enc_num_layers, bidirectional=config.bidirectional, dropout_p=enc_dropout)\n",
        "        dec = DecoderRNN(device, cell_type, output_dim, dec_embedding, hidden, max_length, dec_dropout, dec_num_layers, bidirectional=config.bidirectional)\n",
        "        model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "        # Set experiment name\n",
        "        exp_name = str(config.cell_type)\n",
        "        exp_name = exp_name\n",
        "        exp_name = exp_name+'_optim_'+ config.optimiser\n",
        "        wandb.run.name = exp_name\n",
        "\n",
        "        # Set optimizer based on config\n",
        "        if (config.optimiser == 'adam'):\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "        elif (config.optimiser == 'nadam'):\n",
        "            optimizer = torch.optim.NAdam(model.parameters(), lr=0.001)\n",
        "\n",
        "        # Set loss criterion\n",
        "        criterion = nn.NLLLoss()\n",
        "\n",
        "        # Train the model\n",
        "        trainIters(model, pairs, 32, config.epochs, optimizer, config.teacher_forcing_ratio)\n",
        "\n",
        "\n",
        "# Run the sweep\n",
        "wandb.agent(sweep_id, train_sweep, count=1)\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702,
          "referenced_widgets": [
            "234d251ea8104dee819b39236173e892",
            "29d54038e5b54146b32e30cfd321b436",
            "e17e870ac7a5438985075ad3fd02c683",
            "6c3a71b7f635436f833bc2a0817a8ecf",
            "60a51484c18f4b66ae1673ff6bb9c66a",
            "ed7bdf2d5998416788850d04ad328872",
            "ea2c226883c94ad18d6b6bbf3681af51",
            "de1c94e1730b4738b61030388ec62a18"
          ]
        },
        "id": "zxuzJlxzE2__",
        "outputId": "c7c93e0e-14ac-470d-a774-9fa603337062"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: fxh6gson\n",
            "Sweep URL: https://wandb.ai/ed22s009/CS6910%20Assignment%203/sweeps/fxh6gson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ngfgtva3 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: gru\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_embedding: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_embedding: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimiser: nadam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33med22s009\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230521_124110-ngfgtva3</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ed22s009/CS6910%20Assignment%203/runs/ngfgtva3' target=\"_blank\">wandering-sweep-1</a></strong> to <a href='https://wandb.ai/ed22s009/CS6910%20Assignment%203' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ed22s009/CS6910%20Assignment%203/sweeps/fxh6gson' target=\"_blank\">https://wandb.ai/ed22s009/CS6910%20Assignment%203/sweeps/fxh6gson</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ed22s009/CS6910%20Assignment%203' target=\"_blank\">https://wandb.ai/ed22s009/CS6910%20Assignment%203</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/ed22s009/CS6910%20Assignment%203/sweeps/fxh6gson' target=\"_blank\">https://wandb.ai/ed22s009/CS6910%20Assignment%203/sweeps/fxh6gson</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ed22s009/CS6910%20Assignment%203/runs/ngfgtva3' target=\"_blank\">https://wandb.ai/ed22s009/CS6910%20Assignment%203/runs/ngfgtva3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0, 0.0]\n",
            "Val loss = 1.1140252351760864\n",
            "Word-level-accuracy on val set = 0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "234d251ea8104dee819b39236173e892"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Val_Accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Val_Accuracy</td><td>0.0</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">wandering-sweep-1</strong> at: <a href='https://wandb.ai/ed22s009/CS6910%20Assignment%203/runs/ngfgtva3' target=\"_blank\">https://wandb.ai/ed22s009/CS6910%20Assignment%203/runs/ngfgtva3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230521_124110-ngfgtva3/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to append strings to a CSV file\n",
        "def append_strings_to_csv(file_path, string1, string2, string3):\n",
        "    with open(file_path, 'a', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow([string1, string2, string3])\n",
        "\n",
        "# Calculate the input and output dimensions based on a dictionary\n",
        "input_dim = len(dict)\n",
        "output_dim = len(dict_t)\n",
        "\n",
        "# Set batch sizes, embedding dimensions, hidden sizes, and other parameters\n",
        "batch_size = 32\n",
        "val_batch_size = 32\n",
        "enc_embedding = 128\n",
        "dec_embedding = 128\n",
        "hidden = 512\n",
        "epochs = 8\n",
        "enc_num_layers = 3\n",
        "dec_num_layers = 3\n",
        "enc_dropout = 0.3\n",
        "dec_dropout = 0.3\n",
        "max_length = mtl\n",
        "cell_type = 'lstm'\n",
        "teacher_forcing_ratio = 0\n",
        "\n",
        "# Create encoder, decoder, and Seq2Seq model objects\n",
        "enc = EncoderRNN(device, cell_type, input_dim, enc_embedding, hidden, enc_num_layers, bidirectional=True, dropout_p=enc_dropout)\n",
        "dec = DecoderRNN(device, cell_type, output_dim, dec_embedding, hidden, max_length, dec_dropout, dec_num_layers, bidirectional=True)\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "# Set optimizer and learning rate\n",
        "optimizer = torch.optim.NAdam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Initialize Weights and Biases (wandb)\n",
        "wandb.init()\n",
        "\n",
        "# Train the model using the defined training function\n",
        "trainIters(model, pairs, 32, epochs, optimizer, teacher_forcing_ratio)\n",
        "\n",
        "# Select test pairs for evaluation\n",
        "test_pairs = pairs[2]\n",
        "test_word_count = 0\n",
        "\n",
        "# Set the initial count and name for the output file\n",
        "count = 1\n",
        "name = 'AttentionPredictions' + str(count)\n",
        "\n",
        "# Define the file path for saving the CSV file\n",
        "file_path = '/content/drive/MyDrive/aksharantar_sampled/mar' + name + '.csv'\n",
        "\n",
        "# Iterate over the test pairs in batches\n",
        "for i in np.arange(start=0, stop=len(test_pairs) - batch_size, step=batch_size):\n",
        "    if (i + batch_size > len(test_pairs)):\n",
        "        batch_size = len(test_pairs) - i + 1\n",
        "\n",
        "    # Initialize empty lists for storing input and target tensors\n",
        "    test_input_tensor = []\n",
        "    test_target_tensor = []\n",
        "\n",
        "    # Populate the input and target tensors for the current batch\n",
        "    for j in range(batch_size):\n",
        "        test_input_tensor.append(test_pairs[i + j][0])\n",
        "        test_target_tensor.append(test_pairs[i + j][1])\n",
        "\n",
        "    # Convert the input and target tensors to the appropriate format and move them to the GPU\n",
        "    test_input_tensor = torch.stack(test_input_tensor).squeeze(1).long().cuda()\n",
        "    test_target_tensor = torch.stack(test_target_tensor).squeeze(1).long().cuda()\n",
        "\n",
        "    # Perform inference using the model on the test input tensor\n",
        "    test_out = model.inference(test_input_tensor, test_target_tensor)\n",
        "\n",
        "    # Permute the dimensions of the output tensor\n",
        "    test_out = test_out.permute(0, 2, 1)\n",
        "\n",
        "    # Calculate the test loss\n",
        "    test_loss = criterion(test_out, test_target_tensor)\n",
        "\n",
        "    # Iterate over the batch and append the input, target, and output strings to the CSV file\n",
        "    for j in range(batch_size):\n",
        "        input_str = key_presenting(dict, test_input_tensor[j])\n",
        "        output_str = key_presenting(dict_t, torch.argmax(test_out[j], dim=0))\n",
        "        target_str = key_presenting(dict_t, test_target_tensor[j])\n",
        "        append_strings_to_csv(file_path, input_str, target_str, output_str)\n",
        "\n",
        "    # Calculate the word accuracy for the test batch\n",
        "    test_accuracy_word = word_acc(test_target_tensor, test_out)\n",
        "    test_word_count = test_word_count + (test_accuracy_word) * batch_size\n",
        "\n",
        "# Calculate the overall word accuracy for the test set\n",
        "test_word_accuracy = test_word_count / (len(test_pairs) - batch_size)\n",
        "\n",
        "# Print the accuracy on the test set\n",
        "print(f\"Accuracy on Test Set is {test_word_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "dkshny8eZr2t",
        "outputId": "ceabc7f6-d2e3-494d-f246-5255bcfb7b85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230521_130320-ngfgtva3</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ed22s009/CS6910%20Assignment%203/runs/ngfgtva3' target=\"_blank\">gru_optim_nadam</a></strong> to <a href='https://wandb.ai/ed22s009/CS6910%20Assignment%203' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ed22s009/CS6910%20Assignment%203/sweeps/fxh6gson' target=\"_blank\">https://wandb.ai/ed22s009/CS6910%20Assignment%203/sweeps/fxh6gson</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ed22s009/CS6910%20Assignment%203' target=\"_blank\">https://wandb.ai/ed22s009/CS6910%20Assignment%203</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/ed22s009/CS6910%20Assignment%203/sweeps/fxh6gson' target=\"_blank\">https://wandb.ai/ed22s009/CS6910%20Assignment%203/sweeps/fxh6gson</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ed22s009/CS6910%20Assignment%203/runs/ngfgtva3' target=\"_blank\">https://wandb.ai/ed22s009/CS6910%20Assignment%203/runs/ngfgtva3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1m 27s (- 0m 12s) (7 87%) 1.0391052\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0002461235540241201, 0.0, 0.0014767413241447206, 0.0014767413241447206]\n",
            "Val loss = 0.7258210778236389\n",
            "Word-level-accuracy on val set = 0.04331774550824514\n",
            "Accuracy on Test Set is 0.1553039625892198\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionDecoderRNN(nn.Module):\n",
        "    def __init__(this, device, cell_type, output_vocab, embed_size, hidden_size, max_length, dropout_p=0.1, num_layers = 1, bidirectional = False):\n",
        "        super(AttentionDecoderRNN, this).__init__()\n",
        "        this.hidden_size = hidden_size  # Initialize hidden size\n",
        "        this.output_size = output_vocab  # Initialize output vocabulary size\n",
        "        this.embed_size = embed_size  # Initialize embedding size\n",
        "        this.dropout_p = dropout_p  # Initialize dropout probability\n",
        "        this.cell_type = cell_type  # Initialize cell type\n",
        "        this.max_length = max_length  # Initialize maximum length\n",
        "        this.device = device  # Initialize device\n",
        "        this.num_layers = num_layers  # Initialize number of layers\n",
        "        this.embedding_decoder = nn.Embedding(this.output_size, this.embed_size)  # Initialize decoder embedding\n",
        "        this.dropout = nn.Dropout(this.dropout_p)  # Initialize dropout layer\n",
        "        this.bidirectional = bidirectional  # Initialize bidirectional flag\n",
        "\n",
        "        # Initialize the appropriate RNN cell based on the given cell type\n",
        "        if (cell_type == 'lstm'):\n",
        "          this.rnn = nn.LSTM( this.embed_size + hidden_size*(1+int(this.bidirectional)), hidden_size, num_layers = this.num_layers, batch_first=True, bidirectional=this.bidirectional, dropout = this.dropout_p)\n",
        "        elif (cell_type == 'gru'):\n",
        "          this.rnn = nn.GRU(this.embed_size+ hidden_size*(1+int(this.bidirectional)), hidden_size, num_layers = this.num_layers, batch_first=True, bidirectional=this.bidirectional, dropout = this.dropout_p)\n",
        "        elif (cell_type == 'rnn'):\n",
        "          this.rnn = nn.RNN(this.embed_size+ hidden_size*(1+int(this.bidirectional)), hidden_size, num_layers = this.num_layers, batch_first=True, bidirectional=this.bidirectional, dropout = this.dropout_p)\n",
        "\n",
        "        # Initialize attention layers\n",
        "        this.energy = nn.Linear(hidden_size*(2+int(this.bidirectional)),hidden_size)\n",
        "        this.value = nn.Linear(hidden_size,1, bias = False)\n",
        "        this.softmax = nn.Softmax(dim = 0)\n",
        "        this.relu = nn.ReLU()\n",
        "        this.tanh = nn.Tanh()\n",
        "\n",
        "        # Initialize output layers\n",
        "        this.out = nn.Linear((1+int(this.bidirectional))*this.hidden_size, this.output_size)\n",
        "        this.out_activation = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "        # Initialize hidden reshape layer\n",
        "        this.hidden_reshape_linear = nn.Linear(hidden_size*2,hidden_size)\n",
        "\n",
        "    def forward(this, input, encoder_states, hidden, cell):\n",
        "        input = input.unsqueeze(1)  # Add a dimension to the input tensor\n",
        "        embedded_decoder = this.embedding_decoder(input)  # Embed the input tensor\n",
        "        embedded_decoder = this.dropout(embedded_decoder)  # Apply dropout to the embedded tensor\n",
        "\n",
        "        encoder_states = encoder_states.permute(1,0,2)  # Permute the encoder states tensor\n",
        "        sequence_length = encoder_states.shape[0]  # Get the sequence length\n",
        "        if this.bidirectional == True:\n",
        "          hidden_1 = this.relu(this.hidden_reshape_linear(hidden[0:2].permute(1,0,2).reshape(hidden.shape[1],-1))).unsqueeze(0)  # Reshape and apply a linear layer to the hidden tensor\n",
        "        else:\n",
        "          hidden_1 = hidden[0]  # Use the hidden tensor as is\n",
        "\n",
        "        hidden_reshaped = hidden_1.repeat(sequence_length,1,1)  # Repeat the hidden tensor based on the sequence length\n",
        "\n",
        "        energy = this.value(this.tanh(this.energy(torch.cat((hidden_reshaped,encoder_states),dim=2))))  # Calculate the attention energy\n",
        "        attention = this.softmax(energy)  # Apply softmax to the energy tensor\n",
        "        attention = attention.permute(1,2,0)  # Permute the attention tensor\n",
        "        encoder_states = encoder_states.permute(1,0,2)  # Permute the encoder states tensor\n",
        "        context_vector = torch.bmm(attention, encoder_states)  # Calculate the context vector using attention and encoder states\n",
        "\n",
        "        rnn_input = torch.cat((context_vector,embedded_decoder), dim = 2)  # Concatenate the context vector and embedded decoder tensor\n",
        "\n",
        "        if (this.cell_type == 'lstm'):\n",
        "          decoder_output, (hidden, cell) = this.rnn(rnn_input, (hidden, cell))  # Apply the RNN to the input tensor\n",
        "        elif (this.cell_type == 'gru'):\n",
        "          decoder_output, hidden = this.rnn(rnn_input, hidden)  # Apply the RNN to the input tensor\n",
        "        elif (this.cell_type == 'rnn'):\n",
        "          decoder_output,hidden = this.rnn(rnn_input, hidden)  # Apply the RNN to the input tensor\n",
        "\n",
        "        output = F.relu(this.out(decoder_output))  # Apply ReLU activation to the output tensor\n",
        "        output = F.log_softmax(output,dim=-1)  # Apply log softmax activation to the output tensor\n",
        "\n",
        "        return output, hidden, cell, attention  # Return the output, hidden, cell, and attention tensors\n",
        "\n",
        "    def init_hidden(this, encoder_hidden, encoder_cell, encoder_bidirectional):\n",
        "        hidden = encoder_hidden[-(1+int(encoder_bidirectional)): ].repeat(this.num_layers,1,1)  # Repeat the encoder hidden tensor based on the number of layers\n",
        "        cell = encoder_cell[-(1+int(encoder_bidirectional)): ].repeat(this.num_layers,1,1)  # Repeat the encoder cell tensor based on the number of layers\n",
        "        return hidden, cell  # Return the hidden and cell tensors\n"
      ],
      "metadata": {
        "id": "qg_WNpzgdnyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionSeq2Seq(nn.Module):\n",
        "    def __init__(this, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        this.encoder = encoder  # Initialize the encoder\n",
        "        this.decoder = decoder  # Initialize the decoder\n",
        "        this.device = device    # Set the device (CPU or GPU)\n",
        "        this.mtl = 0            # Initialize the maximum target length\n",
        "        this.sos = 0            # Initialize the start of sequence token\n",
        "\n",
        "    def forward(this, source, target, teacher_forcing_ratio = 0.5):\n",
        "        # Initialize variables\n",
        "        batch_size = target.shape[0]      # Get the batch size\n",
        "        target_len = target.shape[1]      # Get the target sequence length\n",
        "        this.mtl = target_len             # Update the maximum target length\n",
        "        target_vocab_size = this.decoder.output_size  # Get the size of the target vocabulary\n",
        "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(this.device)  # Initialize the outputs tensor\n",
        "        encoder_hidden, encoder_cell = this.encoder.init_hidden(batch_size)  # Initialize the encoder hidden state and cell state\n",
        "\n",
        "        # Forward pass through the encoder\n",
        "        if (this.encoder.cell_type == 'lstm'):\n",
        "          encoder_outputs, encoder_hidden, encoder_cell = this.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "        if (this.encoder.cell_type == 'rnn'):\n",
        "          encoder_outputs, encoder_hidden = this.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "        if (this.encoder.cell_type == 'gru'):\n",
        "          encoder_outputs, encoder_hidden = this.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "\n",
        "        # Initialize decoder input\n",
        "        input = target[:, 0]    # Get the first target token\n",
        "        this.sos = target[:, 0]  # Update the start of sequence token\n",
        "        hidden, cell = this.decoder.init_hidden(encoder_hidden, encoder_cell, this.encoder.bidirectional)  # Initialize the decoder hidden state and cell state\n",
        "        \n",
        "        # Decode sequence\n",
        "        for t in range(1, target_len):\n",
        "            output, hidden, cell, _ = this.decoder.forward(input, encoder_outputs, hidden, cell)  # Perform a forward pass through the decoder\n",
        "            outputs[:, t] = output.squeeze(1)  # Store the output in the outputs tensor\n",
        "            teacher_force = random.random() < teacher_forcing_ratio  # Determine if teacher forcing will be used\n",
        "            top1 = output.argmax(-1)  # Get the index of the highest probability token\n",
        "            input = target[:, t] if teacher_force else top1.squeeze(1)  # Update the decoder input based on teacher forcing\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def inference(this, source, target):\n",
        "        # Initialize variables\n",
        "        batch_size = source.shape[0]  # Get the batch size\n",
        "        target_len = this.mtl         # Get the maximum target length\n",
        "        target_vocab_size = this.decoder.output_size  # Get the size of the target vocabulary\n",
        "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(this.device)  # Initialize the outputs tensor\n",
        "        encoder_hidden, encoder_cell = this.encoder.init_hidden(batch_size)  # Initialize the encoder hidden state and cell state\n",
        "\n",
        "        # Forward pass through the encoder\n",
        "        if (this.encoder.cell_type == 'lstm'):\n",
        "          encoder_outputs, encoder_hidden, encoder_cell = this.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "        if (this.encoder.cell_type == 'rnn'):\n",
        "          encoder_outputs, encoder_hidden = this.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "        if (this.encoder.cell_type == 'gru'):\n",
        "          encoder_outputs, encoder_hidden = this.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "\n",
        "        # Initialize decoder input\n",
        "        input = this.sos                    # Get the start of sequence token\n",
        "        input_len = encoder_outputs.shape[1]  # Get the input sequence length\n",
        "        hidden, cell = this.decoder.init_hidden(encoder_hidden, encoder_cell, this.encoder.bidirectional)  # Initialize the decoder hidden state and cell state\n",
        "        attention_map = torch.zeros(batch_size, target_len, input_len)  # Initialize the attention map tensor\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            output, hidden, cell, attention = this.decoder.forward(input, encoder_outputs, hidden, cell)  # Perform a forward pass through the decoder\n",
        "            attention_map[:, t-1, :] = attention.squeeze(1)  # Store the attention weights in the attention map tensor\n",
        "            outputs[:, t] = output.squeeze(1)  # Store the output in the outputs tensor\n",
        "            top1 = output.argmax(-1)  # Get the index of the highest probability token\n",
        "            input = top1.squeeze(1)  # Update the decoder input with the predicted token\n",
        "\n",
        "        return outputs, attention_map\n"
      ],
      "metadata": {
        "id": "1NyL4anCdtC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sweep configuration for Bayesian optimization\n",
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'name': 'Hyperparameter Tuning-Bayesian'\n",
        "}\n",
        "\n",
        "# Define the metric to be maximized during tuning\n",
        "metric = {\n",
        "    'name': 'Val_Accuracy',\n",
        "    'goal': 'maximize'   \n",
        "}\n",
        "\n",
        "# Add the metric to the sweep configuration\n",
        "sweep_config['metric'] = metric\n",
        "\n",
        "# Define the dictionary of hyperparameters and their values to be tuned\n",
        "parameters_dict = {\n",
        "    'optimiser': {\n",
        "        'values': ['nadam']\n",
        "    },\n",
        "    'teacher_forcing_ratio': {\n",
        "        'values': [0.5, 0.7]\n",
        "    },\n",
        "    'bidirectional': {\n",
        "        'values': [True, False]\n",
        "    },\n",
        "    'enc_embedding': {\n",
        "        'values': [128, 256]\n",
        "    },\n",
        "    'dec_embedding': {\n",
        "        'values': [128, 256]\n",
        "    },\n",
        "    'epochs': {\n",
        "        'values': [3, 5]\n",
        "    },\n",
        "    'hidden_size': {\n",
        "        'values': [64, 128, 256]\n",
        "    },\n",
        "    'enc_layers': {\n",
        "        'values': [2, 3]\n",
        "    },\n",
        "    'dec_layers': {\n",
        "        'values': [2, 3]\n",
        "    },\n",
        "    'dropout': {\n",
        "        'values': [0.25, 0.4]\n",
        "    },\n",
        "    'cell_type': {\n",
        "        'values': ['gru', 'rnn']\n",
        "    }\n",
        "}\n",
        "\n",
        "# Add the hyperparameter dictionary to the sweep configuration\n",
        "sweep_config['parameters'] = parameters_dict\n",
        "\n",
        "# Initialize the sweep with the given configuration and project name\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"Attention Assignment 3\")\n",
        "\n",
        "# Function to train the model with different configurations\n",
        "def train_sweep(config=None):\n",
        "    with wandb.init(config=config) as run:\n",
        "        config = wandb.config\n",
        "\n",
        "        # Set up the model and hyperparameters\n",
        "        input_dim = len(dict)\n",
        "        output_dim = len(dict_t)\n",
        "        batch_size = 32\n",
        "        val_batch_size = 32\n",
        "        enc_embedding = config.enc_embedding\n",
        "        dec_embedding = config.dec_embedding\n",
        "        hidden = config.hidden_size\n",
        "        enc_num_layers = config.enc_layers\n",
        "        dec_num_layers = config.dec_layers\n",
        "        enc_dropout = config.dropout\n",
        "        dec_dropout = config.dropout\n",
        "        max_length = mtl\n",
        "        cell_type = config.cell_type\n",
        "\n",
        "        # Create encoder and decoder instances\n",
        "        enc = EncoderRNN(device, cell_type, input_dim, enc_embedding, hidden, enc_num_layers, bidirectional=config.bidirectional, dropout_p=enc_dropout)\n",
        "        dec = AttentionDecoderRNN(device, cell_type, output_dim, dec_embedding, hidden, max_length, dec_dropout, dec_num_layers, bidirectional=config.bidirectional)\n",
        "        model = AttentionSeq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "        # Determine the model name based on bidirectional and cell type\n",
        "        if config.bidirectional:\n",
        "            m = '_bi_'\n",
        "        else:\n",
        "            m = '_uni_'\n",
        "\n",
        "        exp_name = str(config.cell_type)\n",
        "        exp_name = exp_name\n",
        "        exp_name = exp_name + '_optim_' + config.optimiser\n",
        "\n",
        "        wandb.run.name = exp_name\n",
        "\n",
        "        # Choose the optimizer based on the configuration\n",
        "        if config.optimiser == 'adam':\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "        elif config.optimiser == 'nadam':\n",
        "            optimizer = torch.optim.NAdam(model.parameters(), lr=0.001)\n",
        "\n",
        "        criterion = nn.NLLLoss()\n",
        "\n",
        "        # Train the model with the given hyperparameters\n",
        "        trainIters(model, pairs, 32, config.epochs, optimizer, config.teacher_forcing_ratio, Attention=True)\n",
        "\n",
        "# Run the sweep agent to train the model with different configurations\n",
        "wandb.agent(sweep_id, train_sweep, count=50)\n",
        "\n",
        "# Finish the run\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 968
        },
        "id": "PdnU7lTcfZcI",
        "outputId": "845142e7-e7ae-45b4-f1f3-6b073db6c8a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: 3d3bggif\n",
            "Sweep URL: https://wandb.ai/ed22s009/Attention%20Assignment%203/sweeps/3d3bggif\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: p5y9khrm with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: rnn\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_embedding: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_embedding: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimiser: nadam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33med22s009\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230521_132952-p5y9khrm</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ed22s009/Attention%20Assignment%203/runs/p5y9khrm' target=\"_blank\">elated-sweep-1</a></strong> to <a href='https://wandb.ai/ed22s009/Attention%20Assignment%203' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ed22s009/Attention%20Assignment%203/sweeps/3d3bggif' target=\"_blank\">https://wandb.ai/ed22s009/Attention%20Assignment%203/sweeps/3d3bggif</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ed22s009/Attention%20Assignment%203' target=\"_blank\">https://wandb.ai/ed22s009/Attention%20Assignment%203</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/ed22s009/Attention%20Assignment%203/sweeps/3d3bggif' target=\"_blank\">https://wandb.ai/ed22s009/Attention%20Assignment%203/sweeps/3d3bggif</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ed22s009/Attention%20Assignment%203/runs/p5y9khrm' target=\"_blank\">https://wandb.ai/ed22s009/Attention%20Assignment%203/runs/p5y9khrm</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0, 0.0, 0.0]\n",
            "Val loss = 1.3782434463500977\n",
            "Word-level-accuracy on val set = 0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Val_Accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Val_Accuracy</td><td>0.0</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">elated-sweep-1</strong> at: <a href='https://wandb.ai/ed22s009/Attention%20Assignment%203/runs/p5y9khrm' target=\"_blank\">https://wandb.ai/ed22s009/Attention%20Assignment%203/runs/p5y9khrm</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230521_132952-p5y9khrm/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: v6tf2y2p with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: rnn\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_embedding: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.25\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_embedding: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimiser: nadam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230521_133023-v6tf2y2p</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ed22s009/Attention%20Assignment%203/runs/v6tf2y2p' target=\"_blank\">pretty-sweep-2</a></strong> to <a href='https://wandb.ai/ed22s009/Attention%20Assignment%203' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ed22s009/Attention%20Assignment%203/sweeps/3d3bggif' target=\"_blank\">https://wandb.ai/ed22s009/Attention%20Assignment%203/sweeps/3d3bggif</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ed22s009/Attention%20Assignment%203' target=\"_blank\">https://wandb.ai/ed22s009/Attention%20Assignment%203</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/ed22s009/Attention%20Assignment%203/sweeps/3d3bggif' target=\"_blank\">https://wandb.ai/ed22s009/Attention%20Assignment%203/sweeps/3d3bggif</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ed22s009/Attention%20Assignment%203/runs/v6tf2y2p' target=\"_blank\">https://wandb.ai/ed22s009/Attention%20Assignment%203/runs/v6tf2y2p</a>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}