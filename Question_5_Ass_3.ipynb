{
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swapnilmn/Assignment_03/blob/main/Question_5_Ass_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5"
      ],
      "metadata": {
        "id": "Be-2IYuGwILt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "!pip install wandb -qU\n",
        "import wandb\n",
        "\n",
        "# Check if CUDA is available and set the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def my_function():\n",
        "    # Install and import the wandb library for experiment tracking\n",
        "    !pip install wandb -qU\n",
        "    import wandb\n",
        "    \n",
        "    # Login to wandb with your API key\n",
        "    !wandb login 5dbfdc4a0de265f2dd69d623f169b7884aa4436f\n",
        "    \n",
        "    # Mount Google Drive to access files\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Call the function\n",
        "my_function()\n"
      ],
      "metadata": {
        "id": "rgNYBCrQug_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class Helper_Functions:\n",
        "    @staticmethod\n",
        "    def Time(s):\n",
        "        # Converts seconds to minutes and seconds\n",
        "        m = math.floor(s / 60)\n",
        "        s -= m * 60\n",
        "        return '%dm %ds' % (m, s)\n",
        "\n",
        "    @staticmethod\n",
        "    def Span(since, percent):\n",
        "        # Calculates the elapsed time and estimated remaining time\n",
        "        now = time.time()\n",
        "        s = now - since\n",
        "        es = s / percent\n",
        "        rs = es - s\n",
        "        return '%s (- %s)' % (Helper_Functions.Time(s), Helper_Functions.Time(rs))\n",
        "\n",
        "    @staticmethod\n",
        "    def key_printing(d, v):\n",
        "        # Prints keys from a dictionary that have specific values\n",
        "        f = [k for k, val in d.items() if val in v]\n",
        "        if f:\n",
        "            s = ''.join(f)\n",
        "            if s[-1] == '\\n':\n",
        "                s = s[:-1]\n",
        "            elif s[0] == '\\t':\n",
        "                s = s[1:]\n",
        "        else:\n",
        "            print(\"No keys\")\n",
        "\n",
        "        return s\n",
        "\n",
        "    @staticmethod\n",
        "    def char_acc(t, o):\n",
        "        # Calculates character-level accuracy between predicted and target tensors\n",
        "        with torch.no_grad():\n",
        "            se = [[o[i][j].item() == t[i][j].item() for j in range(t.shape[1])] for i in range(t.shape[0])]\n",
        "            c = np.sum(se)\n",
        "            tc = np.sum([len(row) for row in se])\n",
        "        return c / tc\n",
        "\n",
        "    @staticmethod\n",
        "    def word_acc(t, o):\n",
        "        # Calculates word-level accuracy between predicted and target tensors\n",
        "        o1 = torch.argmax(o, dim=1)\n",
        "        with torch.no_grad():\n",
        "            c = sum([(o1[i] == t[i]).sum().item() == t.shape[1] for i in range(t.shape[0])])\n",
        "        return c / t.shape[0]\n",
        "\n",
        "    @staticmethod\n",
        "    def sample_equidi_pts(d, e):\n",
        "        # Samples equidistant points from a given list\n",
        "        s = len(d) // e\n",
        "        i = np.arange(0, len(d), s)\n",
        "        p = [d[j] for j in i]\n",
        "        return p\n",
        "\n",
        "\n",
        "HF = Helper_Functions()\n",
        "\n",
        "# Creating a dictionary of helper functions\n",
        "variables = {\n",
        "    'asMinutes': HF.Time,\n",
        "    'timeSince': HF.Span,\n",
        "    'key_printing': HF.key_printing,\n",
        "    'char_acc': HF.char_acc,\n",
        "    'word_acc': HF.word_acc,\n",
        "    'sample_equidi_pts': HF.sample_equidi_pts\n",
        "}\n",
        "\n",
        "# Accessing the variables\n",
        "asMinutes = variables['asMinutes']\n",
        "timeSince = variables['timeSince']\n",
        "key_printing = variables['key_printing']\n",
        "char_acc = variables['char_acc']\n",
        "word_acc = variables['word_acc']\n",
        "sample_equidi_pts = variables['sample_equidi_pts']\n",
        "\n",
        "# Usage\n",
        "print(asMinutes)  # Prints the formatted time in minutes and seconds\n",
        "print(timeSince)  # Prints the elapsed time and estimated remaining time\n",
        "print(key_printing)  # Prints keys from a dictionary that have specific values\n",
        "print(char_acc)  # Calculates and returns character-level accuracy\n",
        "print(word_acc)  # Calculates and returns word-level accuracy\n",
        "print(sample_equidi_pts)  # Samples equidistant points from a given list\n"
      ],
      "metadata": {
        "id": "yMtxX6Pfv-1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dictionary of variables and their corresponding characters\n",
        "variables = {\n",
        "    's_t_c_r': '\\t',     # Represents the tab character\n",
        "    'e_d_c_r': '\\n',       # Represents the newline character\n",
        "    'b_k_c_r': ' ',      # Represents a blank space character\n",
        "    'u_n_c_r': '\\r'    # Represents a carriage return character\n",
        "}\n",
        "\n",
        "# Assign the value of 's_t_c_r' from the 'variables' dictionary to the variable 's_t_c_r'\n",
        "s_t_c_r = variables['s_t_c_r']\n",
        "\n",
        "# Assign the value of 'e_d_c_r' from the 'variables' dictionary to the variable 'e_d_c_r'\n",
        "e_d_c_r = variables['e_d_c_r']\n",
        "\n",
        "# Assign the value of 'b_k_c_r' from the 'variables' dictionary to the variable 'b_k_c_r'\n",
        "b_k_c_r = variables['b_k_c_r']\n",
        "\n",
        "# Assign the value of 'u_n_c_r' from the 'variables' dictionary to the variable 'u_n_c_r'\n",
        "u_n_c_r = variables['u_n_c_r']\n"
      ],
      "metadata": {
        "id": "DS5XcLIToSUz",
        "execution": {
          "iopub.status.busy": "2023-05-21T05:31:38.074939Z",
          "iopub.execute_input": "2023-05-21T05:31:38.075348Z",
          "iopub.status.idle": "2023-05-21T05:31:38.080865Z",
          "shell.execute_reply.started": "2023-05-21T05:31:38.075311Z",
          "shell.execute_reply": "2023-05-21T05:31:38.079611Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv  # Import the csv module\n",
        "import torch  # Import the torch module\n",
        "\n",
        "class DataPreProcessor:\n",
        "    def __init__(self):\n",
        "        self.file_paths = {\n",
        "            'file1': '/content/drive/MyDrive/aksharantar_sampled/mar/mar_test.csv',\n",
        "            'file2': '/content/drive/MyDrive/aksharantar_sampled/mar/mar_train.csv',\n",
        "            'file3': '/content/drive/MyDrive/aksharantar_sampled/mar/mar_train.csv'\n",
        "        }\n",
        "\n",
        "        self.variables = {\n",
        "            's_t_c_r': '\\t',      # Represents the tab character\n",
        "            'e_d_c_r': '\\n',        # Represents the newline character\n",
        "            'b_k_c_r': ' ',       # Represents a blank space character\n",
        "            'u_n_c_r': '\\r'     # Represents a carriage return character\n",
        "        }\n",
        "\n",
        "    def read_csv_files(self):\n",
        "        fr = {}  # Dictionary to store csv reader objects\n",
        "        hd = {}  # Dictionary to store headers of each file\n",
        "        dt = {}  # Dictionary to store data from each file\n",
        "\n",
        "        for key, fp in self.file_paths.items():\n",
        "            file = open(fp)  # Open the file with the given file path\n",
        "            csv_reader = csv.reader(file)  # Create a csv reader object\n",
        "            fr[key] = csv_reader  # Store the csv reader object in the dictionary\n",
        "            hd[key] = next(csv_reader)  # Retrieve the headers from the csv reader\n",
        "            dt[key] = []  # Initialize an empty list to store the data\n",
        "\n",
        "            for row in csv_reader:\n",
        "                dt[key].append(row)  # Append each row of data to the corresponding list\n",
        "\n",
        "            file.close()  # Close the file after reading\n",
        "\n",
        "        # Return the headers and data from each file\n",
        "        return hd['file1'], hd['file2'], hd['file3'], dt['file1'], dt['file2'], dt['file3']\n",
        "\n",
        "    def Reading_Data(self, lst):\n",
        "        i = [pair[0] for pair in lst]  # Extract the first element from each pair in the given list\n",
        "        t = [pair[1] for pair in lst]  # Extract the second element from each pair in the given list\n",
        "        return i, t  # Return the extracted first elements as 'i' and second elements as 't'\n",
        "\n",
        "\n",
        "    def Dict_lang(self, inputs, targets):\n",
        "        i_dict = {}         # Dictionary to store character-to-index mapping for inputs\n",
        "        max_i_length = 0    # Variable to track maximum input string length\n",
        "        i_char = []         # List to store unique characters in inputs\n",
        "\n",
        "        t_dict = {}         # Dictionary to store character-to-index mapping for targets\n",
        "        max_t_length = 0    # Variable to track maximum target string length\n",
        "        t_char = []         # List to store unique characters in targets\n",
        "\n",
        "        # Encoding Inputs and updating i_dict\n",
        "        for string in inputs:\n",
        "            max_i_length = max(len(string), max_i_length)  # Update maximum input string length\n",
        "            for char in string:\n",
        "                if char not in i_dict:\n",
        "                    i_dict[char] = len(i_char)   # Assign a unique index to each unique character\n",
        "                    i_char.append(char)         # Store the unique character in the list\n",
        "\n",
        "        if self.variables['b_k_c_r'] not in i_dict:\n",
        "            i_dict[self.variables['b_k_c_r']] = len(i_char)   # Assign index to the blank character if not present\n",
        "            i_char.append(self.variables['b_k_c_r'])\n",
        "\n",
        "        i_dict[self.variables['u_n_c_r']] = len(i_char)     # Assign index to the unknown character\n",
        "        i_char.append(self.variables['u_n_c_r'])\n",
        "\n",
        "        if self.variables['s_t_c_r'] not in t_dict:\n",
        "            t_dict[self.variables['s_t_c_r']] = len(t_char)   # Assign index to the start character if not present\n",
        "            t_char.append(self.variables['s_t_c_r'])\n",
        "\n",
        "        # Encoding Targets and updating t_dict\n",
        "        for string in targets:\n",
        "            max_t_length = max(len(string) + 2, max_t_length)    # Update maximum target string length\n",
        "            for char in string:\n",
        "                if char not in t_dict:\n",
        "                    t_dict[char] = len(t_char)   # Assign a unique index to each unique character\n",
        "                    t_char.append(char)         # Store the unique character in the list\n",
        "\n",
        "        if self.variables['e_d_c_r'] not in t_dict:\n",
        "            t_dict[self.variables['e_d_c_r']] = len(t_char)     # Assign index to the end character if not present\n",
        "            t_char.append(self.variables['e_d_c_r'])\n",
        "\n",
        "        if self.variables['b_k_c_r'] not in t_dict:\n",
        "            t_dict[self.variables['b_k_c_r']] = len(t_char)   # Assign index to the blank character if not present\n",
        "            t_char.append(self.variables['b_k_c_r'])\n",
        "\n",
        "        return i_dict, max_i_length, i_char, t_dict, max_t_length, t_char\n",
        "\n",
        "    def encoding_words(self, wl, ld, ml, l):\n",
        "        encds = []   # List to store the encoded sequences\n",
        "        for w in wl:\n",
        "            encd = [ld[c] if c in ld else ld[self.variables['u_n_c_r']] for c in w]\n",
        "            # Assign the index of each character in the word, or the index of unknown character if not present\n",
        "\n",
        "            if l == 0:\n",
        "                encd.extend([ld[self.variables['b_k_c_r']]] * (ml - len(encd)))\n",
        "                # If 'l' is 0 (inputs), pad the sequence with blank character indices up to the maximum length\n",
        "            if l == 1:\n",
        "                encd = [ld[self.variables['s_t_c_r']]] + encd + [ld[self.variables['e_d_c_r']]]\n",
        "                encd.extend([ld[self.variables['b_k_c_r']]] * (ml - len(encd)))\n",
        "                # If 'l' is 1 (targets), add start and end character indices to the sequence and pad it with blank character indices up to the maximum length\n",
        "\n",
        "            encds.append(encd)  # Append the encoded sequence to the list\n",
        "\n",
        "        return encds  # Return the list of encoded sequences\n",
        "\n",
        "\n",
        "    def Tokenzieations(self, train, val, test, input_dict, target_dict, max_input_length, max_target_length):\n",
        "        t_i, t_t = self.Reading_Data(train)  # Reading train data\n",
        "        te_i, te_t = self.Reading_Data(test)  # Reading test data\n",
        "        v_i, v_t = self.Reading_Data(val)  # Reading validation data\n",
        "\n",
        "        e_t_i = self.encoding_words(t_i, input_dict, max_input_length, 0)  # Encoding train inputs\n",
        "        e_t_t = self.encoding_words(t_t, target_dict, max_target_length, 1)  # Encoding train targets\n",
        "        e_v_i = self.encoding_words(v_i, input_dict, max_input_length, 0)  # Encoding validation inputs\n",
        "        e_v_t = self.encoding_words(v_t, target_dict, max_target_length, 1)  # Encoding validation targets\n",
        "        e_te_i = self.encoding_words(te_i, input_dict, max_input_length, 0)  # Encoding test inputs\n",
        "        e_te_t = self.encoding_words(te_t, target_dict, max_target_length, 1)  # Encoding test targets\n",
        "\n",
        "        return {\n",
        "            'en_tr_ip': e_t_i,\n",
        "            'en_tr_tr': e_t_t,\n",
        "            'en_vl_ip': e_v_i,\n",
        "            'en_vl_tr': e_v_t,\n",
        "            'en_tt_ip': e_te_i,\n",
        "            'en_tt_tr': e_te_t\n",
        "        }\n",
        "\n",
        "    def tensor_pair_conversion(self, tt_ip, tt_tr):\n",
        "        pairs = [(torch.tensor(input_data), torch.tensor(target_data)) for input_data, target_data in\n",
        "                zip(tt_ip, tt_tr)]  # Creating pairs of tensor inputs and targets\n",
        "        return pairs\n",
        "\n",
        "DPP = DataPreProcessor()\n",
        "\n",
        "# Dictionary of functions\n",
        "function_dict = {\n",
        "    'tensor_pair_conversion': DPP.tensor_pair_conversion,\n",
        "    'Tokenzieations': DPP.Tokenzieations,\n",
        "    'encoding_words': DPP.encoding_words,\n",
        "    'Reading_Data': DPP.Reading_Data,\n",
        "    'Dict_lang': DPP.Dict_lang,\n",
        "    'read_csv_files': DPP.read_csv_files\n",
        "}\n",
        "\n",
        "tensor_pair_conversion = function_dict['tensor_pair_conversion']\n",
        "Tokenzieations = function_dict['Tokenzieations']\n",
        "encoding_words = function_dict['encoding_words']\n",
        "Reading_Data = function_dict['Reading_Data']\n",
        "Dict_lang = function_dict['Dict_lang']\n",
        "read_csv_files = function_dict['read_csv_files']\n",
        "\n",
        "# Call the function and assign the returned values to variables\n",
        "h1, h2, h3, test, val, train = read_csv_files()\n",
        "\n",
        "tt_ip, tt_tr = Reading_Data(train)  # Reading train data\n",
        "tst_ip, tst_tr = Reading_Data(test)  # Reading test data\n",
        "vl_ip, vl_tr = Reading_Data(val)  # Reading validation data\n",
        "\n",
        "print(tt_ip[1])  # Print train input at index 1\n",
        "print(tt_tr[1])  # Print train target at index 1\n",
        "\n",
        "input_dict, max_input_length, input_char, target_dict, max_target_length, target_char = Dict_lang(tt_ip + vl_ip + tst_ip, tt_tr + vl_tr + tst_tr)  # Generating dictionaries for inputs and targets\n",
        "\n",
        "result = Tokenzieations(train, val, test, input_dict, target_dict, max_input_length, max_target_length)  # Tokenizing the data\n",
        "\n",
        "en_tr_ip = result['en_tr_ip']  # Encoded train inputs\n",
        "en_tr_tr = result['en_tr_tr']  # Encoded train targets\n",
        "en_vl_ip = result['en_vl_ip']  # Encoded validation inputs\n",
        "en_vl_tr = result['en_vl_tr']  # Encoded validation targets\n",
        "en_tt_ip = result['en_tt_ip']  # Encoded test inputs\n",
        "en_tt_tr = result['en_tt_tr']  # Encoded test targets\n",
        "\n",
        "r = random.randint(0,100)  # Generate a random number between 0 and 100\n",
        "print(key_printing(input_dict, en_tr_ip[int(r)]))  # Print the keys corresponding to the values in input_dict for the randomly selected encoded train input\n",
        "print(key_printing(target_dict, en_tr_tr[int(r)]))  # Print the keys corresponding to the values in target_dict for the randomly selected encoded train target\n",
        "\n",
        "en_tt_pa = tensor_pair_conversion(en_tr_ip, en_tr_tr)  # Convert encoded train inputs and targets into pairs of tensors\n",
        "en_vl_pa = tensor_pair_conversion(en_vl_ip, en_vl_tr)  # Convert encoded validation inputs and targets into pairs of tensors\n",
        "en_tst_pa = tensor_pair_conversion(en_tt_ip, en_tt_tr)  # Convert encoded test inputs and targets into pairs of tensors\n",
        "\n",
        "pairs = (en_tt_pa, en_vl_pa, en_tst_pa)  # Combine pairs into a tuple\n",
        "pair = random.choice(en_tt_pa)  # Randomly select a pair from the encoded train pairs\n",
        "\n",
        "print(key_printing(input_dict, pair[0]))  # Print the keys corresponding to the values in input_dict for the selected pair input\n",
        "print(key_printing(target_dict, pair[1]))  # Print the keys corresponding to the values in target_dict for the selected pair target\n"
      ],
      "metadata": {
        "id": "hVm6ME71IQEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Recurrent_NN_Encoder(nn.Module):\n",
        "    def __init__(self, device, cell_type, vocab_size, embed_dim, hidden_size, num_layers=1, bidirectional=False, dropout_p=0):\n",
        "        super(Recurrent_NN_Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # Embedding layer for converting input indices to dense vectors\n",
        "        self.hidden_size = hidden_size  # Size of the hidden state in the RNN\n",
        "        self.num_layers = num_layers  # Number of layers in the RNN\n",
        "        self.bidirectional = bidirectional  # Flag indicating whether the RNN is bidirectional or not\n",
        "        self.cell_type = cell_type  # Type of RNN cell ('lstm', 'rnn', or 'gru')\n",
        "        self.dropout_p = dropout_p  # Dropout probability\n",
        "        self.dropout = nn.Dropout(self.dropout_p)  # Dropout layer for regularization\n",
        "        \n",
        "        # Initialize the RNN cell based on the specified cell type\n",
        "        if cell_type == 'lstm':\n",
        "            self.rnn = nn.LSTM(embed_dim, hidden_size, num_layers=self.num_layers, batch_first=True, dropout=dropout_p, bidirectional=self.bidirectional)\n",
        "        elif cell_type == 'rnn':\n",
        "            self.rnn = nn.RNN(embed_dim, hidden_size, num_layers=self.num_layers, batch_first=True, dropout=dropout_p, bidirectional=self.bidirectional)\n",
        "        elif cell_type == 'gru':\n",
        "            self.rnn = nn.GRU(embed_dim, hidden_size, num_layers=self.num_layers, batch_first=True, dropout=dropout_p, bidirectional=self.bidirectional)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        out = self.embedding(x)  # Perform embedding lookup to get dense representations of input indices\n",
        "        out = self.dropout(out)  # Apply dropout to the input embeddings\n",
        "        if self.cell_type == 'lstm':\n",
        "            out, (hidden, cell) = self.rnn(out, (hidden, cell))  # Forward pass through the LSTM\n",
        "            return out, hidden, cell\n",
        "        elif self.cell_type == 'rnn':\n",
        "            out, hidden = self.rnn(out, hidden)  # Forward pass through the RNN\n",
        "            return out, hidden\n",
        "        elif self.cell_type == 'gru':\n",
        "            out, hidden = self.rnn(out, hidden)  # Forward pass through the GRU\n",
        "            return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        # Initialize the hidden and cell states with random values\n",
        "        hidden = torch.randn((1 + int(self.bidirectional)) * self.num_layers, batch_size, self.hidden_size, device=device)\n",
        "        cell = torch.randn((1 + int(self.bidirectional)) * self.num_layers, batch_size, self.hidden_size, device=device)\n",
        "        return hidden, cell\n",
        "\n",
        "\n",
        "class Recurrent_NN_Decoder(nn.Module):\n",
        "    def __init__(self, device, cell_type, output_vocab, embed_size, hidden_size, max_length, dropout_p=0.1, num_layers=1, bidirectional=False):\n",
        "        super(Recurrent_NN_Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size  # Hidden size of the decoder\n",
        "        self.output_size = output_vocab  # Size of the output vocabulary\n",
        "        self.embed_size = embed_size  # Size of the input embedding\n",
        "        self.dropout_p = dropout_p  # Dropout probability\n",
        "        self.cell_type = cell_type  # Type of RNN cell (lstm, gru, rnn)\n",
        "        self.max_length = max_length  # Maximum length of input sequence\n",
        "        self.device = device  # Device (e.g., CPU or GPU) to be used for computations\n",
        "        self.num_layers = num_layers  # Number of layers in the decoder\n",
        "        self.embedding_decoder = nn.Embedding(self.output_size, self.embed_size)  # Embedding layer for the decoder\n",
        "        self.dropout = nn.Dropout(self.dropout_p)  # Dropout layer\n",
        "        self.bidirectional = bidirectional  # Flag indicating if the encoder is bidirectional\n",
        "\n",
        "        # Determine the type of RNN cell based on the given cell_type\n",
        "        if cell_type == 'lstm':\n",
        "            self.rnn = nn.LSTM(self.embed_size, hidden_size, num_layers=self.num_layers, batch_first=True, bidirectional=self.bidirectional, dropout=self.dropout_p)\n",
        "        elif cell_type == 'gru':\n",
        "            self.rnn = nn.GRU(self.embed_size, hidden_size, num_layers=self.num_layers, batch_first=True, bidirectional=self.bidirectional, dropout=self.dropout_p)\n",
        "        elif cell_type == 'rnn':\n",
        "            self.rnn = nn.RNN(self.embed_size, hidden_size, num_layers=self.num_layers, batch_first=True, bidirectional=self.bidirectional, dropout=self.dropout_p)\n",
        "\n",
        "        self.out = nn.Linear((1 + int(self.bidirectional)) * self.hidden_size, self.output_size)  # Linear layer for output prediction\n",
        "        self.out_activation = nn.LogSoftmax(dim=-1)  # Log softmax activation for output probabilities\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        input = input.unsqueeze(1)  # Add a singleton dimension to the input tensor\n",
        "        embedded_decoder = self.embedding_decoder(input)  # Apply embedding to the input\n",
        "        embedded_decoder = self.dropout(embedded_decoder)  # Apply dropout to the embedded input\n",
        "\n",
        "        # Pass the embedded input through the RNN cell\n",
        "        if self.cell_type == 'lstm':\n",
        "            output, (hidden, cell) = self.rnn(embedded_decoder, (hidden, cell))\n",
        "        elif self.cell_type == 'gru':\n",
        "            output, hidden = self.rnn(embedded_decoder, hidden)\n",
        "        elif self.cell_type == 'rnn':\n",
        "            output, hidden = self.rnn(embedded_decoder, hidden)\n",
        "\n",
        "        output = F.relu(self.out(output))  # Apply ReLU activation to the output\n",
        "        output = F.log_softmax(output, dim=-1)  # Apply log softmax activation to obtain output probabilities\n",
        "\n",
        "        return output, hidden, cell  # Return the output, hidden state, and cell state\n",
        "\n",
        "    def init_hidden(self, encoder_hidden, encoder_cell, encoder_bidirectional):\n",
        "        hidden = encoder_hidden[-(1 + int(encoder_bidirectional)): ].repeat(self.num_layers, 1, 1)  # Initialize the hidden state using the encoder's hidden state\n",
        "        cell = encoder_cell[-(1 + int(encoder_bidirectional)): ].repeat(self.num_layers, 1, 1)  # Initialize the cell state using the encoder's cell state\n",
        "        return hidden, cell  # Return the initialized hidden state and cell state\n",
        "\n",
        "\n",
        "\n",
        "class Seqence_2_Seqence_Network(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder  # Initialize the encoder\n",
        "        self.decoder = decoder  # Initialize the decoder\n",
        "        self.device = device  # Store the device (e.g., CPU or GPU)\n",
        "        self.max_target_length = 0  # Variable to store the maximum target sequence length\n",
        "        self.sos = 0  # Start-of-sequence token\n",
        "        \n",
        "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "        batch_size = target.shape[0]  # Get the batch size\n",
        "        target_len = target.shape[1]  # Get the target sequence length\n",
        "        self.max_target_length = target_len  # Update the maximum target sequence length\n",
        "        target_vocab_size = self.decoder.output_size  # Get the target vocabulary size\n",
        "        \n",
        "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)  # Initialize the outputs tensor\n",
        "        \n",
        "        encoder_hidden, encoder_cell = self.encoder.init_hidden(batch_size)  # Initialize the encoder hidden state and cell state\n",
        "        \n",
        "        if (self.encoder.cell_type == 'lstm'):\n",
        "            encoder_outputs, encoder_hidden, encoder_cell = self.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "            # Forward pass through the encoder LSTM\n",
        "        if (self.encoder.cell_type == 'rnn'):\n",
        "            encoder_outputs, encoder_hidden = self.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "            # Forward pass through the encoder RNN\n",
        "        if (self.encoder.cell_type == 'gru'):\n",
        "            encoder_outputs, encoder_hidden = self.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "            # Forward pass through the encoder GRU\n",
        "        \n",
        "        input = target[:, 0]  # Set the first input to the decoder as the <sos> token\n",
        "        self.sos = target[:, 0]  # Store the <sos> token\n",
        "        \n",
        "        hidden, cell = self.decoder.init_hidden(encoder_hidden, encoder_cell, self.encoder.bidirectional)\n",
        "        # Initialize the decoder hidden state and cell state\n",
        "        \n",
        "        for t in range(1, target_len):\n",
        "            output, hidden, cell = self.decoder.forward(input, hidden, cell)\n",
        "            # Forward pass through the decoder\n",
        "            outputs[:, t] = output.squeeze(1)  # Store the decoder output in the outputs tensor\n",
        "            teacher_force = random.random() < teacher_forcing_ratio  # Determine whether to use teacher forcing\n",
        "            top1 = output.argmax(-1)  # Get the index of the highest probability output\n",
        "            input = target[:, t] if teacher_force else top1.squeeze(1)  # Set the next input to the decoder\n",
        "            \n",
        "        return outputs\n",
        "    \n",
        "    def inference(self, source, target):\n",
        "        batch_size = source.shape[0]  # Get the batch size\n",
        "        target_len = self.max_target_length  # Get the maximum target sequence length\n",
        "        target_vocab_size = self.decoder.output_size  # Get the target vocabulary size\n",
        "        \n",
        "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)  # Initialize the outputs tensor\n",
        "        \n",
        "        encoder_hidden, encoder_cell = self.encoder.init_hidden(batch_size)  # Initialize the encoder hidden state and cell state\n",
        "        \n",
        "        if (self.encoder.cell_type == 'lstm'):\n",
        "            encoder_outputs, encoder_hidden, encoder_cell = self.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "            # Forward pass through the encoder LSTM\n",
        "        if (self.encoder.cell_type == 'rnn'):\n",
        "            encoder_outputs, encoder_hidden = self.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "            # Forward pass through the encoder RNN\n",
        "        if (self.encoder.cell_type == 'gru'):\n",
        "            encoder_outputs, encoder_hidden = self.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "            # Forward pass through the encoder GRU\n",
        "        \n",
        "        input = self.sos  # Set the first input to the <sos> token\n",
        "        \n",
        "        hidden, cell = self.decoder.init_hidden(encoder_hidden, encoder_cell, self.encoder.bidirectional)\n",
        "        # Initialize the decoder hidden state and cell state\n",
        "        \n",
        "        for t in range(1, target_len):\n",
        "            output, hidden, cell = self.decoder.forward(input, hidden, cell)\n",
        "            # Forward pass through the decoder\n",
        "            outputs[:, t] = output.squeeze(1)  # Store the decoder output in the outputs tensor\n",
        "            top1 = output.argmax(-1)  # Get the index of the highest probability output\n",
        "            input = top1.squeeze(1)  # Set the next input to the decoder\n",
        "            \n",
        "        return outputs\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UBE07BLxae-0",
        "execution": {
          "iopub.status.busy": "2023-05-21T05:31:46.924970Z",
          "iopub.execute_input": "2023-05-21T05:31:46.925567Z",
          "iopub.status.idle": "2023-05-21T05:31:46.949929Z",
          "shell.execute_reply.started": "2023-05-21T05:31:46.925519Z",
          "shell.execute_reply": "2023-05-21T05:31:46.948659Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vanilla_model(input_dict, target_dict):\n",
        "    variables = {}\n",
        "    \n",
        "    # Count the number of elements in the input and target dictionaries\n",
        "    variables['a'] = len(input_dict)\n",
        "    variables['b'] = len(target_dict)\n",
        "    \n",
        "    # Set values for variables c, d, e, f, g, h, i, j, k, and max_length\n",
        "    variables['c'] = 32    #batch size\n",
        "    variables['d'] = 32     #val batch size\n",
        "    variables['e'] = 256    #enc_embedding\n",
        "    variables['f'] = 256    #dec embedding\n",
        "    variables['g'] = 512    #hidden_size \n",
        "    variables['h'] = 3     #enc_number_layers\n",
        "    variables['i'] = 2     #dec_num_layers\n",
        "    variables['j'] = 0.4    #enc_dropout\n",
        "    variables['k'] = 0.4    #dec_drop_out\n",
        "    variables['max_length'] = max_target_length\n",
        "    \n",
        "    # Set value for variable l\n",
        "    variables['l'] = 'gru'\n",
        "    \n",
        "    # Create an instance of Recurrent_NN_Encoder and assign it to variable m\n",
        "    variables['m'] = Recurrent_NN_Encoder(device, variables['l'], variables['a'], variables['e'], variables['g'],\n",
        "                                variables['h'], bidirectional=True, dropout_p=variables['j'])\n",
        "    \n",
        "    # Create an instance of Recurrent_NN_Decoder and assign it to variable n\n",
        "    variables['n'] = Recurrent_NN_Decoder(device, variables['l'], variables['b'], variables['f'], variables['g'],\n",
        "                                variables['max_length'], variables['k'], variables['i'], bidirectional=True)\n",
        "    \n",
        "    # Create an instance of Seqence_2_Seqence_Network by passing in the encoder and decoder instances, and assign it to variable o\n",
        "    variables['o'] = Seqence_2_Seqence_Network(variables['m'], variables['n'], device).to(device)\n",
        "    \n",
        "    # Define a function count_parameters(model) that returns the total number of trainable parameters in a model\n",
        "    def count_parameters(model):\n",
        "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "    # Print the number of trainable parameters in the Seqence_2_Seqence_Network model\n",
        "    print(f' {count_parameters(variables[\"o\"]):,} trainable parameters')\n",
        "    \n",
        "    # Create an instance of Adam optimizer and assign it to variable p\n",
        "    variables['p'] = torch.optim.Adam(variables['o'].parameters(), lr=0.001)\n",
        "    \n",
        "    # Create an instance of negative log likelihood loss and assign it to variable q\n",
        "    variables['q'] = nn.NLLLoss()\n",
        "    \n",
        "    # Return the Seqence_2_Seqence_Network model, optimizer, and criterion\n",
        "    return variables['o'], variables['p'], variables['q']\n",
        "\n",
        "# Call the vanilla_model function with input_dict and target_dict, and assign the returned values to model, optimizer, and criterion respectively\n",
        "model, optimizer, criterion = vanilla_model(input_dict, target_dict)\n"
      ],
      "metadata": {
        "id": "LTsT62M9BrDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_function(model, pairs, batch_size, n_iters, optimizer, tf, print_every=10, plot_every=10, log = True, Attention = False):\n",
        "\n",
        "def train_function(m, p, b, n, o, t, p_e=10, p_v=10, l=True, A=False):\n",
        "    s = time.time()  # Starting time\n",
        "\n",
        "    p_l = []  # List to store training loss\n",
        "    t_w_a = []  # List to store word-level accuracy on training set\n",
        "    v_w_a = []  # List to store word-level accuracy on validation set\n",
        "    p_l_t = 0  # Training loss per iteration\n",
        "    p_l_v_t = 0  # Validation loss per iteration\n",
        "\n",
        "    t_p = p[0]  # Training data\n",
        "    v_p = p[1]  # Validation data\n",
        "    t_a = 0  # Accumulated word-level accuracy on training set\n",
        "\n",
        "    c = nn.NLLLoss()  # Negative log likelihood loss\n",
        "\n",
        "    count = 0  # Iteration count\n",
        "    for it in range(1, n + 1):  # Number of iterations\n",
        "        for i in range(0, len(t_p) - b, b):  # Batch processing\n",
        "            t_a = 0  # Reset accumulated word-level accuracy\n",
        "            count += 1  # Increment iteration count\n",
        "\n",
        "            if i + b > len(t_p):\n",
        "                b = len(t_p) - i + 1  # Adjust batch size if remaining samples are less than the batch size\n",
        "\n",
        "            i_t = torch.stack([t_p[i + j][0] for j in range(b)]).squeeze(1).long().cuda()  # Input tensor\n",
        "            t_t = torch.stack([t_p[i + j][1] for j in range(b)]).squeeze(1).long().cuda()  # Target tensor\n",
        "\n",
        "            o.zero_grad()  # Clear gradients\n",
        "            out = m(i_t, t_t, teacher_forcing_ratio=t if count < 4000 else 0)  # Forward pass\n",
        "            out = torch.permute(out, [0, 2, 1])  # Permute output tensor dimensions\n",
        "            l = c(out, t_t)  # Calculate loss\n",
        "\n",
        "            t_a_w = word_acc(t_t, out) * b  # Calculate word-level accuracy on training set\n",
        "            t_a += t_a_w  # Accumulate word-level accuracy\n",
        "            l.backward()  # Backward pass\n",
        "            torch.nn.utils.clip_grad_norm_(m.parameters(), 1)  # Clip gradients to avoid exploding gradients\n",
        "            o.step()  # Update model parameters\n",
        "\n",
        "            p_l_t += l  # Accumulate training loss per iteration\n",
        "            p_l_v_t += l  # Accumulate validation loss per iteration\n",
        "\n",
        "            if count % 800 == 0:\n",
        "                v_i_t = torch.stack([v_p[j][0] for j in range(b)]).squeeze(1).long().cuda()  # Validation input tensor\n",
        "                v_t_t = torch.stack([v_p[j][1] for j in range(b)]).squeeze(1).long().cuda()  # Validation target tensor\n",
        "\n",
        "                if A:\n",
        "                    v_o, _ = m.inference(v_i_t, v_t_t)\n",
        "                else:\n",
        "                    v_o = m.inference(v_i_t, v_t_t)\n",
        "\n",
        "                v_o = v_o.permute(0, 2, 1)  # Permute validation output tensor dimensions\n",
        "                v_l = c(v_o, v_t_t)  # Calculate validation loss\n",
        "                v_l_s = v_l\n",
        "                wandb.log({'Val_Loss': v_l_s})  # Log validation loss\n",
        "\n",
        "            if count % 800 == 0:\n",
        "                p_l_a = p_l_t / 800  # Average training loss over 800 iterations\n",
        "                p_l_t = 0  # Reset training loss accumulator\n",
        "                print('%s (%d %d%%) %.7f' % (timeSince(s, it / n), it, it / n * 100, p_l_a))  # Print progress\n",
        "\n",
        "            if count % 800 == 0:\n",
        "                p_l_a = p_l_v_t / 800  # Average validation loss over 800 iterations\n",
        "                p_l.append(p_l_a.detach())  # Append training loss to list\n",
        "                wandb.log({'Train Loss': p_l_a})  # Log training loss\n",
        "                p_l_v_t = 0  # Reset validation loss accumulator\n",
        "\n",
        "        t_a = t_a / (len(t_p) - b)  # Calculate average word-level accuracy on training set\n",
        "        t_w_a.append(t_a)  # Append word-level accuracy to list\n",
        "\n",
        "    print(t_w_a)  # Print word-level accuracy on training set\n",
        "\n",
        "    p_l = [l.cpu().numpy() for l in p_l]  # Convert training loss to numpy array\n",
        "    p_l_s = sample_equidi_pts(p_l, n)  # Sample equidistant points from the training loss\n",
        "\n",
        "    w_c = 0  # Total correct predictions\n",
        "    for i in range(0, len(v_p) - b, b):  # Batch processing on validation set\n",
        "        if i + b > len(v_p):\n",
        "            b = len(v_p) - i + 1  # Adjust batch size if remaining samples are less than the batch size\n",
        "\n",
        "        v_i_t = torch.stack([v_p[i + j][0] for j in range(b)]).squeeze(1).long().cuda()  # Validation input tensor\n",
        "        v_t_t = torch.stack([v_p[i + j][1] for j in range(b)]).squeeze(1).long().cuda()  # Validation target tensor\n",
        "\n",
        "        if A:\n",
        "            v_o, _ = m.inference(v_i_t, v_t_t)\n",
        "        else:\n",
        "            v_o = m.inference(v_i_t, v_t_t)\n",
        "\n",
        "        v_o = v_o.permute(0, 2, 1)  # Permute validation output tensor dimensions\n",
        "        v_l = c(v_o, v_t_t)  # Calculate validation loss\n",
        "\n",
        "        v_a_w = word_acc(v_t_t, v_o)  # Calculate word-level accuracy on validation set\n",
        "        w_c += v_a_w * b  # Accumulate correct predictions\n",
        "\n",
        "    w_a = w_c / (len(v_p) - b)  # Average word-level accuracy on validation set\n",
        "    m = {'Val_Accuracy': w_a}  # Log validation accuracy\n",
        "    wandb.log(m)\n",
        "\n",
        "    print(f\"Val loss = {v_l}\")  # Print validation loss\n",
        "    print(f'Word-level-accuracy on val set = {w_a}')  # Print word-level accuracy on validation set\n"
      ],
      "metadata": {
        "id": "3_VZGr2LQ4Lk",
        "execution": {
          "iopub.status.busy": "2023-05-21T05:32:02.467407Z",
          "iopub.execute_input": "2023-05-21T05:32:02.467856Z",
          "iopub.status.idle": "2023-05-21T05:32:07.376445Z",
          "shell.execute_reply.started": "2023-05-21T05:32:02.467816Z",
          "shell.execute_reply": "2023-05-21T05:32:07.375225Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the sweep configuration\n",
        "sw = {\n",
        "    'method': 'bayes',\n",
        "    'name': 'Hyperparameter Tuning-Bayesian'\n",
        "}\n",
        "\n",
        "# Defining the metric configuration\n",
        "m = {\n",
        "    'name': 'Val_Accuracy',\n",
        "    'goal': 'maximize'\n",
        "}\n",
        "\n",
        "# Adding the metric configuration to the sweep configuration\n",
        "sw['metric'] = m\n",
        "\n",
        "# Defining the parameters configuration\n",
        "p = {\n",
        "    'optimiser': {'values': ['adam','nadam']},\n",
        "    'teacher_forcing_ratio': {'values': [0.3, 0.5, 0.7]},\n",
        "    'bidirectional': {'values': [True, False]},\n",
        "    'enc_embedding': {'values': [128,256]},\n",
        "    'dec_embedding': {'values': [128,256]},\n",
        "    'epochs': {'values': [1]},\n",
        "    'hidden_size': {'values': [64,128,256,512]},\n",
        "    'enc_layers': {'values': [2,3]},\n",
        "    'dec_layers': {'values': [2,3]},\n",
        "    'dropout': {'values': [0.3,0.4]},\n",
        "    'cell_type': {'values': ['lstm','gru','rnn']}\n",
        "}\n",
        "\n",
        "# Adding the parameters configuration to the sweep configuration\n",
        "sw['parameters'] = p\n",
        "\n",
        "# Creating a sweep using the sweep configuration and assigning it to sw_id\n",
        "sw_id = wandb.sweep(sw, project=\"CS6910 Assignment 3\")\n",
        "\n",
        "# Function to train the sweep\n",
        "def train_sweep(c=None):\n",
        "    # Initializing wandb run with the given config\n",
        "    with wandb.init(config=c) as r:\n",
        "        c = wandb.config\n",
        "\n",
        "        # Creating the validation data dictionary\n",
        "        vd = {\n",
        "            'input_dim': len(input_dict),\n",
        "            'output_dim': len(target_dict),\n",
        "            'batch_size': 32,\n",
        "            'val_batch_size': 32,\n",
        "            'enc_embedding': c.enc_embedding,\n",
        "            'dec_embedding': c.dec_embedding,\n",
        "            'hidden': c.hidden_size,\n",
        "            'enc_num_layers': c.enc_layers,\n",
        "            'dec_num_layers': c.dec_layers,\n",
        "            'enc_dropout': c.dropout,\n",
        "            'dec_dropout': c.dropout,\n",
        "            'max_length': max_target_length,\n",
        "            'cell_type': c.cell_type\n",
        "        }\n",
        "\n",
        "        # Extracting values from the validation data dictionary\n",
        "        input_dim = vd['input_dim']\n",
        "        output_dim = vd['output_dim']\n",
        "        batch_size = vd['batch_size']\n",
        "        val_batch_size = vd['val_batch_size']\n",
        "        enc_embedding = vd['enc_embedding']\n",
        "        dec_embedding = vd['dec_embedding']\n",
        "        hidden = vd['hidden']\n",
        "        enc_num_layers = vd['enc_num_layers']\n",
        "        dec_num_layers = vd['dec_num_layers']\n",
        "        enc_dropout = vd['enc_dropout']\n",
        "        dec_dropout = vd['dec_dropout']\n",
        "        max_length = vd['max_length']\n",
        "        cell_type = vd['cell_type']\n",
        "\n",
        "        # Creating the encoder and decoder models\n",
        "        enc = Recurrent_NN_Encoder(device, cell_type, input_dim, enc_embedding, hidden, enc_num_layers,\n",
        "                         bidirectional=c.bidirectional, dropout_p=enc_dropout)\n",
        "        dec = Recurrent_NN_Decoder(device, cell_type, output_dim, dec_embedding, hidden, max_length, dec_dropout,\n",
        "                         dec_num_layers, bidirectional=c.bidirectional)\n",
        "        model = Seqence_2_Seqence_Network(enc, dec, device).to(device)\n",
        "\n",
        "        # Setting the experiment name based on the config\n",
        "        exp_name = f\"{c.cell_type}_e_{c.optimiser}\"\n",
        "\n",
        "        # Assigning the experiment name to the wandb run\n",
        "        wandb.run.name = exp_name\n",
        "\n",
        "        # Creating the optimizer based on the chosen optimiser\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001) if c.optimiser == 'adam' else torch.optim.NAdam(\n",
        "            model.parameters(), lr=0.001)\n",
        "\n",
        "        criterion = nn.NLLLoss()\n",
        "\n",
        "\n",
        "        # Training the model using the train_function function\n",
        "        train_function(model, pairs, 32, c.epochs, optimizer, c.teacher_forcing_ratio)\n",
        "# def train_function(model, pairs, batch_size, n_iters, optimizer, tf, print_every=10, plot_every=10, log = True, Attention = False):\n",
        "\n",
        "# Running the sweep agent with the given sweep ID and train_sweep function\n",
        "wandb.agent(sw_id, train_sweep, count=1)\n",
        "\n",
        "# Finishing the wandb run\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "-ssEjRurX-IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionRecurrent_NN_Decoder(nn.Module):\n",
        "    def __init__(self, device, cell_type, output_vocab, embed_size, hidden_size, max_length, dropout_p=0.1, num_layers=1, bidirectional=False):\n",
        "        super(AttentionRecurrent_NN_Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size  # Hidden size of the decoder\n",
        "        self.output_size = output_vocab  # Size of the output vocabulary\n",
        "        self.embed_size = embed_size  # Size of the input embedding\n",
        "        self.dropout_p = dropout_p  # Dropout probability\n",
        "        self.cell_type = cell_type  # Type of RNN cell (lstm, gru, rnn)\n",
        "        self.max_length = max_length  # Maximum length of input sequence\n",
        "        self.device = device  # Device (e.g., CPU or GPU) to be used for computations\n",
        "        self.num_layers = num_layers  # Number of layers in the decoder\n",
        "        self.embedding_decoder = nn.Embedding(self.output_size, self.embed_size)  # Embedding layer for the decoder\n",
        "        self.dropout = nn.Dropout(self.dropout_p)  # Dropout layer\n",
        "        self.bidirectional = bidirectional  # Flag indicating if the encoder is bidirectional\n",
        "\n",
        "        # Determine the type of RNN cell based on the given cell_type\n",
        "        if cell_type == 'lstm':\n",
        "            self.rnn = nn.LSTM(self.embed_size + hidden_size * (1 + int(self.bidirectional)), hidden_size, num_layers=self.num_layers, batch_first=True, bidirectional=self.bidirectional, dropout=self.dropout_p)\n",
        "        elif cell_type == 'gru':\n",
        "            self.rnn = nn.GRU(self.embed_size + hidden_size * (1 + int(self.bidirectional)), hidden_size, num_layers=self.num_layers, batch_first=True, bidirectional=self.bidirectional, dropout=self.dropout_p)\n",
        "        elif cell_type == 'rnn':\n",
        "            self.rnn = nn.RNN(self.embed_size + hidden_size * (1 + int(self.bidirectional)), hidden_size, num_layers=self.num_layers, batch_first=True, bidirectional=self.bidirectional)\n",
        "\n",
        "        self.energy = nn.Linear(hidden_size * (2 + int(self.bidirectional)), hidden_size)  # Linear layer for energy calculation\n",
        "        self.value = nn.Linear(hidden_size, 1, bias=False)  # Linear layer for value calculation\n",
        "        self.softmax = nn.Softmax(dim=0)  # Softmax activation for attention weights\n",
        "        self.relu = nn.ReLU()  # ReLU activation function\n",
        "        self.tanh = nn.Tanh()  # Hyperbolic tangent activation function\n",
        "        \n",
        "        self.out = nn.Linear((1 + int(self.bidirectional)) * self.hidden_size, self.output_size)  # Linear layer for output prediction\n",
        "        self.out_activation = nn.LogSoftmax(dim=-1)  # Log softmax activation for output probabilities\n",
        "  \n",
        "        self.hidden_reshape_linear = nn.Linear(hidden_size * 2, hidden_size)  # Linear layer for reshaping the hidden state\n",
        "\n",
        "    def forward(self, input, encoder_states, hidden, cell):\n",
        "        input = input.unsqueeze(1)  # Add a singleton dimension to the input tensor\n",
        "        embedded_decoder = self.embedding_decoder(input)  # Apply embedding to the input\n",
        "        embedded_decoder = self.dropout(embedded_decoder)  # Apply dropout to the embedded input\n",
        "\n",
        "        encoder_states = encoder_states.permute(1, 0, 2)  # Permute the encoder states tensor\n",
        "        sequence_length = encoder_states.shape[0]  # Obtain the sequence length\n",
        "        if self.bidirectional:\n",
        "            hidden_1 = self.relu(self.hidden_reshape_linear(hidden[0:2].permute(1, 0, 2).reshape(hidden.shape[1], -1))).unsqueeze(0)\n",
        "        else:\n",
        "            hidden_1 = hidden[0]\n",
        "\n",
        "        hidden_reshaped = hidden_1.repeat(sequence_length, 1, 1)  # Reshape and repeat the hidden state\n",
        "\n",
        "        energy = self.value(self.tanh(self.energy(torch.cat((hidden_reshaped, encoder_states), dim=2))))  # Compute the energy\n",
        "        attention = self.softmax(energy)  # Apply softmax to obtain attention weights\n",
        "        attention = attention.permute(1, 2, 0)  # Permute the attention tensor\n",
        "        encoder_states = encoder_states.permute(1, 0, 2)  # Permute the encoder states tensor\n",
        "        context_vector = torch.bmm(attention, encoder_states)  # Compute the context vector\n",
        "\n",
        "        rnn_input = torch.cat((context_vector, embedded_decoder), dim=2)  # Concatenate the context vector and embedded decoder input\n",
        "      \n",
        "        if self.cell_type == 'lstm':\n",
        "            decoder_output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))  # Pass the input through the RNN cell\n",
        "        elif self.cell_type == 'gru':\n",
        "            decoder_output, hidden = self.rnn(rnn_input, hidden)  # Pass the input through the RNN cell\n",
        "        elif self.cell_type == 'rnn':\n",
        "            decoder_output, hidden = self.rnn(rnn_input, hidden)  # Pass the input through the RNN cell\n",
        "\n",
        "        output = F.relu(self.out(decoder_output))  # Apply ReLU activation to the output\n",
        "        output = F.log_softmax(output, dim=-1)  # Apply log softmax activation to obtain output probabilities\n",
        "\n",
        "        return output, hidden, cell, attention  # Return the output, hidden state, cell state, and attention weights\n",
        "\n",
        "    def init_hidden(self, encoder_hidden, encoder_cell, encoder_bidirectional):\n",
        "        hidden = encoder_hidden[-(1 + int(encoder_bidirectional)): ].repeat(self.num_layers, 1, 1)  # Initialize the hidden state using the encoder's hidden state\n",
        "        cell = encoder_cell[-(1 + int(encoder_bidirectional)): ].repeat(self.num_layers, 1, 1)  # Initialize the cell state using the encoder's cell state\n",
        "        return hidden, cell  # Return the initialized hidden state and cell state\n"
      ],
      "metadata": {
        "id": "VobL42C0y3h3",
        "execution": {
          "iopub.status.busy": "2023-05-21T05:32:37.349008Z",
          "iopub.execute_input": "2023-05-21T05:32:37.349481Z",
          "iopub.status.idle": "2023-05-21T05:32:37.375779Z",
          "shell.execute_reply.started": "2023-05-21T05:32:37.349442Z",
          "shell.execute_reply": "2023-05-21T05:32:37.374432Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionSeqence_2_Seqence_Network(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder  # Initialize the encoder\n",
        "        self.decoder = decoder  # Initialize the decoder\n",
        "        self.device = device  # Store the device (e.g., CPU or GPU)\n",
        "        self.max_target_length = 0  # Variable to store the maximum target sequence length\n",
        "        self.sos = 0  # Start-of-sequence token\n",
        "               \n",
        "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "        batch_size = target.shape[0]  # Get the batch size\n",
        "        target_len = target.shape[1]  # Get the target sequence length\n",
        "        self.max_target_length = target_len  # Update the maximum target sequence length\n",
        "        target_vocab_size = self.decoder.output_size  # Get the target vocabulary size\n",
        "        \n",
        "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)  # Initialize the outputs tensor\n",
        "\n",
        "        encoder_hidden, encoder_cell = self.encoder.init_hidden(batch_size)  # Initialize the encoder hidden state and cell state\n",
        "\n",
        "        if (self.encoder.cell_type == 'lstm'):\n",
        "            encoder_outputs, encoder_hidden, encoder_cell = self.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "            # Forward pass through the encoder LSTM\n",
        "        if (self.encoder.cell_type == 'rnn'):\n",
        "            encoder_outputs, encoder_hidden = self.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "            # Forward pass through the encoder RNN\n",
        "        if (self.encoder.cell_type == 'gru'):\n",
        "            encoder_outputs, encoder_hidden = self.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "            # Forward pass through the encoder GRU\n",
        "\n",
        "        input = target[:, 0]  # Set the first input to the decoder as the <sos> token\n",
        "        self.sos = target[:, 0]  # Store the <sos> token\n",
        "        hidden, cell = self.decoder.init_hidden(encoder_hidden, encoder_cell, self.encoder.bidirectional)\n",
        "        # Initialize the decoder hidden state and cell state\n",
        "        \n",
        "        for t in range(1, target_len):\n",
        "            output, hidden, cell, _ = self.decoder.forward(input, encoder_outputs, hidden, cell)\n",
        "            # Forward pass through the decoder with attention\n",
        "            outputs[:, t] = output.squeeze(1)  # Store the decoder output in the outputs tensor\n",
        "            teacher_force = random.random() < teacher_forcing_ratio  # Determine whether to use teacher forcing\n",
        "            top1 = output.argmax(-1)  # Get the index of the highest probability output\n",
        "            input = target[:, t] if teacher_force else top1.squeeze(1)  # Set the next input to the decoder\n",
        "            \n",
        "        return outputs\n",
        "\n",
        "    def inference(self, source, target):\n",
        "        batch_size = source.shape[0]  # Get the batch size\n",
        "        target_len = self.max_target_length  # Get the maximum target sequence length\n",
        "        target_vocab_size = self.decoder.output_size  # Get the target vocabulary size\n",
        "        \n",
        "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)  # Initialize the outputs tensor\n",
        "\n",
        "        encoder_hidden, encoder_cell = self.encoder.init_hidden(batch_size)  # Initialize the encoder hidden state and cell state\n",
        "\n",
        "        if (self.encoder.cell_type == 'lstm'):\n",
        "            encoder_outputs, encoder_hidden, encoder_cell = self.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "            # Forward pass through the encoder LSTM\n",
        "        if (self.encoder.cell_type == 'rnn'):\n",
        "            encoder_outputs, encoder_hidden = self.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "            # Forward pass through the encoder RNN\n",
        "        if (self.encoder.cell_type == 'gru'):\n",
        "            encoder_outputs, encoder_hidden = self.encoder.forward(source, encoder_hidden, encoder_cell)\n",
        "            # Forward pass through the encoder GRU\n",
        "\n",
        "        input = self.sos  # Set the first input to the <sos> token\n",
        "        input_len = encoder_outputs.shape[1]  # Get the length of the encoder outputs\n",
        "        hidden, cell = self.decoder.init_hidden(encoder_hidden, encoder_cell, self.encoder.bidirectional)\n",
        "        attention_map = torch.zeros(batch_size, target_len, input_len)  # Initialize the attention map\n",
        "        \n",
        "        for t in range(1, target_len):\n",
        "            output, hidden, cell, attention = self.decoder.forward(input, encoder_outputs, hidden, cell)\n",
        "            # Forward pass through the decoder with attention\n",
        "            attention_map[:, t - 1, :] = attention.squeeze(1)  # Store the attention weights in the attention map\n",
        "            outputs[:, t] = output.squeeze(1)  # Store the decoder output in the outputs tensor\n",
        "            top1 = output.argmax(-1)  # Get the index of the highest probability output\n",
        "            input = top1.squeeze(1)  # Set the next input to the decoder\n",
        "            \n",
        "        return outputs, attention_map\n"
      ],
      "metadata": {
        "id": "JinZtTpR4q52",
        "execution": {
          "iopub.status.busy": "2023-05-21T05:32:40.224719Z",
          "iopub.execute_input": "2023-05-21T05:32:40.225123Z",
          "iopub.status.idle": "2023-05-21T05:32:40.249703Z",
          "shell.execute_reply.started": "2023-05-21T05:32:40.225088Z",
          "shell.execute_reply": "2023-05-21T05:32:40.248541Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def AttentionModel():\n",
        "    I = len(input_dict)  # Input dimension\n",
        "    O = len(target_dict)  # Output dimension\n",
        "    B = 32  # Batch size for training\n",
        "    V = 32  # Batch size for validation\n",
        "    E_e = 256  # Encoder embedding dimension\n",
        "    E_d = 256  # Decoder embedding dimension\n",
        "    H = 512  # Hidden dimension\n",
        "    L_e = 1  # Number of layers in the encoder\n",
        "    L_d = 1  # Number of layers in the decoder\n",
        "    D_e = 0.25  # Dropout probability for the encoder\n",
        "    D_d = 0.25  # Dropout probability for the decoder\n",
        "    M = max_target_length  # Maximum target length\n",
        "    C = 'lstm'  # Type of cell used in the encoder and decoder\n",
        "    T = 0.7  # Teacher forcing ratio\n",
        "\n",
        "    enc = Recurrent_NN_Encoder(device, C, I, E_e, H, L_e, bidirectional=True, dropout_p=D_e)  # Initialize the encoder\n",
        "    dec = AttentionRecurrent_NN_Decoder(device, C, O, E_d, H, M, D_d, L_d, bidirectional=True)  # Initialize the decoder\n",
        "    model = AttentionSeqence_2_Seqence_Network(enc, dec, device).to(device)  # Initialize the model\n",
        "    optimizer = torch.optim.NAdam(model.parameters(), lr=0.001)  # Initialize the optimizer\n",
        "    criterion = nn.NLLLoss()  # Initialize the criterion for loss calculation\n",
        "    wandb.init(project=\"CS6910 Assignment 3\")  # Initialize the Weights and Biases project\n",
        "    train_function(model, pairs, B, 1, optimizer, T, A=True)  # Train the model\n",
        "\n",
        "# def train_function(model, pairs, batch_size, n_iters, optimizer, tf, print_every=10, plot_every=10, log=True, Attention=False)\n",
        "AttentionModel()"
      ],
      "metadata": {
        "id": "dImmn7d3hIEN",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'bayes',  # Specifies the search method for hyperparameter optimization\n",
        "    'name': 'Comparing a model with and without Attention-Bayesian'  # Name of the sweep\n",
        "}\n",
        "\n",
        "sweep_config['metric'] = {\n",
        "    'name': 'Val_Accuracy',  # Name of the metric to be optimized\n",
        "    'goal': 'maximize'  # Goal of the optimization (maximize or minimize)\n",
        "}\n",
        "\n",
        "sweep_config['parameters'] = {\n",
        "    'optimiser': {'values': ['nadam','adam']},  # Hyperparameter: optimizer\n",
        "    'teacher_forcing_ratio': {'values': [0.3,0.5,0.7]},  # Hyperparameter: teacher forcing ratio\n",
        "    'bidirectional': {'values': [True, False]},  # Hyperparameter: bidirectional\n",
        "    'enc_embedding': {'values': [128, 256]},  # Hyperparameter: encoder embedding size\n",
        "    'dec_embedding': {'values': [128, 256]},  # Hyperparameter: decoder embedding size\n",
        "    'epochs': {'values': [1]},  # Hyperparameter: number of epochs\n",
        "    'hidden_size': {'values': [64,128,256,512]},  # Hyperparameter: hidden size\n",
        "    'enc_layers': {'values': [2,3]},  # Hyperparameter: number of encoder layers\n",
        "    'dec_layers': {'values': [2,3]},  # Hyperparameter: number of decoder layers\n",
        "    'dropout': {'values': [0.25,0.3, 0.4]},  # Hyperparameter: dropout rate\n",
        "    'cell_type': {'values': ['gru', 'lstm','rnn']},  # Hyperparameter: type of RNN cell\n",
        "    'Attention': {'values': [True]}  # Hyperparameter: attention flag\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"Attention Assignment 3\")  # Creates a sweep with the given configuration\n",
        "\n",
        "def train_sweep(config=None):\n",
        "    with wandb.init(config=config) as run:\n",
        "        c = wandb.config  # Retrieves the configuration for the current run\n",
        "        i = len(input_dict)  # Length of input dictionary\n",
        "        o = len(target_dict)  # Length of target dictionary\n",
        "        b = 32  # Batch size\n",
        "        v = 32  # Vocabulary size\n",
        "        e = c.enc_embedding  # Encoder embedding size\n",
        "        d = c.dec_embedding  # Decoder embedding size\n",
        "        h = c.hidden_size  # Hidden size\n",
        "        en = c.enc_layers  # Number of encoder layers\n",
        "        de = c.dec_layers  # Number of decoder layers\n",
        "        ed = c.dropout  # Encoder dropout rate\n",
        "        dd = c.dropout  # Decoder dropout rate\n",
        "        l = max_target_length  # Maximum target sequence length\n",
        "        t = c.cell_type  # RNN cell type\n",
        "\n",
        "        enc = Recurrent_NN_Encoder(device, t, i, e, h, en, bidirectional=c.bidirectional, dropout_p=ed)  # Create an Recurrent_NN_Encoder instance\n",
        "        if c.Attention:\n",
        "            dec = AttentionRecurrent_NN_Decoder(device, t, o, d, h, l, dd, de, bidirectional=c.bidirectional)  # Create an AttentionRecurrent_NN_Decoder instance\n",
        "            model = AttentionSeqence_2_Seqence_Network(enc, dec, device).to(device)  # Create an AttentionSeqence_2_Seqence_Network model\n",
        "        else:\n",
        "            dec = Recurrent_NN_Decoder(device, t, o, d, h, l, dd, de, bidirectional=c.bidirectional)  # Create a Recurrent_NN_Decoder instance\n",
        "            model = Seqence_2_Seqence_Network(enc, dec, device).to(device)  # Create a Seqence_2_Seqence_Network model\n",
        "\n",
        "        m = '_bi_' if c.bidirectional else '_uni_'  # Model type indicator (bidirectional or unidirectional)\n",
        "        a = '_Attention' if c.Attention else '_'  # Attention indicator\n",
        "\n",
        "        n = f\"{c.cell_type}{m}e_{c.epochs}_optim_{c.optimiser}{a}\"\n",
        "\n",
        "        wandb.run.name = n  # Set the name of the run in WandB\n",
        "\n",
        "        if c.optimiser == 'adam':\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Create an Adam optimizer\n",
        "        elif c.optimiser == 'nadam':\n",
        "            optimizer = torch.optim.NAdam(model.parameters(), lr=0.001)  # Create a Nadam optimizer\n",
        "\n",
        "        criterion = nn.NLLLoss()  # Define the loss criterion\n",
        "        train_function(model, pairs, 32, c.epochs, optimizer, c.teacher_forcing_ratio, A=c.Attention)  # Train the model\n",
        "# def train_function(model, pairs, batch_size, n_iters, optimizer, tf, print_every=10, plot_every=10, log = True, Attention = False):\n",
        "wandb.agent(sweep_id, train_sweep, count=1)  # Run the sweep using the train_sweep function\n",
        "wandb.finish()  # Finish the WandB run\n"
      ],
      "metadata": {
        "id": "80QsG0s3x_UN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def BestAttebtionModel():\n",
        "    i = len(input_dict)  # Input dimension\n",
        "    o = len(target_dict)  # Output dimension\n",
        "    b = v = 32  # Batch sizes\n",
        "    e = 128  # Embedding dimensions\n",
        "    h = 256  # Hidden layer size\n",
        "    l = 128  # Number of layers\n",
        "    d = 0.25  # Dropout ratios\n",
        "    m = max_target_length  # Maximum target length\n",
        "    c = 'gru'  # Cell type\n",
        "    t = 0.7  # Teacher forcing ratio\n",
        "    \n",
        "    enc = Recurrent_NN_Encoder(device, c, i, e, h, l, bidirectional=True, dropout_p=d)  # Initialize encoder\n",
        "    dec = AttentionRecurrent_NN_Decoder(device, c, o, e, h, m, d, l, bidirectional=True)  # Initialize decoder\n",
        "    model = AttentionSeqence_2_Seqence_Network(enc, dec, device).to(device)  # Initialize model\n",
        "    optimizer = torch.optim.NAdam(model.parameters(), lr=0.001)  # Initialize optimizer\n",
        "    criterion = nn.NLLLoss()  # Initialize criterion\n",
        "    \n",
        "    wandb.init(project=\"Attention Assignment 3\")  # Initialize wandb\n",
        "    \n",
        "    train_function(model, pairs, b, 1, optimizer, t, A=True)  # Train the model\n",
        "BestAttebtionModel()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-21T06:11:07.224794Z",
          "iopub.execute_input": "2023-05-21T06:11:07.225465Z",
          "iopub.status.idle": "2023-05-21T06:19:40.662833Z",
          "shell.execute_reply.started": "2023-05-21T06:11:07.225422Z",
          "shell.execute_reply": "2023-05-21T06:19:40.661701Z"
        },
        "trusted": true,
        "id": "I6whgJJ-rBMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class CSVAppender:\n",
        "    def __init__(self, input_dict, target_dict, max_target_length):\n",
        "        # Variable assignments with single letters\n",
        "        self.i = len(input_dict)  # input_dim\n",
        "        self.o = len(target_dict)  # output_dim\n",
        "        self.b = 32  # batch_size\n",
        "        self.v = 32  # val_batch_size\n",
        "        self.e1 = 128  # enc_embedding\n",
        "        self.e2 = 128  # dec_embedding\n",
        "        self.h = 256  # hidden\n",
        "        self.el = 2  # enc_num_layers\n",
        "        self.dl = 2  # dec_num_layers\n",
        "        self.edo = 0.3  # enc_dropout\n",
        "        self.ddo = 0.3  # dec_dropout\n",
        "        self.m = max_target_length  # max_length\n",
        "        self.ct = 'gru'  # cell_type\n",
        "        self.tfr = 0.7  # teacher_forcing_ratio\n",
        "\n",
        "        # Model initialization\n",
        "        self.enc = Recurrent_NN_Encoder(device, self.ct, self.i, self.e1, self.h, self.el, bidirectional=True, dropout_p=self.edo)\n",
        "        self.dec = AttentionRecurrent_NN_Decoder(device, self.ct, self.o, self.e2, self.h, self.m, self.ddo, self.dl, bidirectional=True)\n",
        "        self.model = AttentionSeqence_2_Seqence_Network(self.enc, self.dec, device).to(device)\n",
        "        self.optimizer = torch.optim.NAdam(self.model.parameters(), lr=0.001)\n",
        "        self.criterion = nn.NLLLoss()\n",
        "\n",
        "        # Initialize wandb (assuming the wandb library is imported and configured properly)\n",
        "        wandb.init(project=\"Attention Assignment 3\")\n",
        "\n",
        "    def train(self, pairs):\n",
        "        # Training iteration\n",
        "        train_function(self.model, pairs, self.b, 1, self.optimizer, self.tfr, A=True)\n",
        "\n",
        "    def append_strings_to_csv(self, file_path, string1, string2, string3):\n",
        "        with open(file_path, 'a', newline='') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerow([string1, string2, string3])\n",
        "\n",
        "    def test(self, pairs):\n",
        "        # pairs[2]\n",
        "        tp = pairs[2]\n",
        "        # test_word_count\n",
        "        twc = 0\n",
        "        # batch_size\n",
        "        bs = 32\n",
        "        # count\n",
        "        c = 10\n",
        "        # name\n",
        "        n = 'mar'\n",
        "        # file_path\n",
        "        fp = '/content/drive/MyDrive/aksharantar_sampled/' + n + '.csv'\n",
        "\n",
        "        for i in np.arange(start=0, stop=len(tp)-bs, step=bs):\n",
        "            if (i + bs > len(tp)):\n",
        "                bs = len(tp) - i + 1\n",
        "            test_input_tensor = []\n",
        "            test_target_tensor = []\n",
        "            for j in range(bs):\n",
        "                test_input_tensor.append(tp[i+j][0])\n",
        "                test_target_tensor.append(tp[i+j][1])\n",
        "\n",
        "            test_input_tensor = torch.stack(test_input_tensor).squeeze(1).long().cuda()\n",
        "            test_target_tensor = torch.stack(test_target_tensor).squeeze(1).long().cuda()\n",
        "\n",
        "            test_out, attention_map = self.model.inference(test_input_tensor, test_target_tensor)\n",
        "            test_out = torch.permute(test_out, [0, 2, 1])\n",
        "            test_loss = self.criterion(test_out, test_target_tensor)\n",
        "            for j in range(bs):\n",
        "                input_str = key_printing(input_dict, test_input_tensor[j])\n",
        "                output_str = key_printing(target_dict, torch.argmax(test_out[j], dim=0))\n",
        "                target_str = key_printing(target_dict, test_target_tensor[j])\n",
        "                self.append_strings_to_csv(fp, input_str, target_str, output_str)\n",
        "\n",
        "            test_accuracy_word = word_acc(test_target_tensor, test_out)\n",
        "\n",
        "            twc = twc + (test_accuracy_word)*bs\n",
        "\n",
        "        test_word_accuracy = twc / (len(tp)-bs)\n",
        "        print(f\"Accuracy on Test Set is {test_word_accuracy}\")\n",
        "        print(attention_map.shape)\n",
        "\n",
        "\n",
        "model = CSVAppender(input_dict, target_dict, max_target_length)\n",
        "model.train(pairs)\n",
        "model.test(pairs)\n"
      ],
      "metadata": {
        "id": "hCaP3lV1lSf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "\n",
        "# Import required libraries\n",
        "\n",
        "import csv\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Variable assignments with single letters\n",
        "i = len(input_dict)  # input_dim\n",
        "o = len(target_dict)  # output_dim\n",
        "b = 32  # batch_size\n",
        "v = 32  # val_batch_size\n",
        "e1 = 128  # enc_embedding\n",
        "e2 = 128  # dec_embedding\n",
        "h = 512  # hidden\n",
        "el = 2  # enc_num_layers\n",
        "dl = 2  # dec_num_layers\n",
        "edo = 0.3  # enc_dropout\n",
        "ddo = 0.3  # dec_dropout\n",
        "m = max_target_length  # max_length\n",
        "ct = 'gru'  # cell_type\n",
        "tfr = 0.7  # teacher_forcing_ratio\n",
        "\n",
        "# Model initialization\n",
        "enc = Recurrent_NN_Encoder(device, ct, i, e1, h, el, bidirectional=True, dropout_p=edo)  # Initialize the encoder\n",
        "dec = AttentionRecurrent_NN_Decoder(device, ct, o, e2, h, m, ddo, dl, bidirectional=True)  # Initialize the decoder\n",
        "model = AttentionSeqence_2_Seqence_Network(enc, dec, device).to(device)  # Initialize the sequence-to-sequence model\n",
        "optimizer = torch.optim.NAdam(model.parameters(), lr=0.001)  # Initialize the optimizer\n",
        "criterion = nn.NLLLoss()  # Initialize the loss function\n",
        "\n",
        "# Initialize wandb (assuming the wandb library is imported and configured properly)\n",
        "wandb.init(project=\"Attention-Heat-Maps\")\n",
        "\n",
        "# Training iteration\n",
        "train_function(model, pairs, b, 1, optimizer, tfr, A=True)  # Perform training iterations\n",
        "\n",
        "class SingleClass:\n",
        "    def __init__(self):\n",
        "        self.test_pairs = pairs[2]  # Set the test pairs\n",
        "        self.test_word_count = 0  # Initialize the word count\n",
        "        self.count = 10  # Set the count\n",
        "        self.name = 'mar1'  # Set the name\n",
        "        self.file_path = '/content/drive/MyDrive/aksharantar_sampled/' + self.name + '.csv'  # Set the file path\n",
        "    \n",
        "    def append_strings_to_csv(self, fp, s1, s2, s3):  # Method to append strings to a CSV file\n",
        "        with open(fp, 'a', newline='') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerow([s1, s2, s3])\n",
        "    \n",
        "    def find_index(self, tensor, req_num):  # Method to find the index of a required number in a tensor\n",
        "        indices = np.where(tensor.cpu() == req_num)\n",
        "        if indices[0].size > 0:\n",
        "            return indices[0]+1\n",
        "        else:\n",
        "            return len(tensor)\n",
        "    \n",
        "    def pad_idx(self, inp_seq, target_seq, inp_dict, target_dict):  # Method to find the padding index\n",
        "        inp_pad = inp_dict[' ']\n",
        "        target_pad = target_dict[' ']\n",
        "        pad_idx_inp = self.find_index(inp_seq, inp_pad)\n",
        "        pad_idx_target = self.find_index(target_seq, target_pad)\n",
        "        return pad_idx_inp[0], pad_idx_target[0]\n",
        "    \n",
        "    def run(self):  # Method to run the test\n",
        "        batch_size = 32  # Set the batch size\n",
        "        for i in np.arange(start=0, stop=len(self.test_pairs)-batch_size, step=batch_size):\n",
        "            if (i + batch_size > len(self.test_pairs)):\n",
        "                batch_size = len(self.test_pairs) - i + 1\n",
        "            test_input_tensor = []\n",
        "            test_target_tensor = []\n",
        "            for j in range(batch_size):\n",
        "                test_input_tensor.append(self.test_pairs[i+j][0])\n",
        "                test_target_tensor.append(self.test_pairs[i+j][1])\n",
        "  \n",
        "            test_input_tensor = torch.stack(test_input_tensor).squeeze(1).long().cuda()  # Convert input to tensor and move to GPU\n",
        "            test_target_tensor = torch.stack(test_target_tensor).squeeze(1).long().cuda()  # Convert target to tensor and move to GPU\n",
        "  \n",
        "            test_out, attention_map = model.inference(test_input_tensor, test_target_tensor)  # Perform inference\n",
        "            test_out = torch.permute(test_out, [0, 2, 1])  # Permute the dimensions of test_out tensor\n",
        "            test_loss = criterion(test_out, test_target_tensor)  # Calculate the loss\n",
        "            inp_dict = input_dict\n",
        "            for j in range(batch_size):\n",
        "                inp_str = key_printing(inp_dict, test_input_tensor[j])  # Convert input tensor to string\n",
        "                out_str = key_printing(target_dict, torch.argmax(test_out[j], dim=0))  # Convert output tensor to string\n",
        "                target_str = key_printing(target_dict, test_target_tensor[j])  # Convert target tensor to string\n",
        "            test_accuracy_word = word_acc(test_target_tensor, test_out)  # Calculate word-level accuracy\n",
        "            self.test_word_count = self.test_word_count + (test_accuracy_word) * batch_size\n",
        "  \n",
        "        test_word_accuracy = self.test_word_count / (len(self.test_pairs) - batch_size)  # Calculate test word accuracy\n",
        "        print(f\"Accuracy on Test Set {test_word_accuracy}\")  # Print test word accuracy\n",
        "        print(attention_map.shape)  # Print the shape of attention map\n",
        "  \n",
        "        # print(inp_dict)  # Print the input dictionary\n",
        "        # print(target_dict)  # Print the target dictionary\n",
        "  \n",
        "        wandb.init(project=\"CS6910 Assignment 3\")  # Initialize wandb for logging\n",
        "        for i in range(10):\n",
        "            inp_seq = test_input_tensor[i+10]\n",
        "            target_seq = test_target_tensor[i+10]\n",
        "            attention_seq = attention_map[i+10]\n",
        "            pad_idx_inp, pad_idx_target = self.pad_idx(inp_seq, target_seq, inp_dict, target_dict)\n",
        "            print(pad_idx_inp, pad_idx_target)\n",
        "            attention_seq_sub = attention_seq[0:int(pad_idx_inp), 0:int(pad_idx_target)]\n",
        "            fig = plt.imshow(attention_seq_sub.detach().numpy())\n",
        "            fig = fig.get_figure()\n",
        "            row_labels = key_printing(inp_dict, inp_seq[0:pad_idx_inp])\n",
        "            column_labels = key_printing(target_dict, target_seq[0:pad_idx_target])\n",
        "            plt.xticks(ticks=np.arange(len(column_labels)), labels=column_labels)\n",
        "            plt.yticks(ticks=np.arange(len(row_labels)), labels=row_labels)\n",
        "            wandb.log({'Attention Map': fig})\n",
        "            plt.show()\n",
        "\n",
        "# Create an instance of the SingleClass\n",
        "sc = SingleClass()\n",
        "\n",
        "# Call the run method to execute the code\n",
        "sc.run()\n"
      ],
      "metadata": {
        "id": "t53fo3QHth7s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}