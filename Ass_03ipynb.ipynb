{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9BJf+nlDWtavDplRSOsqR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swapnilmn/Assignment_03/blob/main/Ass_03ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yes | wget \"https://drive.google.com/file/d/1uRKU4as2NlS9i8sdLRS1e326vQRdhvfw/view?usp=sharing\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEAQdHUvh-pW",
        "outputId": "6f3fe100-5cea-4ebd-b398-ae594b85807a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-17 11:49:35--  https://drive.google.com/file/d/1uRKU4as2NlS9i8sdLRS1e326vQRdhvfw/view?usp=sharing\n",
            "Resolving drive.google.com (drive.google.com)... 173.194.194.102, 173.194.194.113, 173.194.194.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|173.194.194.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘view?usp=sharing’\n",
            "\n",
            "view?usp=sharing        [ <=>                ]  73.85K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-05-17 11:49:35 (55.7 MB/s) - ‘view?usp=sharing’ saved [75622]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyxtGz_Nib8r",
        "outputId": "9de47f95-bb2e-4684-8093-8309bb741c04"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Drafts"
      ],
      "metadata": {
        "id": "ltdb3EjSx2SL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_data(path):\n",
        "  \"\"\"Loads the data from the specified path.\n",
        "\n",
        "  Args:\n",
        "    path: The path to the data file.\n",
        "\n",
        "  Returns:\n",
        "    A Pandas DataFrame containing the data.\n",
        "  \"\"\"\n",
        "\n",
        "  with open(path) as fil:\n",
        "    df = pd.read_csv(fil, sep=',', header=None, names=['en', 'ma', ''], skip_blank_lines=True, index_col=None)\n",
        "\n",
        "  df = df[df['en'].notna()]\n",
        "  df = df[df['ma'].notna()]\n",
        "  df = df[['en', 'ma']]\n",
        "\n",
        "  return df\n",
        "\n",
        "train = load_data(\"/content/drive/MyDrive/aksharantar_sampled/aksharantar_sampled/mar/mar_train.csv\")\n",
        "val = load_data(\"/content/drive/MyDrive/aksharantar_sampled/aksharantar_sampled/mar/mar_valid.csv\")\n",
        "test = load_data(\"/content/drive/MyDrive/aksharantar_sampled/aksharantar_sampled/mar/mar_test.csv\")"
      ],
      "metadata": {
        "id": "lRM7w6BjgjP5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unique_tokenize(data):\n",
        "    english_tokens = set()\n",
        "    marathi_tokens = set()\n",
        "\n",
        "    for english, marathi in list(zip(data['en'].values, data['ma'].values)):\n",
        "        for ch in english:\n",
        "            english_tokens.add(ch)\n",
        "        for ch in marathi:\n",
        "            marathi_tokens.add(ch)\n",
        "\n",
        "    english_tokens = sorted(list(english_tokens))\n",
        "    marathi_tokens = sorted(list(marathi_tokens))\n",
        "\n",
        "    return marathi_tokens, english_tokens\n",
        "\n",
        "marathi_tokens , english_tokens = unique_tokenize(train)\n",
        "print(english_tokens)\n",
        "print(len(marathi_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCKBqgDxhP9L",
        "outputId": "60558c4a-2813-430d-e709-7582c7262963"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_map(data):\n",
        "    e_vocab = set()\n",
        "    m_vocab = set()\n",
        "\n",
        "    for e, m in zip(data['en'].values, data['ma'].values):\n",
        "        for ch in e:\n",
        "            e_vocab.add(ch)\n",
        "        for ch in m:\n",
        "            m_vocab.add(ch)\n",
        "\n",
        "    e_vocab = sorted(list(e_vocab))\n",
        "    m_vocab = sorted(list(m_vocab))\n",
        "\n",
        "    e_to_i = {ch: i + 1 for i, ch in enumerate(e_vocab)}\n",
        "    m_to_i = {ch: i + 1 for i, ch in enumerate(m_vocab)}\n",
        "\n",
        "    m_to_i[' '] = 0\n",
        "    e_to_i[' '] = 0\n",
        "\n",
        "    m_to_i[';'] = 65\n",
        "    m_to_i['.'] = 66\n",
        "    e_to_i[';'] = 27\n",
        "    e_to_i['.'] = 28\n",
        "\n",
        "    m_to_i['<unk>'] = 64\n",
        "\n",
        "    return e_to_i, m_to_i\n",
        "\n",
        "mar_token_map, eng_token_map = tokenize_map(train)\n",
        "print(mar_token_map)\n",
        "print(eng_token_map)\n",
        "print('mar:',len(mar_token_map))\n",
        "print('eng:',len(eng_token_map))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9QV1vwejGPy",
        "outputId": "6ff15c9b-359d-4aa5-9472-2fe39cc6833c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, ' ': 0, ';': 27, '.': 28}\n",
            "{'ँ': 1, 'ं': 2, 'ः': 3, 'अ': 4, 'आ': 5, 'इ': 6, 'ई': 7, 'उ': 8, 'ऊ': 9, 'ऋ': 10, 'ऍ': 11, 'ए': 12, 'ऐ': 13, 'ऑ': 14, 'ओ': 15, 'औ': 16, 'क': 17, 'ख': 18, 'ग': 19, 'घ': 20, 'च': 21, 'छ': 22, 'ज': 23, 'झ': 24, 'ञ': 25, 'ट': 26, 'ठ': 27, 'ड': 28, 'ढ': 29, 'ण': 30, 'त': 31, 'थ': 32, 'द': 33, 'ध': 34, 'न': 35, 'प': 36, 'फ': 37, 'ब': 38, 'भ': 39, 'म': 40, 'य': 41, 'र': 42, 'ल': 43, 'ळ': 44, 'व': 45, 'श': 46, 'ष': 47, 'स': 48, 'ह': 49, '़': 50, 'ा': 51, 'ि': 52, 'ी': 53, 'ु': 54, 'ू': 55, 'ृ': 56, 'ॅ': 57, 'े': 58, 'ै': 59, 'ॉ': 60, 'ो': 61, 'ौ': 62, '्': 63, ' ': 0, ';': 65, '.': 66, '<unk>': 64}\n",
            "mar: 29\n",
            "eng: 67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x = test['en'].values\n",
        "y = test['ma'].values\n",
        "x=';'+ x + '.'\n",
        "y = ';'+y+'.'\n",
        "#Getting max length\n",
        "max_eng_len = max([len(i) for i in x])\n",
        "max_mar_len = max([len(i) for i in y])\n",
        "print(max_eng_len,max_mar_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihYCmfmokKV3",
        "outputId": "2e823b34-0e09-45e6-8f41-283061c0a50f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dont change varibles, write code in other template for same task \n",
        "chnage local variables\n",
        "\n",
        "\n",
        "def load_data(path):\n",
        "    with open(path) as fil:\n",
        "        data = pd.read_csv(fil, sep=',', header=None, names=[\"en\", \"ma\", \"\"], skip_blank_lines=True, index_col=None)\n",
        "    data = data[data['en'].notna()]\n",
        "    data = data[data['ma'].notna()]\n",
        "    data = data[['en', 'ma']]\n",
        "    return data\n",
        "\n",
        "train = load_data(\"/content/drive/MyDrive/aksharantar_sampled/aksharantar_sampled/mar/mar_train.csv\")\n",
        "val = load_data(\"/content/drive/MyDrive/aksharantar_sampled/aksharantar_sampled/mar/mar_valid.csv\")\n",
        "test = load_data(\"/content/drive/MyDrive/aksharantar_sampled/aksharantar_sampled/mar/mar_test.csv\")\n",
        "\n",
        "def unique_tokenize(data):\n",
        "    english = data['en'].values\n",
        "    marathi = data['ma'].values\n",
        "    english_tokens = set()\n",
        "    marathi_tokens = set()\n",
        "\n",
        "    for x, y in zip(english, marathi):\n",
        "        for ch in x:\n",
        "            english_tokens.add(ch)\n",
        "        for ch in y:\n",
        "            marathi_tokens.add(ch)\n",
        "    english_tokens = sorted(list(english_tokens))\n",
        "    marathi_tokens = sorted(list(marathi_tokens))\n",
        "    return marathi_tokens, english_tokens\n",
        "\n",
        "marathi_tokens, english_tokens = unique_tokenize(train)\n",
        "\n",
        "def tokenize_map(marathi_tokens, english_tokens):\n",
        "    marathi_token_map = dict([(ch, i + 1) for i, ch in enumerate(marathi_tokens)])\n",
        "    english_token_map = dict([(ch, i + 1) for i, ch in enumerate(english_tokens)])\n",
        "    marathi_token_map[\" \"] = 0\n",
        "    english_token_map[\" \"] = 0\n",
        "    marathi_token_map[';'] = 65\n",
        "    marathi_token_map['.'] = 66\n",
        "    english_token_map[';'] = 27\n",
        "    english_token_map['.'] = 28\n",
        "    marathi_token_map['<unk>'] = 64\n",
        "\n",
        "    return marathi_token_map, english_token_map\n",
        "\n",
        "mar_token_map, eng_token_map = tokenize_map(marathi_tokens, english_tokens)\n",
        "\n",
        "unknown_token = 64\n",
        "\n",
        "x = test['en'].values\n",
        "y = test['ma'].values\n",
        "x=';'+ x + '.'\n",
        "y = ';'+y+'.'\n",
        "#Getting max length\n",
        "max_eng_len = max([len(i) for i in x])\n",
        "max_mar_len = max([len(i) for i in y])\n",
        "print(max_eng_len,max_mar_len)\n",
        "\n",
        "def process(data):\n",
        "    x, y = data['en'].values, data['ma'].values\n",
        "    x = \";\" + x + \".\"\n",
        "    y = \";\" + y + \".\"\n",
        "    print(x[0:3])\n",
        "    print(y[0:3]) \n",
        "\n",
        "    a = torch.zeros((len(x), max_eng_len), dtype=torch.int64)\n",
        "    b = torch.zeros((len(y), max_eng_len), dtype=torch.int64)\n",
        "    \n",
        "    data = []\n",
        "    for i, (xx, yy) in enumerate(zip(x, y)):\n",
        "        for j, ch in enumerate(xx):\n",
        "            a[i, j] = eng_token_map[ch]\n",
        "        \n",
        "        for j, ch in enumerate(yy):\n",
        "            if ch in mar_token_map: \n",
        "                b[i, j] = mar_token_map[ch]\n",
        "            else:\n",
        "                b[i, j] = unknown_token\n",
        "        \n",
        "        data.append((a[i], b[i]))\n",
        "    data = [(a[i], b[i]) for i in range(len(x))]\n",
        "    print(a.shape)\n",
        "    print(b.shape)\n",
        "    return data\n",
        "\n",
        "train_process = process(train)\n",
        "val_process = process(val)\n",
        "test_process = process(test)\n",
        "\n",
        "print('\\n')\n",
        "print('num of rows:', len(train_process))\n",
        "print('num of columns:', len(train_process[0]))\n",
        "print(train_process[0][1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPRNJHh7o4RP",
        "outputId": "a931f233-95f8-4633-e413-6821c06a90d7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30 22\n",
            "[';fusharun.' ';bhulthapana.' ';vhayaki.']\n",
            "[';फुशारुन.' ';भूलथापाना.' ';व्हायकी.']\n",
            "torch.Size([51200, 30])\n",
            "torch.Size([51200, 30])\n",
            "[';garvyabarobarach.' ';reo.' ';sangrahalaye.']\n",
            "[';गारव्याबरोबरच.' ';रियो.' ';संग्रहालये.']\n",
            "torch.Size([4096, 30])\n",
            "torch.Size([4096, 30])\n",
            "[';heetler.' ';kshama.' ';jinkta.']\n",
            "[';हिटलर.' ';क्षमा.' ';जिंकता.']\n",
            "torch.Size([4096, 30])\n",
            "torch.Size([4096, 30])\n",
            "\n",
            "\n",
            "num of rows: 51200\n",
            "num of columns: 2\n",
            "tensor([65, 37, 54, 46, 51, 42, 54, 35, 66,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "def load_data(path):\n",
        "    with open(path) as fil:\n",
        "        data = pd.read_csv(fil, sep=',', header=None, names=[\"en\", \"ma\", \"\"], skip_blank_lines=True, index_col=None)\n",
        "    data = data[data['en'].notna()]\n",
        "    data = data[data['ma'].notna()]\n",
        "    data = data[['en', 'ma']]\n",
        "    return data\n",
        "\n",
        "train_data = load_data(\"/content/drive/MyDrive/aksharantar_sampled/aksharantar_sampled/mar/mar_train.csv\")\n",
        "val_data = load_data(\"/content/drive/MyDrive/aksharantar_sampled/aksharantar_sampled/mar/mar_valid.csv\")\n",
        "test_data = load_data(\"/content/drive/MyDrive/aksharantar_sampled/aksharantar_sampled/mar/mar_test.csv\")\n",
        "\n",
        "def unique_tokenize(data):\n",
        "    english = data['en'].values\n",
        "    marathi = data['ma'].values\n",
        "    english_tokens = set()\n",
        "    marathi_tokens = set()\n",
        "\n",
        "    for x, y in zip(english, marathi):\n",
        "        for ch in x:\n",
        "            english_tokens.add(ch)\n",
        "        for ch in y:\n",
        "            marathi_tokens.add(ch)\n",
        "    english_tokens = sorted(list(english_tokens))\n",
        "    marathi_tokens = sorted(list(marathi_tokens))\n",
        "    return marathi_tokens, english_tokens\n",
        "\n",
        "marathi_tokens, english_tokens = unique_tokenize(train_data)\n",
        "\n",
        "def tokenize_map(marathi_tokens, english_tokens):\n",
        "    marathi_token_map = dict([(ch, i + 1) for i, ch in enumerate(marathi_tokens)])\n",
        "    english_token_map = dict([(ch, i + 1) for i, ch in enumerate(english_tokens)])\n",
        "    marathi_token_map[\" \"] = 0\n",
        "    english_token_map[\" \"] = 0\n",
        "    marathi_token_map[';'] = 65\n",
        "    marathi_token_map['.'] = 66\n",
        "    english_token_map[';'] = 27\n",
        "    english_token_map['.'] = 28\n",
        "    marathi_token_map['<unk>'] = 64\n",
        "\n",
        "    return marathi_token_map, english_token_map\n",
        "\n",
        "marathi_token_map, english_token_map = tokenize_map(marathi_tokens, english_tokens)\n",
        "\n",
        "unknown_token = 64\n",
        "\n",
        "x = test_data['en'].values\n",
        "y = test_data['ma'].values\n",
        "x = ';' + x + '.'\n",
        "y = ';' + y + '.'\n",
        "\n",
        "# Getting max length\n",
        "max_eng_len = max([len(i) for i in x])\n",
        "max_mar_len = max([len(i) for i in y])\n",
        "print(max_eng_len, max_mar_len)\n",
        "\n",
        "def process(data):\n",
        "    x_values, y_values = data['en'].values, data['ma'].values\n",
        "    x_values = ';' + x_values + '.'\n",
        "    y_values = ';' + y_values + '.'\n",
        "    print(x_values[0:3])\n",
        "    print(y_values[0:3]) \n",
        "\n",
        "    a = torch.zeros((len(x_values), max_eng_len), dtype=torch.int64)\n",
        "    b = torch.zeros((len(y_values), max_eng_len), dtype=torch.int64)\n",
        "\n",
        "    processed_data = []\n",
        "    for i, (xx, yy) in enumerate(zip(x_values, y_values)):\n",
        "        for j, ch in enumerate(xx):\n",
        "            a[i, j] = eng_token_map[ch]\n",
        "\n",
        "        for j, ch in enumerate(yy):\n",
        "            if ch in mar_token_map:\n",
        "                b[i, j] = mar_token_map[ch]\n",
        "            else:\n",
        "                b[i, j] = unknown_token\n",
        "\n",
        "        processed_data.append((a[i], b[i]))\n",
        "    processed_data = [(a[i], b[i]) for i in range(len(x_values))]\n",
        "    print(a.shape)\n",
        "    print(b.shape)\n",
        "    return processed_data\n",
        "\n",
        "train_processed = process(train_data)\n",
        "val_processed = process(val_data)\n",
        "test_processed = process(test_data)\n",
        "\n",
        "print('\\n')\n",
        "print('Number of rows:', len(train_processed))\n",
        "print('Number of columns:', len(train_processed[0]))\n",
        "print(train_processed[0][1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3S30rlA3n-zG",
        "outputId": "c3549cfc-59ae-42e4-9ec6-be2d4556c38b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30 22\n",
            "[';fusharun.' ';bhulthapana.' ';vhayaki.']\n",
            "[';फुशारुन.' ';भूलथापाना.' ';व्हायकी.']\n",
            "torch.Size([51200, 30])\n",
            "torch.Size([51200, 30])\n",
            "[';garvyabarobarach.' ';reo.' ';sangrahalaye.']\n",
            "[';गारव्याबरोबरच.' ';रियो.' ';संग्रहालये.']\n",
            "torch.Size([4096, 30])\n",
            "torch.Size([4096, 30])\n",
            "[';heetler.' ';kshama.' ';jinkta.']\n",
            "[';हिटलर.' ';क्षमा.' ';जिंकता.']\n",
            "torch.Size([4096, 30])\n",
            "torch.Size([4096, 30])\n",
            "\n",
            "\n",
            "Number of rows: 51200\n",
            "Number of columns: 2\n",
            "tensor([65, 37, 54, 46, 51, 42, 54, 35, 66,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Use class template \n",
        "change function names make code long\n",
        "\n",
        "def load_data(path):\n",
        "    with open(path) as fil:\n",
        "        data = pd.read_csv(fil, sep=',', header=None, names=[\"en\", \"ma\", \"\"], skip_blank_lines=True, index_col=None)\n",
        "    data = data[data['en'].notna()]\n",
        "    data = data[data['ma'].notna()]\n",
        "    data = data[['en', 'ma']]\n",
        "    return data\n",
        "\n",
        "train_data = load_data(\"/content/drive/MyDrive/aksharantar_sampled/aksharantar_sampled/mar/mar_train.csv\")\n",
        "val_data = load_data(\"/content/drive/MyDrive/aksharantar_sampled/aksharantar_sampled/mar/mar_valid.csv\")\n",
        "test_data = load_data(\"/content/drive/MyDrive/aksharantar_sampled/aksharantar_sampled/mar/mar_test.csv\")\n",
        "\n",
        "def unique_tokenize(data):\n",
        "    e = data['en'].values\n",
        "    m = data['ma'].values\n",
        "    e_tokens = set()\n",
        "    m_tokens = set()\n",
        "\n",
        "    for x, y in zip(e, m):\n",
        "        for ch in x:\n",
        "            e_tokens.add(ch)\n",
        "        for ch in y:\n",
        "            m_tokens.add(ch)\n",
        "    e_tokens = sorted(list(e_tokens))\n",
        "    m_tokens = sorted(list(m_tokens))\n",
        "    return m_tokens, e_tokens\n",
        "\n",
        "m_tokens, e_tokens = unique_tokenize(train_data)\n",
        "\n",
        "def tokenize_map(m_tokens, e_tokens):\n",
        "    m_token_map = dict([(ch, i + 1) for i, ch in enumerate(m_tokens)])\n",
        "    e_token_map = dict([(ch, i + 1) for i, ch in enumerate(e_tokens)])\n",
        "    m_token_map[\" \"] = 0\n",
        "    e_token_map[\" \"] = 0\n",
        "    m_token_map[';'] = 65\n",
        "    m_token_map['.'] = 66\n",
        "    e_token_map[';'] = 27\n",
        "    e_token_map['.'] = 28\n",
        "    m_token_map['<unk>'] = 64\n",
        "\n",
        "    return m_token_map, e_token_map\n",
        "\n",
        "m_token_map, e_token_map = tokenize_map(m_tokens, e_tokens)\n",
        "\n",
        "unknown_token = 64\n",
        "\n",
        "x = test_data['en'].values\n",
        "y = test_data['ma'].values\n",
        "x = ';' + x + '.'\n",
        "y = ';' + y + '.'\n",
        "\n",
        "# Getting max length\n",
        "max_e_len = max([len(i) for i in x])\n",
        "max_m_len = max([len(i) for i in y])\n",
        "print(max_e_len, max_m_len)\n",
        "\n",
        "def process(data):\n",
        "    x_vals, y_vals = data['en'].values, data['ma'].values\n",
        "    x_vals = ';' + x_vals + '.'\n",
        "    y_vals = ';' + y_vals + '.'\n",
        "    print(x_vals[0:3])\n",
        "    print(y_vals[0:3]) \n",
        "\n",
        "    a = torch.zeros((len(x_vals), max_e_len), dtype=torch.int64)\n",
        "    b = torch.zeros((len(y_vals), max_e_len), dtype=torch.int64)\n",
        "\n",
        "    processed_data = []\n",
        "    for i, (xx, yy) in enumerate(zip(x_vals, y_vals)):\n",
        "        for j, ch in enumerate(xx):\n",
        "            a[i, j] = e_token_map[ch]\n",
        "\n",
        "        for j, ch in enumerate(yy):\n",
        "            if ch in m_token_map:\n",
        "                b[i, j] = m_token_map[ch]\n",
        "            else:\n",
        "                b[i, j] = unknown_token\n",
        "\n",
        "        processed_data.append((a[i], b[i]))\n",
        "    processed_data = [(a[i], b[i]) for i in range(len(x_vals))]\n",
        "    print(a.shape)\n",
        "    print(b.shape)\n",
        "    return processed_data\n",
        "\n",
        "train_processed = process(train_data)\n",
        "val_processed = process(val_data)\n",
        "test_processed = process(test_data)\n",
        "\n",
        "print('\\n')\n",
        "print('Number of rows:', len(train_processed))\n",
        "print('Number of columns:', len(train_processed[0]))\n",
        "print(train_processed[0][1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEQjB4WStD_3",
        "outputId": "283c30fa-f4a1-4a07-8608-8645b039a720"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30 22\n",
            "[';fusharun.' ';bhulthapana.' ';vhayaki.']\n",
            "[';फुशारुन.' ';भूलथापाना.' ';व्हायकी.']\n",
            "torch.Size([51200, 30])\n",
            "torch.Size([51200, 30])\n",
            "[';garvyabarobarach.' ';reo.' ';sangrahalaye.']\n",
            "[';गारव्याबरोबरच.' ';रियो.' ';संग्रहालये.']\n",
            "torch.Size([4096, 30])\n",
            "torch.Size([4096, 30])\n",
            "[';heetler.' ';kshama.' ';jinkta.']\n",
            "[';हिटलर.' ';क्षमा.' ';जिंकता.']\n",
            "torch.Size([4096, 30])\n",
            "torch.Size([4096, 30])\n",
            "\n",
            "\n",
            "Number of rows: 51200\n",
            "Number of columns: 2\n",
            "tensor([65, 37, 54, 46, 51, 42, 54, 35, 66,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing"
      ],
      "metadata": {
        "id": "HX0kwyi1x6Vn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "class DataLoader:\n",
        "    def __init__(self, train_path, val_path, test_path):\n",
        "        self.train_data = self.load_data(train_path)\n",
        "        self.val_data = self.load_data(val_path)\n",
        "        self.test_data = self.load_data(test_path)\n",
        "        self.m_token_map, self.e_token_map = self.tokenize_map(self.unique_tokenize(self.train_data))\n",
        "        self.max_e_len, self.max_m_len = self.get_max_lengths(self.test_data['en'].values, self.test_data['ma'].values)\n",
        "        self.unknown_token = 64\n",
        "\n",
        "    def load_data(self, path):\n",
        "        with open(path) as fil:\n",
        "            data = pd.read_csv(fil, sep=',', header=None, names=[\"en\", \"ma\", \"\"], skip_blank_lines=True, index_col=None)\n",
        "        data = data[data['en'].notna()]\n",
        "        data = data[data['ma'].notna()]\n",
        "        data = data[['en', 'ma']]\n",
        "        return data\n",
        "\n",
        "    def unique_tokenize(self, data):\n",
        "        e = data['en'].values\n",
        "        m = data['ma'].values\n",
        "        e_tokens = set()\n",
        "        m_tokens = set()\n",
        "\n",
        "        for x, y in zip(e, m):\n",
        "            for ch in x:\n",
        "                e_tokens.add(ch)\n",
        "            for ch in y:\n",
        "                m_tokens.add(ch)\n",
        "        e_tokens = sorted(list(e_tokens))\n",
        "        m_tokens = sorted(list(m_tokens))\n",
        "        return m_tokens, e_tokens\n",
        "\n",
        "    def tokenize_map(self, tokens):\n",
        "        m_tokens, e_tokens = tokens\n",
        "        m_token_map = dict([(ch, i + 1) for i, ch in enumerate(m_tokens)])\n",
        "        e_token_map = dict([(ch, i + 1) for i, ch in enumerate(e_tokens)])\n",
        "        m_token_map[\" \"] = 0\n",
        "        e_token_map[\" \"] = 0\n",
        "        m_token_map[';'] = 65\n",
        "        m_token_map['.'] = 66\n",
        "        e_token_map[';'] = 27\n",
        "        e_token_map['.'] = 28\n",
        "        m_token_map['<unk>'] = 64\n",
        "\n",
        "        return m_token_map, e_token_map\n",
        "\n",
        "    def get_max_lengths(self, x_vals, y_vals):\n",
        "        x = ';' + x_vals + '.'\n",
        "        y = ';' + y_vals + '.'\n",
        "        max_e_len = max([len(i) for i in x])\n",
        "        max_m_len = max([len(i) for i in y])\n",
        "        return max_e_len, max_m_len\n",
        "\n",
        "    def process(self, data):\n",
        "        x_vals, y_vals = data['en'].values, data['ma'].values\n",
        "        x_vals = ';' + x_vals + '.'\n",
        "        y_vals = ';' + y_vals + '.'\n",
        "        print(x_vals[0:3])\n",
        "        print(y_vals[0:3]) \n",
        "\n",
        "        a = torch.zeros((len(x_vals), self.max_e_len), dtype=torch.int64)\n",
        "        b = torch.zeros((len(y_vals), self.max_e_len), dtype=torch.int64)\n",
        "\n",
        "        processed_data = []\n",
        "        for i, (xx, yy) in enumerate(zip(x_vals, y_vals)):\n",
        "            for j, ch in enumerate(xx):\n",
        "                a[i, j] = self.e_token_map[ch]\n",
        "\n",
        "            for j, ch in enumerate(yy):\n",
        "                if ch in self.m_token_map:\n",
        "                    b[i, j] = self.m_token_map[ch]\n",
        "                else:\n",
        "                    b[i, j] = self.unknown_token\n",
        "\n",
        "            processed_data.append((a[i], b[i]))\n",
        "        processed_data = [(a[i], b[i]) for i in range(len(x_vals))]\n",
        "        return processed_data\n",
        "\n",
        "    def print_summary(self):\n",
        "        train_processed = self.process(self.train_data)\n",
        "        val_processed = self.process(self.val_data)\n",
        "        test_processed = self.process(self.test_data)\n",
        "\n",
        "        print('\\n')\n",
        "        print('Number of rows:', len(train_processed))\n",
        "        print('Number of columns:', len(train_processed[0]))\n",
        "        print(train_processed[0][1])\n",
        "\n",
        "\n",
        "# Example usage\n",
        "data_loader = DataLoader(\n",
        "    \"/content/drive/MyDrive/aksharantar_sampled/aksharantar_sampled/mar/mar_train.csv\",\n",
        "    \"/content/drive/MyDrive/aksharantar_sampled/aksharantar_sampled/mar/mar_valid.csv\",\n",
        "    \"/content/drive/MyDrive/aksharantar_sampled/aksharantar_sampled/mar/mar_test.csv\"\n",
        ")\n",
        "\n",
        "data_loader.print_summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JcHig6xtxt1",
        "outputId": "a58dedc6-1a73-4cad-e0c2-dc6a723ba124"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[';fusharun.' ';bhulthapana.' ';vhayaki.']\n",
            "[';फुशारुन.' ';भूलथापाना.' ';व्हायकी.']\n",
            "[';garvyabarobarach.' ';reo.' ';sangrahalaye.']\n",
            "[';गारव्याबरोबरच.' ';रियो.' ';संग्रहालये.']\n",
            "[';heetler.' ';kshama.' ';jinkta.']\n",
            "[';हिटलर.' ';क्षमा.' ';जिंकता.']\n",
            "\n",
            "\n",
            "Number of rows: 51200\n",
            "Number of columns: 2\n",
            "tensor([65, 37, 54, 46, 51, 42, 54, 35, 66,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n"
          ]
        }
      ]
    }
  ]
}